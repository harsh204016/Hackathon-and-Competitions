{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import inflect\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "import string, unicodedata\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer,PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.245025e+18  Happy #MothersDay to all you amazing mothers o...   en   \n",
       "1  1.245759e+18  Happy Mothers Day Mum - I'm sorry I can't be t...   en   \n",
       "2  1.246087e+18  Happy mothers day To all This doing a mothers ...   en   \n",
       "3  1.244803e+18  Happy mothers day to this beautiful woman...ro...   en   \n",
       "4  1.244876e+18  Remembering the 3 most amazing ladies who made...   en   \n",
       "\n",
       "  retweet_count  original_author  sentiment_class  \n",
       "0             0      BeenXXPired                0  \n",
       "1             1   FestiveFeeling                0  \n",
       "2             0     KrisAllenSak               -1  \n",
       "3             0       Queenuchee                0  \n",
       "4             0  brittan17446794               -1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.246628e+18</td>\n",
       "      <td>3. Yeah, I once cooked potatoes when I was 3 y...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245898e+18</td>\n",
       "      <td>Happy Mother's Day to all the mums, step-mums,...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>iiarushii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244717e+18</td>\n",
       "      <td>I love the people from the UK, however, when I...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245730e+18</td>\n",
       "      <td>Happy 81st Birthday Happy Mother’s Day to my m...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>TheBookTweeters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244636e+18</td>\n",
       "      <td>Happy Mothers day to all those wonderful mothe...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.246628e+18  3. Yeah, I once cooked potatoes when I was 3 y...   en   \n",
       "1  1.245898e+18  Happy Mother's Day to all the mums, step-mums,...   en   \n",
       "2  1.244717e+18  I love the people from the UK, however, when I...   en   \n",
       "3  1.245730e+18  Happy 81st Birthday Happy Mother’s Day to my m...   en   \n",
       "4  1.244636e+18  Happy Mothers day to all those wonderful mothe...   en   \n",
       "\n",
       "  retweet_count  original_author  \n",
       "0             0        LToddWood  \n",
       "1             0        iiarushii  \n",
       "2             0   andreaanderegg  \n",
       "3             1  TheBookTweeters  \n",
       "4             0   andreaanderegg  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['original_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(raw_text):\n",
    "    raw_text=raw_text.strip()\n",
    "    no_ment =re.sub(r'@\\w+', '',raw_text )\n",
    "    link = re.sub(r'https?:\\/\\/?.*[\\r\\n]*','',no_ment)\n",
    "    \n",
    "    new_word = re.sub(r'[!\"#$%&\\'()*+,-/:;<=>?@^_{|}~]', '', link)\n",
    "    no_urls1 = re.sub(r'pic.*[\\r\\n]*','',new_word)\n",
    "    custom_con = re.sub(r\"mother’s\",'mother is',no_urls1)\n",
    "    return(custom_con)\n",
    "\n",
    "def lower(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return(\" \".join(new_words))\n",
    "\n",
    "def denoise_text(text):\n",
    "    # Strip html if any. For ex. removing <html>, <p> tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # Replace contractions in the text. For ex. didn't -> did not\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    words = input_text.split()\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\",'three']\n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return(\" \".join(clean_words))\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(text):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    words = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return(\" \".join(lemmas))\n",
    "def stem_words(text):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    words = text.split()\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return(\" \".join(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE'],\n",
       "       ['HAPPY MOTHER’S DAY! BRATZ WORLD FAMILIEZ YASMIN & HER MUM! (PORTIA) #bratz #bratzworldfamiliez #bratz2008 #bratzkidz #bratzyasmin #bratzportia #theonlygirlswithapassionforfashion #bratzdolls #bratzcollector #happymothersday #mothersday #mothersday2020 #azdoesmakeup pic.twitter.com/5m1T81zhpU'],\n",
       "       [\"Happy Mother’s Day to all you wonderful mums out there! #mothersday #cupcakes #supermums #traceyscakecraft #couturesugarpaste #saracinouk #hinckley @Tracey's CakeCraft https://www. instagram.com/p/B-CUzHjg-Au/ ?igshid=lf3elmlv7q7g …\"],\n",
       "       ['Show me a mother of two as sexy as Rosie Happy Mothers Day beautiful Rosie @officiallrosie I love you #HappyMothersDayRosie #HappyMothersDay2020 #WhyILoveRoksie #Roksie pic.twitter.com/6oMZGHcIVe']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[4:8,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "    train['original_text'] = train['original_text'].apply(lower)\n",
    "    train['original_text'] = train['original_text'].apply(denoise_text)\n",
    "    train['original_text']= train['original_text'].apply(remove_urls)\n",
    "    train['original_text'] = train['original_text'].apply(remove_stopwords)\n",
    "    #train['original_text'] = train['original_text'].apply(stem_words)\n",
    "    train['original_text'] = train['original_text'].apply(lemmatize_verbs)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = normalize(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['remember amaze ladies make late grandmother iris mum carol great grandmother ethel miss never forget happy mother day great mums love send xxxx'],\n",
       "       ['happy mother day bratz world familiez yasmin mum portia bratz bratzworldfamiliez bratz2008 bratzkidz bratzyasmin bratzportia theonlygirlswithapassionforfashion bratzdolls bratzcollector happymothersday mothersday mothersday2020 azdoesmakeup'],\n",
       "       ['happy mother day wonderful mums mothersday cupcakes supermums traceyscakecraft couturesugarpaste saracinouk hinckley cakecraft'],\n",
       "       ['show mother two sexy rosie happy mother day beautiful rosie love happymothersdayrosie happymothersday2020 whyiloveroksie roksie']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[4:8,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3235, 6), (1387, 5))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 4396),\n",
       " ('day', 3688),\n",
       " ('happy', 3398),\n",
       " ('mothersday', 1525),\n",
       " ('love', 1059),\n",
       " ('mum', 866),\n",
       " ('not', 800),\n",
       " ('mums', 754),\n",
       " ('today', 516),\n",
       " ('us', 423)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in train[\"original_text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>amaze know hard not able see today us protect ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>mum sorry not bring flower cwtch honestly poin...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>days work today quiet time reflect dog walk fi...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>beautiful womanroyalty soothe mummy jeremy eme...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>remember amaze ladies make late grandmother ir...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.245025e+18  amaze know hard not able see today us protect ...   en   \n",
       "1  1.245759e+18  mum sorry not bring flower cwtch honestly poin...   en   \n",
       "2  1.246087e+18  days work today quiet time reflect dog walk fi...   en   \n",
       "3  1.244803e+18  beautiful womanroyalty soothe mummy jeremy eme...   en   \n",
       "4  1.244876e+18  remember amaze ladies make late grandmother ir...   en   \n",
       "\n",
       "  retweet_count  original_author  sentiment_class  \n",
       "0             0      BeenXXPired                0  \n",
       "1             1   FestiveFeeling                0  \n",
       "2             0     KrisAllenSak               -1  \n",
       "3             0       Queenuchee                0  \n",
       "4             0  brittan17446794               -1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(4)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "train[\"original_text\"] = train[\"original_text\"].apply(lambda text: remove_freqwords(text))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 1059),\n",
       " ('mum', 866),\n",
       " ('not', 800),\n",
       " ('mums', 754),\n",
       " ('today', 516),\n",
       " ('us', 423),\n",
       " ('thank', 368),\n",
       " ('wish', 366),\n",
       " ('amaze', 332),\n",
       " ('hope', 292)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in train[\"original_text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc=preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.sentiment_class.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.original_text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2588,)\n",
      "(647,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xtrain),type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4116411006587467"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(C=4.0)\n",
    "clf1.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf1.predict(xvalid_tfv)\n",
    "f1_score(yvalid,predictions,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english',max_features=2000,\n",
    "                     max_df=0.90, min_df=2)\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:   21.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.3965021345241771\n"
     ]
    }
   ],
   "source": [
    "RandomForestClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=600, random_state=42,verbose=1).fit(xtrain_ctv, ytrain) \n",
    "prediction = rf.predict(xvalid_ctv)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.3965021345241771\n"
     ]
    }
   ],
   "source": [
    "clf2 = LogisticRegression(C=4.0)\n",
    "clf2.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf2.predict(xvalid_ctv)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.36014130971693936\n"
     ]
    }
   ],
   "source": [
    "clf3 = MultinomialNB()\n",
    "clf3.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf3.predict(xvalid_tfv)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.4029537192037015\n"
     ]
    }
   ],
   "source": [
    "clf4 = MultinomialNB()\n",
    "clf4.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf4.predict(xvalid_ctv)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=100)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.4022984606095279\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf5 = SVC(C=6.0, probability=False) # since we need probabilities\n",
    "clf5.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf5.predict(xvalid_svd_scl)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.44125688179247874\n"
     ]
    }
   ],
   "source": [
    "clf6 = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf6.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf6.predict(xvalid_tfv.tocsc())\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.4019174122292343\n"
     ]
    }
   ],
   "source": [
    "clf6 = xgb.XGBClassifier(max_depth=10, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf6.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf6.predict(xvalid_tfv.tocsc())\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.38714198705602904\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on ctv\n",
    "clf7 = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf7.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf7.predict(xvalid_ctv.tocsc())\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.3893393747349895\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = xgb.XGBClassifier(max_depth=10, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1,\n",
    "                        verbose=1)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict(xvalid_svd)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.3978327051324242\n"
     ]
    }
   ],
   "source": [
    "clf8 = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf8.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf8.predict(xvalid_tfv.tocsc())\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:40, 9785.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('d:/NLP/cs224/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 2588/2588 [00:01<00:00, 2081.26it/s]\n",
      "100%|██████████████████████████████████████████| 647/647 [00:00<00:00, 2036.45it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.37453990167520296\n"
     ]
    }
   ],
   "source": [
    "clf9 = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf9.fit(xtrain_glove, ytrain)\n",
    "predictions = clf9.predict(xvalid_glove)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.38174662854611424\n"
     ]
    }
   ],
   "source": [
    "clf10 = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf10.fit(xtrain_glove, ytrain)\n",
    "predictions = clf10.predict(xvalid_glove)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=100, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c6f7c959b0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=32, \n",
    "          epochs=5, verbose=0, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.36204820314787756\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(xvalid_glove_scl)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 9018/9018 [00:00<00:00, 316995.61it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c6a71de0f0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3,\n",
    "                          verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, epochs=100, verbose=0, \n",
    "          validation_data=(xvalid_pad, yvalid_enc),callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.36204820314787756\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(xvalid_pad)\n",
    "print('f1_score',f1_score(yvalid,predictions,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2588 samples, validate on 647 samples\n",
      "Epoch 1/100\n",
      "2588/2588 [==============================] - ETA: 52s - loss: 1.36 - ETA: 25s - loss: 1.28 - ETA: 14s - loss: 1.25 - ETA: 6s - loss: 1.2200 - ETA: 0s - loss: 1.219 - 34s 13ms/step - loss: 1.2222 - val_loss: 1.0570\n",
      "Epoch 2/100\n",
      "2588/2588 [==============================] - ETA: 16s - loss: 1.14 - ETA: 12s - loss: 1.15 - ETA: 8s - loss: 1.1527 - ETA: 4s - loss: 1.153 - ETA: 0s - loss: 1.145 - 23s 9ms/step - loss: 1.1449 - val_loss: 1.0774\n",
      "Epoch 3/100\n",
      "2588/2588 [==============================] - ETA: 17s - loss: 1.10 - ETA: 13s - loss: 1.08 - ETA: 8s - loss: 1.0843 - ETA: 4s - loss: 1.081 - ETA: 0s - loss: 1.085 - 24s 9ms/step - loss: 1.0850 - val_loss: 1.0822\n",
      "Epoch 4/100\n",
      "2588/2588 [==============================] - ETA: 17s - loss: 1.08 - ETA: 12s - loss: 1.09 - ETA: 8s - loss: 1.0843 - ETA: 4s - loss: 1.081 - ETA: 0s - loss: 1.077 - 23s 9ms/step - loss: 1.0763 - val_loss: 1.0801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c6b49ce7b8>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc),\n",
    "          callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c6f8b53240>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, \n",
    "                          verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=0, validation_data=(xvalid_pad, yvalid_enc),\n",
    "          callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36204820314787756"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = model.predict_classes(xvalid_pad)\n",
    "f1_score(yvalid,pr,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparation for submission of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['original_text'] = test['original_text'].apply(lower).apply(denoise_text)\n",
    "test['original_text']= test['original_text'].apply(remove_urls)\n",
    "test['original_text'] = test['original_text'].apply(remove_stopwords)\n",
    "#train['original_text'] = train['original_text'].apply(stem_words)\n",
    "test['original_text'] = test['original_text'].apply(lemmatize_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tfv = tfv.transform(test.original_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre = clf.predict(xtest_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lbl_enc.inverse_transform(test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment_class'] = pred \n",
    "submission = test[['id','sentiment_class']] \n",
    "submission.to_csv('sub_xg_abhi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you are not my best friend's mother's\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoise_text(\"you aren't my best friend's mother's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you aren't my best friend's mother's\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_verbs(\"you aren't my best friend's mother's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you aren't my best friend' mother'\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(\"you aren't my best friend's mother's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"best friend's mother's\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"you aren't my best friend's mother's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

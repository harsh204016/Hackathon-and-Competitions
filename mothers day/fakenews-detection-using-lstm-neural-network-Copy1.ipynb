{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "Application of LSTM and GRU Recurrent Neural Networks in Fake NEWS detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing neural network libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D,Conv1D,GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('d:/datasets/mothers day/dataset/train.csv')\n",
    "test = pd.read_csv('d:/datasets/mothers day/dataset/test.csv')\n",
    "train_data = train.copy()\n",
    "test_data = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3235, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.245025e+18  Happy #MothersDay to all you amazing mothers o...   en   \n",
       "1  1.245759e+18  Happy Mothers Day Mum - I'm sorry I can't be t...   en   \n",
       "2  1.246087e+18  Happy mothers day To all This doing a mothers ...   en   \n",
       "3  1.244803e+18  Happy mothers day to this beautiful woman...ro...   en   \n",
       "4  1.244876e+18  Remembering the 3 most amazing ladies who made...   en   \n",
       "\n",
       "  retweet_count  original_author  sentiment_class  \n",
       "0             0      BeenXXPired                0  \n",
       "1             1   FestiveFeeling                0  \n",
       "2             0     KrisAllenSak               -1  \n",
       "3             0       Queenuchee                0  \n",
       "4             0  brittan17446794               -1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1387, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.246628e+18</td>\n",
       "      <td>3. Yeah, I once cooked potatoes when I was 3 y...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245898e+18</td>\n",
       "      <td>Happy Mother's Day to all the mums, step-mums,...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>iiarushii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244717e+18</td>\n",
       "      <td>I love the people from the UK, however, when I...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245730e+18</td>\n",
       "      <td>Happy 81st Birthday Happy Mother’s Day to my m...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>TheBookTweeters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244636e+18</td>\n",
       "      <td>Happy Mothers day to all those wonderful mothe...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.246628e+18  3. Yeah, I once cooked potatoes when I was 3 y...   en   \n",
       "1  1.245898e+18  Happy Mother's Day to all the mums, step-mums,...   en   \n",
       "2  1.244717e+18  I love the people from the UK, however, when I...   en   \n",
       "3  1.245730e+18  Happy 81st Birthday Happy Mother’s Day to my m...   en   \n",
       "4  1.244636e+18  Happy Mothers day to all those wonderful mothe...   en   \n",
       "\n",
       "  retweet_count  original_author  \n",
       "0             0        LToddWood  \n",
       "1             0        iiarushii  \n",
       "2             0   andreaanderegg  \n",
       "3             1  TheBookTweeters  \n",
       "4             0   andreaanderegg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  sentiment_class\n",
       "0  Happy #MothersDay to all you amazing mothers o...                0\n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be t...                0\n",
       "2  Happy mothers day To all This doing a mothers ...               -1\n",
       "3  Happy mothers day to this beautiful woman...ro...                0\n",
       "4  Remembering the 3 most amazing ladies who made...               -1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping missing values from text columns alone. \n",
    "train_data.drop(['id','lang','retweet_count','original_author'],axis=1,inplace=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit([-1,0,1])\n",
    "train_data['sentiment_class'] =le.transform(train_data['sentiment_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>1</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>1</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  sentiment_class  length\n",
       "0  Happy #MothersDay to all you amazing mothers o...                1     252\n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be t...                1     301\n",
       "2  Happy mothers day To all This doing a mothers ...                0     298\n",
       "3  Happy mothers day to this beautiful woman...ro...                1     155\n",
       "4  Remembering the 3 most amazing ladies who made...                0     254"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "[length.append(len(str(text))) for text in train_data['original_text']]\n",
    "train_data['length'] = length\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 728, 227)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can keep 250 as max features for training the neural network.\n",
    "\n",
    "**minimum length is 73?? Looks like there are some outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[train_data['length'] > 350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 10 outliers in this dataset. Outliers can be removed. It is a good practice to check the outliers before removing them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189    Happy Mother's Day! A bit of time with one of ...\n",
       "1233    Happy Mother's Day to all the amazing Mums out...\n",
       "1301    @BofaFerjani لكل ولد و بنت أمهم متوفية كل سنة ...\n",
       "1328    لكل ولد و بنت أمهم متوفية كل سنة و أمي هيا أمك...\n",
       "1406    Happy Mothers day to my super Mum. Ochie dike ...\n",
       "1826    Happy #MothersDay to all the amazing women who...\n",
       "2207    #القصة_مو_مزحة #خليك_بالبيت #عبير_شمس_الدين #س...\n",
       "2554    First Mothers Day not spent with my mum but we...\n",
       "2647    الشي الوحيد اللي بيخليني اعرس عشان عقب كم سنه ...\n",
       "Name: original_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['original_text'][train_data['length'] < 130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Mostly empty texts. They can be removed since they will surely guide the neural network in the wrong way*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the outliers\n",
    "train_data = train_data.drop(train_data['original_text'][train_data['length'] < 130].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the outliers\n",
    "train_data = train_data.drop(train_data['original_text'][train_data['length'] > 350].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 728, 228)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995    To all the wonderful women out there, you’re a...\n",
       "2038    For all mothers in the world #happy _ mothers_...\n",
       "Name: original_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['original_text'][train_data['length'] < 141]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the Text before feeding it into the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import inflect\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def denoise_text(text):\n",
    "    # Strip html if any. For ex. removing <html>, <p> tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # Replace contractions in the text. For ex. didn't -> did not\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"] \n",
    "    clean_words = [word for word in input_text if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return clean_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "def remove_mention(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'@[\\w]*', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def normalize_text(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_mention(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def text_prepare(text):\n",
    "    text = denoise_text(text)\n",
    "    text = ' '.join([x for x in normalize_text(tokenize(text))])\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate every tweet into english language\n",
    "#TextBlob(c).detect_language\n",
    "def detect(x):\n",
    "    if(TextBlob(x).detect_language()!='en'):\n",
    "        return str(TextBlob(x).translate(to='en'))\n",
    "\n",
    "train_data['original_text'] = train_data['original_text'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#to correct the spellings\n",
    "train_data['original_text'] = train_data['original_text'].apply(lambda x:str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#normalization\n",
    "train_data['original_text'] = [text_prepare(x) for x in train_data['original_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 7  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = 1000  # Size of the validation set\n",
    "NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 24  # Maximum number of words in a sequence\n",
    "GLOVE_DIM = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text - converting the words, letters into counts or numbers. \n",
    "# We dont need to explicitly remove the punctuations. we have an inbuilt option in Tokenizer for this purpose\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = train_data['original_text'])\n",
    "X = tokenizer.texts_to_sequences(texts = train_data['original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now applying padding to make them even shaped.\n",
    "X = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3226,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = train_data['sentiment_class']\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3226, 228)\n",
      "(3226,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "y = train_data['sentiment_class'].values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data training data for training and validation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our training data preprocessed and ready for training the neural network. \n",
    "\n",
    "We have to create a neural network now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# LSTM Neural Network\n",
    "lstm_model = Sequential(name = 'lstm_nn_model')\n",
    "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
    "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
    "lstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
    "lstm_model.add(layer = Dense(units = len(set(y)),  activation = 'softmax', name = 'output_layer'))\n",
    "# compiling the model\n",
    "lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2580/2580 [==============================] - 66s 25ms/sample - loss: 1.0411 - acc: 0.5279\n",
      "Epoch 2/3\n",
      "2580/2580 [==============================] - 62s 24ms/sample - loss: 1.0182 - acc: 0.5329\n",
      "Epoch 3/3\n",
      "2580/2580 [==============================] - 62s 24ms/sample - loss: 1.0122 - acc: 0.5326\n"
     ]
    }
   ],
   "source": [
    "lstm_model_fit = lstm_model.fit(X_train, y_train, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing GRU Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU neural Network\n",
    "gru_model = Sequential(name = 'gru_nn_model')\n",
    "gru_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
    "gru_model.add(layer = GRU(units = 120, dropout = 0.2, \n",
    "                          recurrent_dropout = 0.2, recurrent_activation = 'relu', \n",
    "                          activation = 'relu', name = '2nd_layer'))\n",
    "gru_model.add(layer = Dropout(rate = 0.4, name = '3rd_layer'))\n",
    "gru_model.add(layer = Dense(units = 120, activation = 'relu', name = '4th_layer'))\n",
    "gru_model.add(layer = Dropout(rate = 0.4, name = '5th_layer'))\n",
    "gru_model.add(layer = Dense(units = len(set(y)), activation = 'softmax', name = 'output_layer'))\n",
    "# compiling the model\n",
    "gru_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1st_layer (Embedding)        (None, None, 120)         27360     \n",
      "_________________________________________________________________\n",
      "2nd_layer (GRU)              (None, 120)               86760     \n",
      "_________________________________________________________________\n",
      "3rd_layer (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "4th_layer (Dense)            (None, 120)               14520     \n",
      "_________________________________________________________________\n",
      "5th_layer (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 3)                 363       \n",
      "=================================================================\n",
      "Total params: 129,003\n",
      "Trainable params: 129,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2580/2580 [==============================] - 42s 16ms/sample - loss: 1.0386 - acc: 0.5209\n",
      "Epoch 2/3\n",
      "2580/2580 [==============================] - 40s 16ms/sample - loss: 1.0189 - acc: 0.5329\n",
      "Epoch 3/3\n",
      "2580/2580 [==============================] - 41s 16ms/sample - loss: 1.0114 - acc: 0.5329\n"
     ]
    }
   ],
   "source": [
    "gru_model_fit = gru_model.fit(X_train, y_train, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646/646 [==============================] - 3s 4ms/sample - loss: 1.0453 - acc: 0.4985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0452867805404191, 0.498452]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = gru_model.evaluate(X_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now preparing the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1387, 5)\n",
      "(1387, 5)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test_data = test.copy()\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filling the Missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3. Yeah, I once cooked potatoes when I was 3 y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Mother's Day to all the mums, step-mums,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the people from the UK, however, when I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text\n",
       "0  3. Yeah, I once cooked potatoes when I was 3 y...\n",
       "1  Happy Mother's Day to all the mums, step-mums,...\n",
       "2  I love the people from the UK, however, when I..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.drop(['id','lang','retweet_count','original_author'],axis=1,inplace=True)\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = tokenizer.texts_to_sequences(texts = test_data['original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = pad_sequences(sequences = test_text, maxlen = max_features, padding = 'pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_prediction = lstm_model.predict_classes(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = le.inverse_transform(lstm_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM predictions have more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment_class'] = pred1\n",
    "submission = test[['id','sentiment_class']] \n",
    "submission.to_csv('bilstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_pred = gru_model.predict_classes(test_text)\n",
    "pred2 = le.inverse_transform(gru_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment_class'] = pred2\n",
    "submission = test[['id','sentiment_class']] \n",
    "submission.to_csv('gru.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 120)         27360     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         76928     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 129,251\n",
      "Trainable params: 129,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim = max_features, output_dim = 120))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2580 samples, validate on 646 samples\n",
      "Epoch 1/10\n",
      "2580/2580 [==============================] - 14s 6ms/sample - loss: 1.0354 - acc: 0.5310 - val_loss: 1.0409 - val_acc: 0.4985\n",
      "Epoch 2/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 1.0220 - acc: 0.5329 - val_loss: 1.0441 - val_acc: 0.4985\n",
      "Epoch 3/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 1.0109 - acc: 0.5329 - val_loss: 1.0480 - val_acc: 0.4985\n",
      "Epoch 4/10\n",
      "2580/2580 [==============================] - 14s 5ms/sample - loss: 0.9843 - acc: 0.5326 - val_loss: 1.0577 - val_acc: 0.4985\n",
      "Epoch 5/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 0.9161 - acc: 0.5562 - val_loss: 1.0837 - val_acc: 0.4876\n",
      "Epoch 6/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 0.7820 - acc: 0.6407 - val_loss: 1.3252 - val_acc: 0.4675\n",
      "Epoch 7/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 0.6139 - acc: 0.7306 - val_loss: 1.4374 - val_acc: 0.3607\n",
      "Epoch 8/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 0.4803 - acc: 0.8019 - val_loss: 1.8036 - val_acc: 0.4071\n",
      "Epoch 9/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 0.4083 - acc: 0.8372 - val_loss: 1.8996 - val_acc: 0.3839\n",
      "Epoch 10/10\n",
      "2580/2580 [==============================] - 12s 5ms/sample - loss: 0.2968 - acc: 0.8950 - val_loss: 2.4160 - val_acc: 0.4334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a72a962d68>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=10,verbose=1,batch_size=16,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

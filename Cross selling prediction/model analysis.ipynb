{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split , StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lg\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((381109, 12), (127037, 11))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "train.shape , test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(model,fileName,test_values):\n",
    "    y_pred = model.predict_proba(test_values)[:,1]\n",
    "    sub['Response'] = y_pred\n",
    "    sub.to_csv(fileName+'.csv',index=False)\n",
    "    return sub\n",
    "scores = [0]\n",
    "\n",
    "def getInfo(dataset):\n",
    "    info = pd.DataFrame({'Nunuique':dataset.nunique(),'DataType':dataset.dtypes,'NullValues':dataset.isnull().sum()})\n",
    "    return info\n",
    "\n",
    "def preprocess(train):\n",
    "    sc = StandardScaler()\n",
    "    train = sc.fit_transform(train)\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train[train.Annual_Premium < 200000]\n",
    "#train['Annual_Premium']=np.log(train['Annual_Premium'])\n",
    "#train = train.sample(frac=1).reset_index(drop=True)\n",
    "raw_data = train.append(test)\n",
    "raw_data.index = range(len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_age_map = {\"> 2 Years\":3,\"1-2 Year\":2,\"< 1 Year\":1}\n",
    "dataset[\"Vehicle_Age\"] = dataset[\"Vehicle_Age\"].map(vehicle_age_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new features and binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new features and binning\n",
    "\n",
    "l = []\n",
    "for i in raw_data['Age']:\n",
    "    if(i<25):l.append(1)\n",
    "    elif(i>=25 and i<35):l.append(3)\n",
    "    elif(i>=35 and i<50):l.append(4)\n",
    "    elif(i>=50 and i<60):l.append(5)\n",
    "    else:l.append(2)\n",
    "dataset['Salary'] = l\n",
    "\n",
    "Bins_Age = np.linspace(dataset['Age'].min(),dataset['Age'].max(),6)\n",
    "Bins_Annual_Premium = np.linspace(dataset['Annual_Premium'].min(),dataset['Annual_Premium'].max(),4)\n",
    "Bins_Vintage = np.linspace(dataset['Vintage'].min(),dataset['Vintage'].max(),4)\n",
    "\n",
    "dataset['Experience_level'] =  pd.cut(dataset['Age'] , Bins_Age , labels=['one','two','three','four','five'],include_lowest=True)\n",
    "dataset['Category_of_Annual_Premium'] =  pd.cut(dataset['Annual_Premium'] , Bins_Annual_Premium , labels=['low','medium','high'],include_lowest=True)\n",
    "dataset['Time_of_Trust'] = pd.cut(dataset['Vintage'] , Bins_Vintage , labels=['short','average','long'],include_lowest=True)\n",
    "\n",
    "transform_series = dataset.groupby('Vehicle_Age').size()/len(dataset)\n",
    "dataset['Vehicle_Age_ratio']= dataset['Vehicle_Age'].apply(lambda x: transform_series[x])\n",
    "\n",
    "transform_series = dataset.groupby('Policy_Sales_Channel').size()/len(dataset)\n",
    "dataset['Policy_Sales_Channel_ratio']= dataset['Policy_Sales_Channel'].apply(lambda x: transform_series[x])\n",
    "\n",
    "\n",
    "dataset['Experience_level'] =  pd.cut(dataset['Age'] , Bins_Age , labels=['one','two','three','four','five'],include_lowest=True)\n",
    "dataset['Category_of_Annual_Premium'] =  pd.cut(dataset['Annual_Premium'] , Bins_Annual_Premium , labels=['low','medium','high'],include_lowest=True)\n",
    "\n",
    "dataset['Annual_Premium']=np.log(dataset['Annual_Premium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = dataset.groupby(['Gender','Vehicle_Damage']).size() \n",
    "#db2 = dataset.groupby(['Gender']).size()\n",
    "#dataset['Probab_previously_damage_per_gender'] = dataset['Gender'].apply(lambda x:db[x][1]/db2[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_label_encoded = ['Gender','Vehicle_Damage','Category_of_Annual_Premium','Experience_level','Time_of_Trust']\n",
    "for col in columns_to_be_label_encoded:\n",
    "    le = LabelEncoder()\n",
    "    dataset[col] = le.fit_transform(dataset[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Damage_sums_per_region'] = df.groupby(['Region_Code'])['Vehicle_Damage'].transform('sum')\n",
    "df['Mean_premium_per_region'] = df.groupby(['Region_Code'])['Annual_Premium'].transform('mean')\n",
    "df['Count_unique_policy_sales_per_region'] = df.groupby(['Region_Code'])['Policy_Sales_Channel'].transform('nunique')\n",
    "df['Count_policy_sales_per_region'] = df.groupby(['Region_Code'])['Policy_Sales_Channel'].transform('count')\n",
    "df['Mean_vehicle_age_per_region'] = df.groupby(['Region_Code'])['Vehicle_Age'].transform('mean')\n",
    "df['Mean_age_per_region'] = df.groupby(['Region_Code'])['Age'].transform('mean')\n",
    "df['Mean_salary_per_region'] = df.groupby(['Region_Code'])['Salary'].transform('mean')\n",
    "df['Count_previously_insured_per_region'] = df.groupby(['Region_Code'])['Previously_Insured'].transform('sum')\n",
    "df['Mean_vintage_per_region'] = df.groupby(['Region_Code'])['Vintage'].transform('mean')\n",
    "df['Max_premimum_per_region'] = df.groupby(['Region_Code'])['Annual_Premium'].transform('max')\n",
    "df['Max_premimum_per_region'] = df.groupby(['Region_Code'])['Annual_Premium'].transform('min')\n",
    "df[\"Rank_premium_per_region\"] = df.groupby(\"Region_Code\")['Annual_Premium'].rank(method=\"dense\", ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mean_cat_premium_per_experience'] = df.groupby([\"Experience_level\"])['Category_of_Annual_Premium'].transform('mean')\n",
    "df['Mean_premium_per_experience'] = df.groupby([\"Experience_level\"])['Annual_Premium'].transform('mean')\n",
    "df['Mean_salary_per_experience'] = df.groupby([\"Experience_level\"])['Salary'].transform('mean')\n",
    "#df['Mean_vehicle_damage_per_experience'] = df.groupby([\"Experience_level\"])['Vehicle_Damage'].transform('mean')\n",
    "#df['Mean_vehicle_age_per_experience'] = df.groupby([\"Experience_level\"])['Vehicle_Age'].transform('mean')\n",
    "#df['no_of_people_from_regionwise'] = df.groupby(['Region_Code'])['id'].transform('nunique')\n",
    "#df['no_of_people_in_policy_channel'] = df.groupby(['Policy_Sales_Channel'])['id'].transform('count')\n",
    "#df['no_of_people_with_same_associatedDays'] = df.groupby(['Vintage'])['id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"RANK\"] = df.groupby(\"Region_Code\")['Region_Code'].rank(method=\"first\", ascending=True)\n",
    "#df[\"RANK_avg\"] = df.groupby(\"Region_Code\")['Region_Code'].rank(method=\"average\", ascending=True)\n",
    "#df[\"RANK_max\"] = df.groupby(\"Region_Code\")['Region_Code'].rank(method=\"max\", ascending=True)\n",
    "#df[\"RANK_min\"] = df.groupby(\"Region_Code\")['Region_Code'].rank(method=\"min\", ascending=True)\n",
    "#df[\"RANK_DIFF\"] = df['RANK_max'] - df['RANK_min']\n",
    "\n",
    "#df[\"RANK_Vehicle_Age\"] = df.groupby(['Region_Code','Vehicle_Age'])['Region_Code'].rank(method='first',ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dict_map = df['Policy_Sales_Channel'].value_counts().to_dict()\n",
    "\n",
    "#df['feaquency_count_policy']= df['Policy_Sales_Channel'].map(policy_dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "selector = var_thresh.fit(df)\n",
    "\n",
    "df = df[df.columns[selector.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>...</th>\n",
       "      <th>Mean_vehicle_age_per_region</th>\n",
       "      <th>Mean_age_per_region</th>\n",
       "      <th>Mean_salary_per_region</th>\n",
       "      <th>Count_previously_insured_per_region</th>\n",
       "      <th>Mean_vintage_per_region</th>\n",
       "      <th>Max_premimum_per_region</th>\n",
       "      <th>Rank_premium_per_region</th>\n",
       "      <th>Mean_cat_premium_per_experience</th>\n",
       "      <th>Mean_premium_per_experience</th>\n",
       "      <th>Mean_salary_per_experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.607921</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.916738</td>\n",
       "      <td>46.223536</td>\n",
       "      <td>3.422568</td>\n",
       "      <td>44166</td>\n",
       "      <td>154.266618</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>17341.0</td>\n",
       "      <td>1.000221</td>\n",
       "      <td>9.928456</td>\n",
       "      <td>3.944646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10.420375</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.543040</td>\n",
       "      <td>36.980970</td>\n",
       "      <td>2.852053</td>\n",
       "      <td>6000</td>\n",
       "      <td>154.864442</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>5260.0</td>\n",
       "      <td>1.000533</td>\n",
       "      <td>10.123132</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.553049</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.916738</td>\n",
       "      <td>46.223536</td>\n",
       "      <td>3.422568</td>\n",
       "      <td>44166</td>\n",
       "      <td>154.266618</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>15239.0</td>\n",
       "      <td>1.000326</td>\n",
       "      <td>9.999479</td>\n",
       "      <td>4.694271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.261826</td>\n",
       "      <td>152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.419127</td>\n",
       "      <td>34.235480</td>\n",
       "      <td>2.656716</td>\n",
       "      <td>7662</td>\n",
       "      <td>154.520360</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>2961.0</td>\n",
       "      <td>1.000191</td>\n",
       "      <td>10.049680</td>\n",
       "      <td>1.939291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.221796</td>\n",
       "      <td>152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.501475</td>\n",
       "      <td>36.404016</td>\n",
       "      <td>2.869836</td>\n",
       "      <td>13820</td>\n",
       "      <td>154.851189</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>1.000191</td>\n",
       "      <td>10.049680</td>\n",
       "      <td>1.939291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508141</th>\n",
       "      <td>508142</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.337443</td>\n",
       "      <td>152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.339916</td>\n",
       "      <td>32.467792</td>\n",
       "      <td>2.412366</td>\n",
       "      <td>3172</td>\n",
       "      <td>153.735122</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.000191</td>\n",
       "      <td>10.049680</td>\n",
       "      <td>1.939291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508142</th>\n",
       "      <td>508143</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.264652</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.916738</td>\n",
       "      <td>46.223536</td>\n",
       "      <td>3.422568</td>\n",
       "      <td>44166</td>\n",
       "      <td>154.266618</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>5969.0</td>\n",
       "      <td>1.000221</td>\n",
       "      <td>9.928456</td>\n",
       "      <td>3.944646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508143</th>\n",
       "      <td>508144</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.302331</td>\n",
       "      <td>152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.544865</td>\n",
       "      <td>37.140266</td>\n",
       "      <td>2.709375</td>\n",
       "      <td>13820</td>\n",
       "      <td>154.642676</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>6767.0</td>\n",
       "      <td>1.000191</td>\n",
       "      <td>10.049680</td>\n",
       "      <td>1.939291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508144</th>\n",
       "      <td>508145</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11.048904</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.916738</td>\n",
       "      <td>46.223536</td>\n",
       "      <td>3.422568</td>\n",
       "      <td>44166</td>\n",
       "      <td>154.266618</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>34077.0</td>\n",
       "      <td>1.000402</td>\n",
       "      <td>10.048176</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508145</th>\n",
       "      <td>508146</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10.237349</td>\n",
       "      <td>124.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511487</td>\n",
       "      <td>36.256687</td>\n",
       "      <td>2.773833</td>\n",
       "      <td>8771</td>\n",
       "      <td>152.636057</td>\n",
       "      <td>7.874739</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.000221</td>\n",
       "      <td>9.928456</td>\n",
       "      <td>3.944646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508146 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "0            1       1   44                1         28.0                   0   \n",
       "1            2       1   76                1          3.0                   0   \n",
       "2            3       1   47                1         28.0                   0   \n",
       "3            4       1   21                1         11.0                   1   \n",
       "4            5       0   29                1         41.0                   1   \n",
       "...        ...     ...  ...              ...          ...                 ...   \n",
       "508141  508142       0   26                1         37.0                   1   \n",
       "508142  508143       0   38                1         28.0                   0   \n",
       "508143  508144       1   21                1         46.0                   1   \n",
       "508144  508145       1   71                1         28.0                   1   \n",
       "508145  508146       1   41                1         29.0                   1   \n",
       "\n",
       "        Vehicle_Age  Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  \\\n",
       "0                 3               1       10.607921                  26.0   \n",
       "1                 2               0       10.420375                  26.0   \n",
       "2                 3               1       10.553049                  26.0   \n",
       "3                 1               0       10.261826                 152.0   \n",
       "4                 1               0       10.221796                 152.0   \n",
       "...             ...             ...             ...                   ...   \n",
       "508141            1               0       10.337443                 152.0   \n",
       "508142            2               1       10.264652                 122.0   \n",
       "508143            1               0       10.302331                 152.0   \n",
       "508144            2               0       11.048904                  26.0   \n",
       "508145            2               0       10.237349                 124.0   \n",
       "\n",
       "        ...  Mean_vehicle_age_per_region  Mean_age_per_region  \\\n",
       "0       ...                     1.916738            46.223536   \n",
       "1       ...                     1.543040            36.980970   \n",
       "2       ...                     1.916738            46.223536   \n",
       "3       ...                     1.419127            34.235480   \n",
       "4       ...                     1.501475            36.404016   \n",
       "...     ...                          ...                  ...   \n",
       "508141  ...                     1.339916            32.467792   \n",
       "508142  ...                     1.916738            46.223536   \n",
       "508143  ...                     1.544865            37.140266   \n",
       "508144  ...                     1.916738            46.223536   \n",
       "508145  ...                     1.511487            36.256687   \n",
       "\n",
       "        Mean_salary_per_region  Count_previously_insured_per_region  \\\n",
       "0                     3.422568                                44166   \n",
       "1                     2.852053                                 6000   \n",
       "2                     3.422568                                44166   \n",
       "3                     2.656716                                 7662   \n",
       "4                     2.869836                                13820   \n",
       "...                        ...                                  ...   \n",
       "508141                2.412366                                 3172   \n",
       "508142                3.422568                                44166   \n",
       "508143                2.709375                                13820   \n",
       "508144                3.422568                                44166   \n",
       "508145                2.773833                                 8771   \n",
       "\n",
       "        Mean_vintage_per_region  Max_premimum_per_region  \\\n",
       "0                    154.266618                 7.874739   \n",
       "1                    154.864442                 7.874739   \n",
       "2                    154.266618                 7.874739   \n",
       "3                    154.520360                 7.874739   \n",
       "4                    154.851189                 7.874739   \n",
       "...                         ...                      ...   \n",
       "508141               153.735122                 7.874739   \n",
       "508142               154.266618                 7.874739   \n",
       "508143               154.642676                 7.874739   \n",
       "508144               154.266618                 7.874739   \n",
       "508145               152.636057                 7.874739   \n",
       "\n",
       "        Rank_premium_per_region  Mean_cat_premium_per_experience  \\\n",
       "0                       17341.0                         1.000221   \n",
       "1                        5260.0                         1.000533   \n",
       "2                       15239.0                         1.000326   \n",
       "3                        2961.0                         1.000191   \n",
       "4                        3504.0                         1.000191   \n",
       "...                         ...                              ...   \n",
       "508141                   1964.0                         1.000191   \n",
       "508142                   5969.0                         1.000221   \n",
       "508143                   6767.0                         1.000191   \n",
       "508144                  34077.0                         1.000402   \n",
       "508145                   2013.0                         1.000221   \n",
       "\n",
       "        Mean_premium_per_experience  Mean_salary_per_experience  \n",
       "0                          9.928456                    3.944646  \n",
       "1                         10.123132                    2.000000  \n",
       "2                          9.999479                    4.694271  \n",
       "3                         10.049680                    1.939291  \n",
       "4                         10.049680                    1.939291  \n",
       "...                             ...                         ...  \n",
       "508141                    10.049680                    1.939291  \n",
       "508142                     9.928456                    3.944646  \n",
       "508143                    10.049680                    1.939291  \n",
       "508144                    10.048176                    2.000000  \n",
       "508145                     9.928456                    3.944646  \n",
       "\n",
       "[508146 rows x 32 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getInfo(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((381109, 30), (381109,), (127037, 30))"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[df['Response'].isnull()==False]\n",
    "test_df= df[df['Response'].isnull()==True]\n",
    "X = train_df.drop(['Response','id'],axis=1)\n",
    "y = train_df['Response'] \n",
    "X_pred =test_df.drop(['Response','id'],axis=1)\n",
    "X.shape , y.shape , X_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_test_split of the train and the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323942, 30), (57167, 30), (323942,), (57167,))"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.15,random_state =2020,stratify=y)\n",
    "X_train.shape , X_test.shape , y_train.shape , y_test.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employing various Models for the classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = lg.LGBMClassifier(boosting_type='gbdt',\n",
    "                     n_estimators=450,learning_rate=0.0295,\n",
    "                     objective='binary',metric='auc',is_unbalance=True,\n",
    "                     colsample_bytree=0.5,reg_lambda=10,\n",
    "                     reg_alpha=2,random_state=294,n_jobs=-1,seed=8798,max_depth=7 , max_bin=None,\n",
    "                        early_stopping=20\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.742962\tvalid_0's auc: 0.740566\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\ttraining's auc: 0.841237\tvalid_0's auc: 0.839023\n",
      "[3]\ttraining's auc: 0.848368\tvalid_0's auc: 0.84689\n",
      "[4]\ttraining's auc: 0.845824\tvalid_0's auc: 0.844199\n",
      "[5]\ttraining's auc: 0.848226\tvalid_0's auc: 0.846625\n",
      "[6]\ttraining's auc: 0.848688\tvalid_0's auc: 0.84734\n",
      "[7]\ttraining's auc: 0.851282\tvalid_0's auc: 0.849877\n",
      "[8]\ttraining's auc: 0.852922\tvalid_0's auc: 0.85162\n",
      "[9]\ttraining's auc: 0.853052\tvalid_0's auc: 0.851747\n",
      "[10]\ttraining's auc: 0.853661\tvalid_0's auc: 0.852427\n",
      "[11]\ttraining's auc: 0.854018\tvalid_0's auc: 0.852689\n",
      "[12]\ttraining's auc: 0.854218\tvalid_0's auc: 0.852908\n",
      "[13]\ttraining's auc: 0.854118\tvalid_0's auc: 0.85283\n",
      "[14]\ttraining's auc: 0.854247\tvalid_0's auc: 0.852945\n",
      "[15]\ttraining's auc: 0.854412\tvalid_0's auc: 0.853084\n",
      "[16]\ttraining's auc: 0.854565\tvalid_0's auc: 0.853175\n",
      "[17]\ttraining's auc: 0.85495\tvalid_0's auc: 0.853579\n",
      "[18]\ttraining's auc: 0.854853\tvalid_0's auc: 0.853403\n",
      "[19]\ttraining's auc: 0.854863\tvalid_0's auc: 0.853412\n",
      "[20]\ttraining's auc: 0.854876\tvalid_0's auc: 0.853416\n",
      "[21]\ttraining's auc: 0.855124\tvalid_0's auc: 0.853703\n",
      "[22]\ttraining's auc: 0.855334\tvalid_0's auc: 0.853979\n",
      "[23]\ttraining's auc: 0.855348\tvalid_0's auc: 0.853965\n",
      "[24]\ttraining's auc: 0.855391\tvalid_0's auc: 0.853978\n",
      "[25]\ttraining's auc: 0.85559\tvalid_0's auc: 0.854215\n",
      "[26]\ttraining's auc: 0.855654\tvalid_0's auc: 0.854215\n",
      "[27]\ttraining's auc: 0.85557\tvalid_0's auc: 0.854121\n",
      "[28]\ttraining's auc: 0.855504\tvalid_0's auc: 0.854016\n",
      "[29]\ttraining's auc: 0.85571\tvalid_0's auc: 0.854227\n",
      "[30]\ttraining's auc: 0.85584\tvalid_0's auc: 0.854402\n",
      "[31]\ttraining's auc: 0.85592\tvalid_0's auc: 0.854439\n",
      "[32]\ttraining's auc: 0.856064\tvalid_0's auc: 0.854611\n",
      "[33]\ttraining's auc: 0.856128\tvalid_0's auc: 0.854646\n",
      "[34]\ttraining's auc: 0.856171\tvalid_0's auc: 0.854719\n",
      "[35]\ttraining's auc: 0.856234\tvalid_0's auc: 0.854822\n",
      "[36]\ttraining's auc: 0.856331\tvalid_0's auc: 0.854952\n",
      "[37]\ttraining's auc: 0.856461\tvalid_0's auc: 0.855093\n",
      "[38]\ttraining's auc: 0.856471\tvalid_0's auc: 0.855075\n",
      "[39]\ttraining's auc: 0.856537\tvalid_0's auc: 0.85516\n",
      "[40]\ttraining's auc: 0.856613\tvalid_0's auc: 0.855247\n",
      "[41]\ttraining's auc: 0.856685\tvalid_0's auc: 0.855296\n",
      "[42]\ttraining's auc: 0.856726\tvalid_0's auc: 0.855321\n",
      "[43]\ttraining's auc: 0.856764\tvalid_0's auc: 0.855347\n",
      "[44]\ttraining's auc: 0.856792\tvalid_0's auc: 0.855368\n",
      "[45]\ttraining's auc: 0.856818\tvalid_0's auc: 0.855418\n",
      "[46]\ttraining's auc: 0.856841\tvalid_0's auc: 0.855414\n",
      "[47]\ttraining's auc: 0.85681\tvalid_0's auc: 0.855335\n",
      "[48]\ttraining's auc: 0.856881\tvalid_0's auc: 0.855369\n",
      "[49]\ttraining's auc: 0.85692\tvalid_0's auc: 0.855433\n",
      "[50]\ttraining's auc: 0.856972\tvalid_0's auc: 0.855502\n",
      "[51]\ttraining's auc: 0.856969\tvalid_0's auc: 0.855475\n",
      "[52]\ttraining's auc: 0.857029\tvalid_0's auc: 0.855501\n",
      "[53]\ttraining's auc: 0.857087\tvalid_0's auc: 0.855526\n",
      "[54]\ttraining's auc: 0.857148\tvalid_0's auc: 0.855591\n",
      "[55]\ttraining's auc: 0.857203\tvalid_0's auc: 0.85563\n",
      "[56]\ttraining's auc: 0.857261\tvalid_0's auc: 0.855655\n",
      "[57]\ttraining's auc: 0.857324\tvalid_0's auc: 0.85566\n",
      "[58]\ttraining's auc: 0.857361\tvalid_0's auc: 0.855714\n",
      "[59]\ttraining's auc: 0.857407\tvalid_0's auc: 0.855756\n",
      "[60]\ttraining's auc: 0.857421\tvalid_0's auc: 0.85577\n",
      "[61]\ttraining's auc: 0.857475\tvalid_0's auc: 0.855826\n",
      "[62]\ttraining's auc: 0.857501\tvalid_0's auc: 0.855844\n",
      "[63]\ttraining's auc: 0.857531\tvalid_0's auc: 0.855875\n",
      "[64]\ttraining's auc: 0.857554\tvalid_0's auc: 0.8559\n",
      "[65]\ttraining's auc: 0.857602\tvalid_0's auc: 0.855951\n",
      "[66]\ttraining's auc: 0.857629\tvalid_0's auc: 0.85595\n",
      "[67]\ttraining's auc: 0.857692\tvalid_0's auc: 0.855993\n",
      "[68]\ttraining's auc: 0.857753\tvalid_0's auc: 0.856044\n",
      "[69]\ttraining's auc: 0.857815\tvalid_0's auc: 0.856094\n",
      "[70]\ttraining's auc: 0.857875\tvalid_0's auc: 0.856156\n",
      "[71]\ttraining's auc: 0.857888\tvalid_0's auc: 0.856156\n",
      "[72]\ttraining's auc: 0.857905\tvalid_0's auc: 0.856163\n",
      "[73]\ttraining's auc: 0.857921\tvalid_0's auc: 0.856181\n",
      "[74]\ttraining's auc: 0.857984\tvalid_0's auc: 0.856241\n",
      "[75]\ttraining's auc: 0.858057\tvalid_0's auc: 0.856286\n",
      "[76]\ttraining's auc: 0.858086\tvalid_0's auc: 0.856331\n",
      "[77]\ttraining's auc: 0.858156\tvalid_0's auc: 0.856363\n",
      "[78]\ttraining's auc: 0.858201\tvalid_0's auc: 0.856396\n",
      "[79]\ttraining's auc: 0.858228\tvalid_0's auc: 0.856422\n",
      "[80]\ttraining's auc: 0.858264\tvalid_0's auc: 0.856453\n",
      "[81]\ttraining's auc: 0.858299\tvalid_0's auc: 0.856454\n",
      "[82]\ttraining's auc: 0.858343\tvalid_0's auc: 0.856491\n",
      "[83]\ttraining's auc: 0.858376\tvalid_0's auc: 0.856529\n",
      "[84]\ttraining's auc: 0.858431\tvalid_0's auc: 0.856558\n",
      "[85]\ttraining's auc: 0.858461\tvalid_0's auc: 0.856566\n",
      "[86]\ttraining's auc: 0.858515\tvalid_0's auc: 0.856593\n",
      "[87]\ttraining's auc: 0.858544\tvalid_0's auc: 0.856591\n",
      "[88]\ttraining's auc: 0.858569\tvalid_0's auc: 0.856608\n",
      "[89]\ttraining's auc: 0.858579\tvalid_0's auc: 0.856618\n",
      "[90]\ttraining's auc: 0.858607\tvalid_0's auc: 0.856629\n",
      "[91]\ttraining's auc: 0.858632\tvalid_0's auc: 0.856661\n",
      "[92]\ttraining's auc: 0.858728\tvalid_0's auc: 0.856732\n",
      "[93]\ttraining's auc: 0.858749\tvalid_0's auc: 0.856756\n",
      "[94]\ttraining's auc: 0.858769\tvalid_0's auc: 0.856763\n",
      "[95]\ttraining's auc: 0.858804\tvalid_0's auc: 0.85677\n",
      "[96]\ttraining's auc: 0.858879\tvalid_0's auc: 0.856826\n",
      "[97]\ttraining's auc: 0.858915\tvalid_0's auc: 0.85685\n",
      "[98]\ttraining's auc: 0.858928\tvalid_0's auc: 0.856859\n",
      "[99]\ttraining's auc: 0.858948\tvalid_0's auc: 0.856868\n",
      "[100]\ttraining's auc: 0.858994\tvalid_0's auc: 0.856894\n",
      "[101]\ttraining's auc: 0.859034\tvalid_0's auc: 0.856911\n",
      "[102]\ttraining's auc: 0.859073\tvalid_0's auc: 0.856947\n",
      "[103]\ttraining's auc: 0.859104\tvalid_0's auc: 0.856959\n",
      "[104]\ttraining's auc: 0.859114\tvalid_0's auc: 0.856962\n",
      "[105]\ttraining's auc: 0.859172\tvalid_0's auc: 0.857005\n",
      "[106]\ttraining's auc: 0.859195\tvalid_0's auc: 0.857019\n",
      "[107]\ttraining's auc: 0.859209\tvalid_0's auc: 0.857021\n",
      "[108]\ttraining's auc: 0.859248\tvalid_0's auc: 0.857031\n",
      "[109]\ttraining's auc: 0.859273\tvalid_0's auc: 0.857015\n",
      "[110]\ttraining's auc: 0.859287\tvalid_0's auc: 0.857032\n",
      "[111]\ttraining's auc: 0.85932\tvalid_0's auc: 0.857046\n",
      "[112]\ttraining's auc: 0.859337\tvalid_0's auc: 0.857069\n",
      "[113]\ttraining's auc: 0.859361\tvalid_0's auc: 0.857087\n",
      "[114]\ttraining's auc: 0.859373\tvalid_0's auc: 0.857111\n",
      "[115]\ttraining's auc: 0.859388\tvalid_0's auc: 0.857117\n",
      "[116]\ttraining's auc: 0.859431\tvalid_0's auc: 0.857132\n",
      "[117]\ttraining's auc: 0.859448\tvalid_0's auc: 0.857137\n",
      "[118]\ttraining's auc: 0.859461\tvalid_0's auc: 0.85714\n",
      "[119]\ttraining's auc: 0.859475\tvalid_0's auc: 0.85715\n",
      "[120]\ttraining's auc: 0.859492\tvalid_0's auc: 0.857166\n",
      "[121]\ttraining's auc: 0.859523\tvalid_0's auc: 0.857168\n",
      "[122]\ttraining's auc: 0.859542\tvalid_0's auc: 0.85718\n",
      "[123]\ttraining's auc: 0.859559\tvalid_0's auc: 0.857189\n",
      "[124]\ttraining's auc: 0.859577\tvalid_0's auc: 0.857214\n",
      "[125]\ttraining's auc: 0.859605\tvalid_0's auc: 0.857221\n",
      "[126]\ttraining's auc: 0.859654\tvalid_0's auc: 0.857256\n",
      "[127]\ttraining's auc: 0.859691\tvalid_0's auc: 0.857265\n",
      "[128]\ttraining's auc: 0.859703\tvalid_0's auc: 0.857276\n",
      "[129]\ttraining's auc: 0.859723\tvalid_0's auc: 0.857269\n",
      "[130]\ttraining's auc: 0.859774\tvalid_0's auc: 0.857298\n",
      "[131]\ttraining's auc: 0.85979\tvalid_0's auc: 0.85729\n",
      "[132]\ttraining's auc: 0.859825\tvalid_0's auc: 0.857307\n",
      "[133]\ttraining's auc: 0.859862\tvalid_0's auc: 0.857328\n",
      "[134]\ttraining's auc: 0.859893\tvalid_0's auc: 0.857348\n",
      "[135]\ttraining's auc: 0.859917\tvalid_0's auc: 0.857366\n",
      "[136]\ttraining's auc: 0.859937\tvalid_0's auc: 0.857358\n",
      "[137]\ttraining's auc: 0.859982\tvalid_0's auc: 0.857402\n",
      "[138]\ttraining's auc: 0.860016\tvalid_0's auc: 0.857413\n",
      "[139]\ttraining's auc: 0.860039\tvalid_0's auc: 0.857423\n",
      "[140]\ttraining's auc: 0.860074\tvalid_0's auc: 0.857432\n",
      "[141]\ttraining's auc: 0.860111\tvalid_0's auc: 0.857464\n",
      "[142]\ttraining's auc: 0.860147\tvalid_0's auc: 0.857488\n",
      "[143]\ttraining's auc: 0.860159\tvalid_0's auc: 0.857473\n",
      "[144]\ttraining's auc: 0.86017\tvalid_0's auc: 0.857476\n",
      "[145]\ttraining's auc: 0.860203\tvalid_0's auc: 0.85749\n",
      "[146]\ttraining's auc: 0.860218\tvalid_0's auc: 0.857496\n",
      "[147]\ttraining's auc: 0.860223\tvalid_0's auc: 0.857495\n",
      "[148]\ttraining's auc: 0.860249\tvalid_0's auc: 0.857513\n",
      "[149]\ttraining's auc: 0.86028\tvalid_0's auc: 0.85755\n",
      "[150]\ttraining's auc: 0.860296\tvalid_0's auc: 0.857564\n",
      "[151]\ttraining's auc: 0.860308\tvalid_0's auc: 0.857572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152]\ttraining's auc: 0.860329\tvalid_0's auc: 0.857575\n",
      "[153]\ttraining's auc: 0.860358\tvalid_0's auc: 0.857582\n",
      "[154]\ttraining's auc: 0.860376\tvalid_0's auc: 0.857576\n",
      "[155]\ttraining's auc: 0.860391\tvalid_0's auc: 0.857587\n",
      "[156]\ttraining's auc: 0.860409\tvalid_0's auc: 0.857592\n",
      "[157]\ttraining's auc: 0.860426\tvalid_0's auc: 0.857603\n",
      "[158]\ttraining's auc: 0.860439\tvalid_0's auc: 0.857615\n",
      "[159]\ttraining's auc: 0.860462\tvalid_0's auc: 0.857629\n",
      "[160]\ttraining's auc: 0.860474\tvalid_0's auc: 0.857626\n",
      "[161]\ttraining's auc: 0.8605\tvalid_0's auc: 0.857648\n",
      "[162]\ttraining's auc: 0.860526\tvalid_0's auc: 0.85765\n",
      "[163]\ttraining's auc: 0.86054\tvalid_0's auc: 0.857647\n",
      "[164]\ttraining's auc: 0.860558\tvalid_0's auc: 0.857657\n",
      "[165]\ttraining's auc: 0.860572\tvalid_0's auc: 0.857662\n",
      "[166]\ttraining's auc: 0.8606\tvalid_0's auc: 0.857683\n",
      "[167]\ttraining's auc: 0.860646\tvalid_0's auc: 0.857724\n",
      "[168]\ttraining's auc: 0.860667\tvalid_0's auc: 0.857729\n",
      "[169]\ttraining's auc: 0.860685\tvalid_0's auc: 0.857734\n",
      "[170]\ttraining's auc: 0.860709\tvalid_0's auc: 0.857733\n",
      "[171]\ttraining's auc: 0.860718\tvalid_0's auc: 0.857732\n",
      "[172]\ttraining's auc: 0.860728\tvalid_0's auc: 0.857736\n",
      "[173]\ttraining's auc: 0.860775\tvalid_0's auc: 0.857773\n",
      "[174]\ttraining's auc: 0.860797\tvalid_0's auc: 0.857789\n",
      "[175]\ttraining's auc: 0.860833\tvalid_0's auc: 0.857827\n",
      "[176]\ttraining's auc: 0.860852\tvalid_0's auc: 0.857825\n",
      "[177]\ttraining's auc: 0.860873\tvalid_0's auc: 0.857833\n",
      "[178]\ttraining's auc: 0.860899\tvalid_0's auc: 0.857844\n",
      "[179]\ttraining's auc: 0.860903\tvalid_0's auc: 0.857841\n",
      "[180]\ttraining's auc: 0.860916\tvalid_0's auc: 0.85785\n",
      "[181]\ttraining's auc: 0.860919\tvalid_0's auc: 0.857853\n",
      "[182]\ttraining's auc: 0.860935\tvalid_0's auc: 0.85786\n",
      "[183]\ttraining's auc: 0.860966\tvalid_0's auc: 0.85788\n",
      "[184]\ttraining's auc: 0.860983\tvalid_0's auc: 0.857882\n",
      "[185]\ttraining's auc: 0.86103\tvalid_0's auc: 0.857904\n",
      "[186]\ttraining's auc: 0.861039\tvalid_0's auc: 0.857919\n",
      "[187]\ttraining's auc: 0.861061\tvalid_0's auc: 0.857915\n",
      "[188]\ttraining's auc: 0.861075\tvalid_0's auc: 0.857921\n",
      "[189]\ttraining's auc: 0.861109\tvalid_0's auc: 0.857936\n",
      "[190]\ttraining's auc: 0.861125\tvalid_0's auc: 0.857944\n",
      "[191]\ttraining's auc: 0.861139\tvalid_0's auc: 0.857955\n",
      "[192]\ttraining's auc: 0.861161\tvalid_0's auc: 0.857959\n",
      "[193]\ttraining's auc: 0.861175\tvalid_0's auc: 0.857965\n",
      "[194]\ttraining's auc: 0.86118\tvalid_0's auc: 0.857963\n",
      "[195]\ttraining's auc: 0.861225\tvalid_0's auc: 0.857996\n",
      "[196]\ttraining's auc: 0.861231\tvalid_0's auc: 0.857998\n",
      "[197]\ttraining's auc: 0.861252\tvalid_0's auc: 0.858009\n",
      "[198]\ttraining's auc: 0.861276\tvalid_0's auc: 0.858022\n",
      "[199]\ttraining's auc: 0.861314\tvalid_0's auc: 0.858039\n",
      "[200]\ttraining's auc: 0.861335\tvalid_0's auc: 0.858056\n",
      "[201]\ttraining's auc: 0.861353\tvalid_0's auc: 0.858067\n",
      "[202]\ttraining's auc: 0.861383\tvalid_0's auc: 0.85807\n",
      "[203]\ttraining's auc: 0.861399\tvalid_0's auc: 0.858078\n",
      "[204]\ttraining's auc: 0.861418\tvalid_0's auc: 0.858076\n",
      "[205]\ttraining's auc: 0.861439\tvalid_0's auc: 0.858085\n",
      "[206]\ttraining's auc: 0.861457\tvalid_0's auc: 0.858098\n",
      "[207]\ttraining's auc: 0.861488\tvalid_0's auc: 0.858101\n",
      "[208]\ttraining's auc: 0.861494\tvalid_0's auc: 0.858104\n",
      "[209]\ttraining's auc: 0.861498\tvalid_0's auc: 0.858105\n",
      "[210]\ttraining's auc: 0.861503\tvalid_0's auc: 0.858104\n",
      "[211]\ttraining's auc: 0.861511\tvalid_0's auc: 0.858111\n",
      "[212]\ttraining's auc: 0.861517\tvalid_0's auc: 0.858114\n",
      "[213]\ttraining's auc: 0.861521\tvalid_0's auc: 0.858114\n",
      "[214]\ttraining's auc: 0.861536\tvalid_0's auc: 0.858119\n",
      "[215]\ttraining's auc: 0.861541\tvalid_0's auc: 0.858124\n",
      "[216]\ttraining's auc: 0.861551\tvalid_0's auc: 0.858125\n",
      "[217]\ttraining's auc: 0.86156\tvalid_0's auc: 0.858131\n",
      "[218]\ttraining's auc: 0.861572\tvalid_0's auc: 0.858133\n",
      "[219]\ttraining's auc: 0.861578\tvalid_0's auc: 0.858136\n",
      "[220]\ttraining's auc: 0.861586\tvalid_0's auc: 0.858147\n",
      "[221]\ttraining's auc: 0.861591\tvalid_0's auc: 0.858148\n",
      "[222]\ttraining's auc: 0.861597\tvalid_0's auc: 0.858151\n",
      "[223]\ttraining's auc: 0.86161\tvalid_0's auc: 0.858157\n",
      "[224]\ttraining's auc: 0.861623\tvalid_0's auc: 0.858161\n",
      "[225]\ttraining's auc: 0.861641\tvalid_0's auc: 0.858174\n",
      "[226]\ttraining's auc: 0.861647\tvalid_0's auc: 0.858171\n",
      "[227]\ttraining's auc: 0.861651\tvalid_0's auc: 0.858169\n",
      "[228]\ttraining's auc: 0.861679\tvalid_0's auc: 0.858174\n",
      "[229]\ttraining's auc: 0.861683\tvalid_0's auc: 0.858177\n",
      "[230]\ttraining's auc: 0.861686\tvalid_0's auc: 0.858175\n",
      "[231]\ttraining's auc: 0.861696\tvalid_0's auc: 0.858183\n",
      "[232]\ttraining's auc: 0.861699\tvalid_0's auc: 0.858183\n",
      "[233]\ttraining's auc: 0.861706\tvalid_0's auc: 0.858183\n",
      "[234]\ttraining's auc: 0.861723\tvalid_0's auc: 0.858182\n",
      "[235]\ttraining's auc: 0.86173\tvalid_0's auc: 0.85818\n",
      "[236]\ttraining's auc: 0.861744\tvalid_0's auc: 0.858186\n",
      "[237]\ttraining's auc: 0.861752\tvalid_0's auc: 0.858187\n",
      "[238]\ttraining's auc: 0.861769\tvalid_0's auc: 0.858193\n",
      "[239]\ttraining's auc: 0.861774\tvalid_0's auc: 0.858194\n",
      "[240]\ttraining's auc: 0.861811\tvalid_0's auc: 0.858216\n",
      "[241]\ttraining's auc: 0.86184\tvalid_0's auc: 0.858237\n",
      "[242]\ttraining's auc: 0.861869\tvalid_0's auc: 0.858259\n",
      "[243]\ttraining's auc: 0.861879\tvalid_0's auc: 0.858254\n",
      "[244]\ttraining's auc: 0.861903\tvalid_0's auc: 0.858262\n",
      "[245]\ttraining's auc: 0.861905\tvalid_0's auc: 0.858264\n",
      "[246]\ttraining's auc: 0.861921\tvalid_0's auc: 0.858267\n",
      "[247]\ttraining's auc: 0.861924\tvalid_0's auc: 0.858267\n",
      "[248]\ttraining's auc: 0.861928\tvalid_0's auc: 0.858269\n",
      "[249]\ttraining's auc: 0.861932\tvalid_0's auc: 0.858271\n",
      "[250]\ttraining's auc: 0.861947\tvalid_0's auc: 0.85828\n",
      "[251]\ttraining's auc: 0.861966\tvalid_0's auc: 0.858285\n",
      "[252]\ttraining's auc: 0.861991\tvalid_0's auc: 0.858301\n",
      "[253]\ttraining's auc: 0.862001\tvalid_0's auc: 0.858304\n",
      "[254]\ttraining's auc: 0.862019\tvalid_0's auc: 0.858309\n",
      "[255]\ttraining's auc: 0.862023\tvalid_0's auc: 0.858308\n",
      "[256]\ttraining's auc: 0.862026\tvalid_0's auc: 0.858309\n",
      "[257]\ttraining's auc: 0.86203\tvalid_0's auc: 0.858308\n",
      "[258]\ttraining's auc: 0.862034\tvalid_0's auc: 0.858309\n",
      "[259]\ttraining's auc: 0.862035\tvalid_0's auc: 0.858311\n",
      "[260]\ttraining's auc: 0.862055\tvalid_0's auc: 0.85831\n",
      "[261]\ttraining's auc: 0.862058\tvalid_0's auc: 0.85831\n",
      "[262]\ttraining's auc: 0.862062\tvalid_0's auc: 0.858309\n",
      "[263]\ttraining's auc: 0.862084\tvalid_0's auc: 0.85832\n",
      "[264]\ttraining's auc: 0.862088\tvalid_0's auc: 0.85832\n",
      "[265]\ttraining's auc: 0.862095\tvalid_0's auc: 0.858324\n",
      "[266]\ttraining's auc: 0.862099\tvalid_0's auc: 0.858326\n",
      "[267]\ttraining's auc: 0.862104\tvalid_0's auc: 0.858324\n",
      "[268]\ttraining's auc: 0.862126\tvalid_0's auc: 0.858337\n",
      "[269]\ttraining's auc: 0.862146\tvalid_0's auc: 0.85834\n",
      "[270]\ttraining's auc: 0.862153\tvalid_0's auc: 0.85835\n",
      "[271]\ttraining's auc: 0.862173\tvalid_0's auc: 0.858359\n",
      "[272]\ttraining's auc: 0.862177\tvalid_0's auc: 0.85836\n",
      "[273]\ttraining's auc: 0.862179\tvalid_0's auc: 0.858359\n",
      "[274]\ttraining's auc: 0.862203\tvalid_0's auc: 0.858359\n",
      "[275]\ttraining's auc: 0.86222\tvalid_0's auc: 0.858361\n",
      "[276]\ttraining's auc: 0.862222\tvalid_0's auc: 0.858362\n",
      "[277]\ttraining's auc: 0.862225\tvalid_0's auc: 0.858363\n",
      "[278]\ttraining's auc: 0.862233\tvalid_0's auc: 0.858371\n",
      "[279]\ttraining's auc: 0.862256\tvalid_0's auc: 0.858374\n",
      "[280]\ttraining's auc: 0.862267\tvalid_0's auc: 0.858373\n",
      "[281]\ttraining's auc: 0.86228\tvalid_0's auc: 0.858377\n",
      "[282]\ttraining's auc: 0.862283\tvalid_0's auc: 0.858375\n",
      "[283]\ttraining's auc: 0.862302\tvalid_0's auc: 0.858382\n",
      "[284]\ttraining's auc: 0.862319\tvalid_0's auc: 0.858388\n",
      "[285]\ttraining's auc: 0.862323\tvalid_0's auc: 0.858385\n",
      "[286]\ttraining's auc: 0.862324\tvalid_0's auc: 0.858387\n",
      "[287]\ttraining's auc: 0.862339\tvalid_0's auc: 0.8584\n",
      "[288]\ttraining's auc: 0.86236\tvalid_0's auc: 0.858403\n",
      "[289]\ttraining's auc: 0.862365\tvalid_0's auc: 0.858406\n",
      "[290]\ttraining's auc: 0.862381\tvalid_0's auc: 0.858413\n",
      "[291]\ttraining's auc: 0.862398\tvalid_0's auc: 0.85842\n",
      "[292]\ttraining's auc: 0.862411\tvalid_0's auc: 0.858424\n",
      "[293]\ttraining's auc: 0.862437\tvalid_0's auc: 0.858434\n",
      "[294]\ttraining's auc: 0.86244\tvalid_0's auc: 0.85843\n",
      "[295]\ttraining's auc: 0.862443\tvalid_0's auc: 0.858429\n",
      "[296]\ttraining's auc: 0.862448\tvalid_0's auc: 0.858433\n",
      "[297]\ttraining's auc: 0.862467\tvalid_0's auc: 0.85843\n",
      "[298]\ttraining's auc: 0.86247\tvalid_0's auc: 0.858429\n",
      "[299]\ttraining's auc: 0.862489\tvalid_0's auc: 0.858435\n",
      "[300]\ttraining's auc: 0.862493\tvalid_0's auc: 0.858436\n",
      "[301]\ttraining's auc: 0.862495\tvalid_0's auc: 0.858435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302]\ttraining's auc: 0.862507\tvalid_0's auc: 0.858436\n",
      "[303]\ttraining's auc: 0.86251\tvalid_0's auc: 0.858434\n",
      "[304]\ttraining's auc: 0.862513\tvalid_0's auc: 0.858436\n",
      "[305]\ttraining's auc: 0.862515\tvalid_0's auc: 0.858434\n",
      "[306]\ttraining's auc: 0.862516\tvalid_0's auc: 0.858436\n",
      "[307]\ttraining's auc: 0.862519\tvalid_0's auc: 0.858434\n",
      "[308]\ttraining's auc: 0.862531\tvalid_0's auc: 0.858437\n",
      "[309]\ttraining's auc: 0.862545\tvalid_0's auc: 0.858434\n",
      "[310]\ttraining's auc: 0.862548\tvalid_0's auc: 0.858433\n",
      "[311]\ttraining's auc: 0.862566\tvalid_0's auc: 0.858425\n",
      "[312]\ttraining's auc: 0.86259\tvalid_0's auc: 0.858434\n",
      "[313]\ttraining's auc: 0.862609\tvalid_0's auc: 0.858442\n",
      "[314]\ttraining's auc: 0.862616\tvalid_0's auc: 0.858447\n",
      "[315]\ttraining's auc: 0.862619\tvalid_0's auc: 0.858446\n",
      "[316]\ttraining's auc: 0.862634\tvalid_0's auc: 0.858452\n",
      "[317]\ttraining's auc: 0.862637\tvalid_0's auc: 0.858453\n",
      "[318]\ttraining's auc: 0.862639\tvalid_0's auc: 0.858454\n",
      "[319]\ttraining's auc: 0.862669\tvalid_0's auc: 0.858459\n",
      "[320]\ttraining's auc: 0.862671\tvalid_0's auc: 0.858458\n",
      "[321]\ttraining's auc: 0.862691\tvalid_0's auc: 0.858469\n",
      "[322]\ttraining's auc: 0.862709\tvalid_0's auc: 0.858475\n",
      "[323]\ttraining's auc: 0.862721\tvalid_0's auc: 0.858479\n",
      "[324]\ttraining's auc: 0.862735\tvalid_0's auc: 0.858481\n",
      "[325]\ttraining's auc: 0.862737\tvalid_0's auc: 0.85848\n",
      "[326]\ttraining's auc: 0.862749\tvalid_0's auc: 0.858486\n",
      "[327]\ttraining's auc: 0.862752\tvalid_0's auc: 0.858484\n",
      "[328]\ttraining's auc: 0.86278\tvalid_0's auc: 0.858482\n",
      "[329]\ttraining's auc: 0.862802\tvalid_0's auc: 0.858487\n",
      "[330]\ttraining's auc: 0.862804\tvalid_0's auc: 0.858484\n",
      "[331]\ttraining's auc: 0.86281\tvalid_0's auc: 0.858485\n",
      "[332]\ttraining's auc: 0.862823\tvalid_0's auc: 0.858496\n",
      "[333]\ttraining's auc: 0.862825\tvalid_0's auc: 0.858499\n",
      "[334]\ttraining's auc: 0.862842\tvalid_0's auc: 0.858508\n",
      "[335]\ttraining's auc: 0.862858\tvalid_0's auc: 0.858516\n",
      "[336]\ttraining's auc: 0.862866\tvalid_0's auc: 0.858513\n",
      "[337]\ttraining's auc: 0.862869\tvalid_0's auc: 0.858514\n",
      "[338]\ttraining's auc: 0.862886\tvalid_0's auc: 0.858523\n",
      "[339]\ttraining's auc: 0.86291\tvalid_0's auc: 0.858541\n",
      "[340]\ttraining's auc: 0.862914\tvalid_0's auc: 0.85854\n",
      "[341]\ttraining's auc: 0.862949\tvalid_0's auc: 0.858543\n",
      "[342]\ttraining's auc: 0.862958\tvalid_0's auc: 0.858547\n",
      "[343]\ttraining's auc: 0.86298\tvalid_0's auc: 0.858557\n",
      "[344]\ttraining's auc: 0.862984\tvalid_0's auc: 0.858559\n",
      "[345]\ttraining's auc: 0.862988\tvalid_0's auc: 0.858558\n",
      "[346]\ttraining's auc: 0.863007\tvalid_0's auc: 0.858554\n",
      "[347]\ttraining's auc: 0.86301\tvalid_0's auc: 0.858552\n",
      "[348]\ttraining's auc: 0.863012\tvalid_0's auc: 0.858554\n",
      "[349]\ttraining's auc: 0.863025\tvalid_0's auc: 0.858558\n",
      "[350]\ttraining's auc: 0.863027\tvalid_0's auc: 0.858557\n",
      "[351]\ttraining's auc: 0.863048\tvalid_0's auc: 0.858557\n",
      "[352]\ttraining's auc: 0.863049\tvalid_0's auc: 0.858557\n",
      "[353]\ttraining's auc: 0.863053\tvalid_0's auc: 0.858556\n",
      "[354]\ttraining's auc: 0.863065\tvalid_0's auc: 0.858557\n",
      "[355]\ttraining's auc: 0.86309\tvalid_0's auc: 0.858562\n",
      "[356]\ttraining's auc: 0.863113\tvalid_0's auc: 0.858556\n",
      "[357]\ttraining's auc: 0.863127\tvalid_0's auc: 0.858562\n",
      "[358]\ttraining's auc: 0.863129\tvalid_0's auc: 0.858563\n",
      "[359]\ttraining's auc: 0.863141\tvalid_0's auc: 0.858564\n",
      "[360]\ttraining's auc: 0.863154\tvalid_0's auc: 0.858564\n",
      "[361]\ttraining's auc: 0.863176\tvalid_0's auc: 0.858554\n",
      "[362]\ttraining's auc: 0.86319\tvalid_0's auc: 0.858553\n",
      "[363]\ttraining's auc: 0.863198\tvalid_0's auc: 0.858559\n",
      "[364]\ttraining's auc: 0.863205\tvalid_0's auc: 0.858558\n",
      "[365]\ttraining's auc: 0.863235\tvalid_0's auc: 0.858577\n",
      "[366]\ttraining's auc: 0.863256\tvalid_0's auc: 0.858579\n",
      "[367]\ttraining's auc: 0.863269\tvalid_0's auc: 0.858589\n",
      "[368]\ttraining's auc: 0.863298\tvalid_0's auc: 0.858586\n",
      "[369]\ttraining's auc: 0.863334\tvalid_0's auc: 0.858588\n",
      "[370]\ttraining's auc: 0.863354\tvalid_0's auc: 0.858593\n",
      "[371]\ttraining's auc: 0.863361\tvalid_0's auc: 0.858592\n",
      "[372]\ttraining's auc: 0.863372\tvalid_0's auc: 0.8586\n",
      "[373]\ttraining's auc: 0.8634\tvalid_0's auc: 0.858596\n",
      "[374]\ttraining's auc: 0.863419\tvalid_0's auc: 0.858592\n",
      "[375]\ttraining's auc: 0.863436\tvalid_0's auc: 0.858595\n",
      "[376]\ttraining's auc: 0.863444\tvalid_0's auc: 0.858595\n",
      "[377]\ttraining's auc: 0.863459\tvalid_0's auc: 0.8586\n",
      "[378]\ttraining's auc: 0.863475\tvalid_0's auc: 0.858595\n",
      "[379]\ttraining's auc: 0.863492\tvalid_0's auc: 0.858601\n",
      "[380]\ttraining's auc: 0.863494\tvalid_0's auc: 0.858602\n",
      "[381]\ttraining's auc: 0.86351\tvalid_0's auc: 0.858601\n",
      "[382]\ttraining's auc: 0.863522\tvalid_0's auc: 0.858604\n",
      "[383]\ttraining's auc: 0.863546\tvalid_0's auc: 0.85861\n",
      "[384]\ttraining's auc: 0.863568\tvalid_0's auc: 0.858611\n",
      "[385]\ttraining's auc: 0.863585\tvalid_0's auc: 0.858615\n",
      "[386]\ttraining's auc: 0.863593\tvalid_0's auc: 0.85861\n",
      "[387]\ttraining's auc: 0.863625\tvalid_0's auc: 0.85861\n",
      "[388]\ttraining's auc: 0.863632\tvalid_0's auc: 0.858605\n",
      "[389]\ttraining's auc: 0.863657\tvalid_0's auc: 0.858616\n",
      "[390]\ttraining's auc: 0.863667\tvalid_0's auc: 0.858624\n",
      "[391]\ttraining's auc: 0.863683\tvalid_0's auc: 0.858628\n",
      "[392]\ttraining's auc: 0.863697\tvalid_0's auc: 0.858624\n",
      "[393]\ttraining's auc: 0.86371\tvalid_0's auc: 0.858622\n",
      "[394]\ttraining's auc: 0.86374\tvalid_0's auc: 0.858623\n",
      "[395]\ttraining's auc: 0.863758\tvalid_0's auc: 0.858623\n",
      "[396]\ttraining's auc: 0.86376\tvalid_0's auc: 0.858622\n",
      "[397]\ttraining's auc: 0.863779\tvalid_0's auc: 0.858625\n",
      "[398]\ttraining's auc: 0.863787\tvalid_0's auc: 0.858624\n",
      "[399]\ttraining's auc: 0.863809\tvalid_0's auc: 0.858616\n",
      "[400]\ttraining's auc: 0.86382\tvalid_0's auc: 0.858618\n",
      "[401]\ttraining's auc: 0.863826\tvalid_0's auc: 0.85862\n",
      "[402]\ttraining's auc: 0.863846\tvalid_0's auc: 0.858632\n",
      "[403]\ttraining's auc: 0.86387\tvalid_0's auc: 0.858633\n",
      "[404]\ttraining's auc: 0.863885\tvalid_0's auc: 0.858637\n",
      "[405]\ttraining's auc: 0.863897\tvalid_0's auc: 0.858638\n",
      "[406]\ttraining's auc: 0.863903\tvalid_0's auc: 0.858638\n",
      "[407]\ttraining's auc: 0.863918\tvalid_0's auc: 0.85864\n",
      "[408]\ttraining's auc: 0.863936\tvalid_0's auc: 0.858643\n",
      "[409]\ttraining's auc: 0.863955\tvalid_0's auc: 0.858642\n",
      "[410]\ttraining's auc: 0.863983\tvalid_0's auc: 0.858638\n",
      "[411]\ttraining's auc: 0.863998\tvalid_0's auc: 0.858634\n",
      "[412]\ttraining's auc: 0.864019\tvalid_0's auc: 0.858629\n",
      "[413]\ttraining's auc: 0.86403\tvalid_0's auc: 0.858625\n",
      "[414]\ttraining's auc: 0.864034\tvalid_0's auc: 0.858627\n",
      "[415]\ttraining's auc: 0.864062\tvalid_0's auc: 0.858624\n",
      "[416]\ttraining's auc: 0.864073\tvalid_0's auc: 0.858619\n",
      "[417]\ttraining's auc: 0.864089\tvalid_0's auc: 0.858629\n",
      "[418]\ttraining's auc: 0.864108\tvalid_0's auc: 0.85863\n",
      "[419]\ttraining's auc: 0.864133\tvalid_0's auc: 0.858639\n",
      "[420]\ttraining's auc: 0.864151\tvalid_0's auc: 0.858646\n",
      "[421]\ttraining's auc: 0.864161\tvalid_0's auc: 0.858651\n",
      "[422]\ttraining's auc: 0.864182\tvalid_0's auc: 0.858639\n",
      "[423]\ttraining's auc: 0.864206\tvalid_0's auc: 0.858651\n",
      "[424]\ttraining's auc: 0.864217\tvalid_0's auc: 0.858647\n",
      "[425]\ttraining's auc: 0.86423\tvalid_0's auc: 0.858645\n",
      "[426]\ttraining's auc: 0.864244\tvalid_0's auc: 0.858643\n",
      "[427]\ttraining's auc: 0.864256\tvalid_0's auc: 0.858652\n",
      "[428]\ttraining's auc: 0.864269\tvalid_0's auc: 0.858662\n",
      "[429]\ttraining's auc: 0.86428\tvalid_0's auc: 0.858659\n",
      "[430]\ttraining's auc: 0.864296\tvalid_0's auc: 0.858657\n",
      "[431]\ttraining's auc: 0.864313\tvalid_0's auc: 0.858653\n",
      "[432]\ttraining's auc: 0.864316\tvalid_0's auc: 0.858655\n",
      "[433]\ttraining's auc: 0.864328\tvalid_0's auc: 0.858653\n",
      "[434]\ttraining's auc: 0.864352\tvalid_0's auc: 0.858662\n",
      "[435]\ttraining's auc: 0.864363\tvalid_0's auc: 0.858657\n",
      "[436]\ttraining's auc: 0.864376\tvalid_0's auc: 0.85866\n",
      "[437]\ttraining's auc: 0.864393\tvalid_0's auc: 0.858659\n",
      "[438]\ttraining's auc: 0.864401\tvalid_0's auc: 0.85866\n",
      "[439]\ttraining's auc: 0.864419\tvalid_0's auc: 0.85866\n",
      "[440]\ttraining's auc: 0.864445\tvalid_0's auc: 0.85866\n",
      "[441]\ttraining's auc: 0.864455\tvalid_0's auc: 0.858663\n",
      "[442]\ttraining's auc: 0.864466\tvalid_0's auc: 0.858661\n",
      "[443]\ttraining's auc: 0.864476\tvalid_0's auc: 0.858655\n",
      "[444]\ttraining's auc: 0.864492\tvalid_0's auc: 0.858656\n",
      "[445]\ttraining's auc: 0.864506\tvalid_0's auc: 0.858651\n",
      "[446]\ttraining's auc: 0.864526\tvalid_0's auc: 0.858663\n",
      "[447]\ttraining's auc: 0.864552\tvalid_0's auc: 0.858671\n",
      "[448]\ttraining's auc: 0.864562\tvalid_0's auc: 0.858668\n",
      "[449]\ttraining's auc: 0.864578\tvalid_0's auc: 0.85866\n",
      "[450]\ttraining's auc: 0.864604\tvalid_0's auc: 0.858663\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[450]\ttraining's auc: 0.864604\tvalid_0's auc: 0.858663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.5, early_stopping=20, is_unbalance=True,\n",
       "               learning_rate=0.0295, max_bin=None, max_depth=7, metric='auc',\n",
       "               n_estimators=450, objective='binary', random_state=294,\n",
       "               reg_alpha=2, reg_lambda=10, seed=8798)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.fit(X_train,y_train,eval_set=[(X_test,y_test),(X_train, y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24f2fe61a90>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lg.plot_metric(lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.8585388068299891, 0.8584525622460182, 0.8583868882313859, 0.858399842354046, 0.858580474549374, 0.8583973599310237, 0.8586632694031873]\n"
     ]
    }
   ],
   "source": [
    "score = roc_auc_score(y_test,lgb.predict_proba(X_test,lgb.best_iteration_)[:,1])\n",
    "scores.append(score)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>381110</td>\n",
       "      <td>0.004269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381111</td>\n",
       "      <td>0.769540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>381112</td>\n",
       "      <td>0.739819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>381113</td>\n",
       "      <td>0.052382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>381114</td>\n",
       "      <td>0.002757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127032</th>\n",
       "      <td>508142</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127033</th>\n",
       "      <td>508143</td>\n",
       "      <td>0.795874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127034</th>\n",
       "      <td>508144</td>\n",
       "      <td>0.001413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127035</th>\n",
       "      <td>508145</td>\n",
       "      <td>0.001899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127036</th>\n",
       "      <td>508146</td>\n",
       "      <td>0.006318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127037 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  Response\n",
       "0       381110  0.004269\n",
       "1       381111  0.769540\n",
       "2       381112  0.739819\n",
       "3       381113  0.052382\n",
       "4       381114  0.002757\n",
       "...        ...       ...\n",
       "127032  508142  0.003379\n",
       "127033  508143  0.795874\n",
       "127034  508144  0.001413\n",
       "127035  508145  0.001899\n",
       "127036  508146  0.006318\n",
       "\n",
       "[127037 rows x 2 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = savePredictions(lgb,'LightGBM',X_pred)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8549061261818001\n",
      "0.85496873804751\n",
      "0.8549006389585317\n",
      "0.8549746623692587\n"
     ]
    }
   ],
   "source": [
    "l=[8907 , 567 , 3412 , 2030]\n",
    "for i in l:\n",
    "    lgb = lg.LGBMClassifier(boosting_type='gbdt',\n",
    "                     n_estimators=450,depth=10,learning_rate=0.03,\n",
    "                     objective='binary',metric='auc',is_unbalance=True,\n",
    "                     colsample_bytree=0.5,reg_lambda=10,\n",
    "                     reg_alpha=2,random_state=294,n_jobs=-1,seed=i,max_depth=7\n",
    "                    )\n",
    "    lgb.fit(X_train,y_train,eval_set=[(X_test,y_test),(X_train, y_train)],verbose=False)\n",
    "    score = roc_auc_score(y_test,lgb.predict_proba(X_test)[:,1])\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8583244464011723\n",
      "0.8581546628923595\n",
      "0.8583447226048047\n",
      "0.8582845242021591\n",
      "0.8583022482757616\n"
     ]
    }
   ],
   "source": [
    "l=[None,20,60,100,200]\n",
    "for i in l:\n",
    "    lgb = lg.LGBMClassifier(boosting_type='gbdt',\n",
    "                     n_estimators=450,depth=10,learning_rate=0.03,\n",
    "                     objective='binary',metric='auc',is_unbalance=True,\n",
    "                     colsample_bytree=0.5,reg_lambda=10,\n",
    "                     reg_alpha=2,random_state=294,n_jobs=-1,seed=2030,max_depth=7,max_bin=i\n",
    "                    )\n",
    "    lgb.fit(X_train,y_train,eval_set=[(X_test,y_test),(X_train, y_train)],verbose=False)\n",
    "    score = roc_auc_score(y_test,lgb.predict_proba(X_test)[:,1])\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8583545171506552\n",
      "0.8584067291218693\n",
      "0.8583545171506552\n",
      "0.858419037387766\n",
      "0.8576926192399534\n"
     ]
    }
   ],
   "source": [
    "l=[-1,10,20,7,4]\n",
    "for i in l:\n",
    "    lgb = lg.LGBMClassifier(boosting_type='gbdt',\n",
    "                     n_estimators=450,depth=10,learning_rate=0.03,\n",
    "                     objective='binary',metric='auc',is_unbalance=True,\n",
    "                     colsample_bytree=0.5,reg_lambda=10,\n",
    "                     reg_alpha=2,random_state=294,n_jobs=-1,seed=2030,max_depth=i,max_bin=None\n",
    "                    )\n",
    "    lgb.fit(X_train,y_train,eval_set=[(X_test,y_test),(X_train, y_train)],verbose=False)\n",
    "    score = roc_auc_score(y_test,lgb.predict_proba(X_test)[:,1])\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "ufs = SelectKBest(chi2,k=25)\n",
    "ufs.fit(X, y)\n",
    "X_transformed = ufs.transform(X)\n",
    "X_pred = ufs.transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Region_Code', 'Vehicle_Age', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage', 'Mean_premium_per_region', 'Mean_age_per_region', 'Mean_salary_per_region', 'Rank_premium_per_region']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "col_names = X.columns\n",
    "\n",
    "lgb = lg.LGBMClassifier(boosting_type='gbdt',\n",
    "                     n_estimators=450,learning_rate=0.0285,\n",
    "                     objective='binary',metric='auc',is_unbalance=True,\n",
    "                     colsample_bytree=0.5,reg_lambda=10,\n",
    "                     reg_alpha=2,random_state=294,n_jobs=-1,seed=8798,max_depth=7 , max_bin=None)\n",
    "sfm = SelectFromModel(estimator=lgb)\n",
    "X_transformed = sfm.fit_transform(X, y)\n",
    "# see which features were selected\n",
    "support = sfm.get_support()\n",
    "# get feature names\n",
    "print([\n",
    " x for x, y in zip(col_names, support) if y == True\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((381109, 10), (127037, 10))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=['Age', 'Region_Code', 'Vehicle_Age', 'Annual_Premium', 'Policy_Sales_Channel', \n",
    "      'Vintage', 'Mean_premium_per_region', 'Mean_age_per_region', 'Mean_salary_per_region', 'Rank_premium_per_region']\n",
    "train_df = df[df['Response'].isnull()==False]\n",
    "test_df= df[df['Response'].isnull()==True]\n",
    "\n",
    "X = train_df[cols]\n",
    "y = train_df['Response'] \n",
    "X_pred =test_df[cols]\n",
    "\n",
    "X.shape , X_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = lg.LGBMClassifier(boosting_type='gbdt',\n",
    "                     n_estimators=450,learning_rate=0.0295,\n",
    "                     objective='binary',metric='auc',is_unbalance=True,\n",
    "                     colsample_bytree=0.5,reg_lambda=10,\n",
    "                     reg_alpha=2,random_state=294,n_jobs=-1,seed=8798,max_depth=7 , max_bin=None)\n",
    "lgb.fit(X,y)\n",
    "py = lgb.predict_proba(X_pred)[:,1]\n",
    "sub = savePredictions(lgb,'LightGBM',X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\ttraining's auc: 0.846572\tvalid_1's auc: 0.84689\n",
      "[2000]\ttraining's auc: 0.857183\tvalid_1's auc: 0.855992\n",
      "[3000]\ttraining's auc: 0.859843\tvalid_1's auc: 0.857685\n",
      "[4000]\ttraining's auc: 0.861133\tvalid_1's auc: 0.858086\n",
      "[5000]\ttraining's auc: 0.862084\tvalid_1's auc: 0.858278\n",
      "[6000]\ttraining's auc: 0.862823\tvalid_1's auc: 0.858364\n",
      "[7000]\ttraining's auc: 0.863516\tvalid_1's auc: 0.858388\n",
      "[8000]\ttraining's auc: 0.864059\tvalid_1's auc: 0.858358\n",
      "[9000]\ttraining's auc: 0.864629\tvalid_1's auc: 0.858288\n",
      "Early stopping, best iteration is:\n",
      "[6788]\ttraining's auc: 0.863363\tvalid_1's auc: 0.858407\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\ttraining's auc: 0.8466\tvalid_1's auc: 0.846786\n",
      "[2000]\ttraining's auc: 0.857148\tvalid_1's auc: 0.856163\n",
      "[3000]\ttraining's auc: 0.859842\tvalid_1's auc: 0.857967\n",
      "[4000]\ttraining's auc: 0.86117\tvalid_1's auc: 0.858395\n",
      "[5000]\ttraining's auc: 0.862125\tvalid_1's auc: 0.858616\n",
      "[6000]\ttraining's auc: 0.862887\tvalid_1's auc: 0.85863\n",
      "[7000]\ttraining's auc: 0.863578\tvalid_1's auc: 0.858731\n",
      "[8000]\ttraining's auc: 0.864152\tvalid_1's auc: 0.858757\n",
      "[9000]\ttraining's auc: 0.864703\tvalid_1's auc: 0.85877\n",
      "[10000]\ttraining's auc: 0.865223\tvalid_1's auc: 0.858725\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's auc: 0.865223\tvalid_1's auc: 0.858725\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\ttraining's auc: 0.846981\tvalid_1's auc: 0.844425\n",
      "[2000]\ttraining's auc: 0.857499\tvalid_1's auc: 0.854264\n",
      "[3000]\ttraining's auc: 0.860217\tvalid_1's auc: 0.856196\n",
      "[4000]\ttraining's auc: 0.861513\tvalid_1's auc: 0.856622\n",
      "[5000]\ttraining's auc: 0.862495\tvalid_1's auc: 0.856801\n",
      "[6000]\ttraining's auc: 0.863216\tvalid_1's auc: 0.856807\n",
      "[7000]\ttraining's auc: 0.863932\tvalid_1's auc: 0.856873\n",
      "[8000]\ttraining's auc: 0.864509\tvalid_1's auc: 0.85688\n",
      "[9000]\ttraining's auc: 0.865085\tvalid_1's auc: 0.856799\n",
      "[10000]\ttraining's auc: 0.865631\tvalid_1's auc: 0.856773\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9997]\ttraining's auc: 0.865631\tvalid_1's auc: 0.856774\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\ttraining's auc: 0.846801\tvalid_1's auc: 0.845041\n",
      "[2000]\ttraining's auc: 0.857378\tvalid_1's auc: 0.854937\n",
      "[3000]\ttraining's auc: 0.860063\tvalid_1's auc: 0.856858\n",
      "[4000]\ttraining's auc: 0.86137\tvalid_1's auc: 0.857319\n",
      "[5000]\ttraining's auc: 0.862337\tvalid_1's auc: 0.857498\n",
      "[6000]\ttraining's auc: 0.863044\tvalid_1's auc: 0.857553\n",
      "[7000]\ttraining's auc: 0.863713\tvalid_1's auc: 0.857593\n",
      "[8000]\ttraining's auc: 0.864289\tvalid_1's auc: 0.857553\n",
      "[9000]\ttraining's auc: 0.864866\tvalid_1's auc: 0.85744\n",
      "[10000]\ttraining's auc: 0.865399\tvalid_1's auc: 0.857437\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9990]\ttraining's auc: 0.865401\tvalid_1's auc: 0.857444\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 3000 rounds\n",
      "[1000]\ttraining's auc: 0.847014\tvalid_1's auc: 0.843625\n",
      "[2000]\ttraining's auc: 0.857602\tvalid_1's auc: 0.853596\n",
      "[3000]\ttraining's auc: 0.860272\tvalid_1's auc: 0.855565\n",
      "[4000]\ttraining's auc: 0.861535\tvalid_1's auc: 0.856028\n",
      "[5000]\ttraining's auc: 0.862488\tvalid_1's auc: 0.856338\n",
      "[6000]\ttraining's auc: 0.863191\tvalid_1's auc: 0.856446\n",
      "[7000]\ttraining's auc: 0.863853\tvalid_1's auc: 0.856488\n",
      "[8000]\ttraining's auc: 0.864417\tvalid_1's auc: 0.856493\n",
      "[9000]\ttraining's auc: 0.864968\tvalid_1's auc: 0.856515\n",
      "[10000]\ttraining's auc: 0.865491\tvalid_1's auc: 0.856532\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9990]\ttraining's auc: 0.865492\tvalid_1's auc: 0.856521\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-312-734b1e466a83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CV score: {:<8.5f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "features = list(X.columns)\n",
    "\n",
    "param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,  \n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'num_threads': 8,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=False, random_state=44000)\n",
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lg.Dataset(X.iloc[trn_idx][features], label=y.iloc[trn_idx])\n",
    "    val_data = lg.Dataset(X.iloc[val_idx][features], label=y.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lg.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(X_pred, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-313-1b32a553ce76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CV score: {:<8.5f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target.iloc[val_idx], oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[\"Response\"] = predictions\n",
    "sub.to_csv(\"last.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

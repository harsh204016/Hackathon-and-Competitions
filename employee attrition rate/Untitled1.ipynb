{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import random\n",
    "\n",
    "#Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#Torch and Tabnet\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Sklearn only for splitting\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 4  # you can specify your folds here\n",
    "seed = 2020   # seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_val, y_pr):\n",
    "    overall_score=100*max(0,1-np.sqrt(mean_squared_error(y_val,y_pr)))\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "EMP_ID = test['Employee_ID']\n",
    "\n",
    "df[\"kfold\"] = -1\n",
    "\n",
    "df = df.sample(frac=1,random_state=2020).reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=NUM_FOLDS)\n",
    "\n",
    "for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df)):\n",
    "    df.loc[val_, 'kfold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Relationship_Status</th>\n",
       "      <th>Hometown</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Decision_skill_possess</th>\n",
       "      <th>Time_of_service</th>\n",
       "      <th>Time_since_promotion</th>\n",
       "      <th>growth_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR2</th>\n",
       "      <th>VAR3</th>\n",
       "      <th>VAR4</th>\n",
       "      <th>VAR5</th>\n",
       "      <th>VAR6</th>\n",
       "      <th>VAR7</th>\n",
       "      <th>Attrition_rate</th>\n",
       "      <th>kfold</th>\n",
       "      <th>time</th>\n",
       "      <th>age_while_joining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1048</td>\n",
       "      <td>-1.6150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender   Age  Education_Level  Relationship_Status  Hometown  Unit  \\\n",
       "0       0  61.0                1                    0         4     3   \n",
       "1       0  64.0                1                    1         1     5   \n",
       "2       0  26.0                3                    0         1     3   \n",
       "\n",
       "   Decision_skill_possess  Time_of_service  Time_since_promotion  growth_rate  \\\n",
       "0                       2             22.0                     3           59   \n",
       "1                       3             37.0                     1           69   \n",
       "2                       1              3.0                     2           62   \n",
       "\n",
       "   ...    VAR2    VAR3  VAR4  VAR5  VAR6  VAR7  Attrition_rate  kfold  time  \\\n",
       "0  ... -0.1048 -1.6150   2.0     4     7     5          0.0590    0.0  19.0   \n",
       "1  ...  0.0000 -0.4537   2.0     2     7     4          0.0313    0.0  36.0   \n",
       "2  ...  0.7516 -0.4537   4.0     3     8     3          0.1479    0.0   1.0   \n",
       "\n",
       "   age_while_joining  \n",
       "0               39.0  \n",
       "1               27.0  \n",
       "2               23.0  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 25)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintest = df.append(test)\n",
    "traintest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anynull_value 2\n"
     ]
    }
   ],
   "source": [
    "traintest.drop(['Employee_ID'],axis=1,inplace=True)\n",
    "traintest['VAR4'].fillna(4.0,inplace=True)\n",
    "traintest['Age'].fillna(traintest.groupby(['Gender', 'Time_since_promotion'])['Age'].transform('median'),inplace=True)\n",
    "traintest['VAR2'].fillna(0.0,inplace=True)\n",
    "traintest['Pay_Scale'].fillna(traintest.Pay_Scale.median(),inplace=True)\n",
    "traintest['Time_of_service'].fillna(traintest.Time_of_service.mean(),inplace=True)\n",
    "traintest['Work_Life_balance'].fillna(traintest.Work_Life_balance.mean(),inplace=True)\n",
    "traintest['time'] = traintest.apply(lambda row: row.Time_of_service - row.Time_since_promotion , axis = 1)\n",
    "traintest['age_while_joining'] = traintest.apply(lambda r:r.Age - r.Time_of_service,axis=1)\n",
    "cat_features =  list(traintest.select_dtypes(include=object).columns)\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "\n",
    "for i in cat_features:\n",
    "    l = LabelEncoder()\n",
    "    traintest[i] = l.fit_transform(traintest[i].values)\n",
    "    categorical_columns.append(i)\n",
    "    categorical_dims[i]=len(l.classes_)\n",
    "print('anynull_value',traintest.isnull().any().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape (7000, 26)\n",
      "test data shape (3000, 26)\n"
     ]
    }
   ],
   "source": [
    "df = traintest[:7000]\n",
    "test = traintest[7000:]\n",
    "print('training data shape',df.shape)\n",
    "print('test data shape',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Relationship_Status</th>\n",
       "      <th>Hometown</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Decision_skill_possess</th>\n",
       "      <th>Time_of_service</th>\n",
       "      <th>Time_since_promotion</th>\n",
       "      <th>growth_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR2</th>\n",
       "      <th>VAR3</th>\n",
       "      <th>VAR4</th>\n",
       "      <th>VAR5</th>\n",
       "      <th>VAR6</th>\n",
       "      <th>VAR7</th>\n",
       "      <th>Attrition_rate</th>\n",
       "      <th>kfold</th>\n",
       "      <th>time</th>\n",
       "      <th>age_while_joining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1048</td>\n",
       "      <td>-1.6150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.8176</td>\n",
       "      <td>0.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1048</td>\n",
       "      <td>-1.6150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-1.6150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender   Age  Education_Level  Relationship_Status  Hometown  Unit  \\\n",
       "0       0  61.0                1                    0         4     3   \n",
       "1       0  64.0                1                    1         1     5   \n",
       "2       0  26.0                3                    0         1     3   \n",
       "3       1  47.0                1                    0         3     0   \n",
       "4       0  58.0                3                    1         3    10   \n",
       "5       0  61.0                3                    0         1     9   \n",
       "6       1  30.0                3                    1         4     2   \n",
       "7       0  57.0                5                    1         2     3   \n",
       "8       0  20.0                2                    1         1     3   \n",
       "9       0  22.0                3                    1         3     5   \n",
       "\n",
       "   Decision_skill_possess  Time_of_service  Time_since_promotion  growth_rate  \\\n",
       "0                       2             22.0                     3           59   \n",
       "1                       3             37.0                     1           69   \n",
       "2                       1              3.0                     2           62   \n",
       "3                       1             16.0                     4           60   \n",
       "4                       1             26.0                     4           60   \n",
       "5                       3             37.0                     2           33   \n",
       "6                       3              6.0                     3           22   \n",
       "7                       3             22.0                     4           54   \n",
       "8                       1              1.0                     1           46   \n",
       "9                       1              3.0                     3           56   \n",
       "\n",
       "   ...    VAR2    VAR3  VAR4  VAR5  VAR6  VAR7  Attrition_rate  kfold  time  \\\n",
       "0  ... -0.1048 -1.6150   2.0     4     7     5          0.0590    0.0  19.0   \n",
       "1  ...  0.0000 -0.4537   2.0     2     7     4          0.0313    0.0  36.0   \n",
       "2  ...  0.7516 -0.4537   4.0     3     8     3          0.1479    0.0   1.0   \n",
       "3  ...  0.7516 -0.4537   2.0     1     6     3          0.0290    0.0  12.0   \n",
       "4  ... -1.8176  0.7075   4.0     4     8     4          0.0767    0.0  22.0   \n",
       "5  ...  0.0000 -0.4537   1.0     1     6     5          0.1809    0.0  35.0   \n",
       "6  ... -0.1048 -1.6150   2.0     2     9     5          0.2187    0.0   3.0   \n",
       "7  ...  0.7516 -0.4537   1.0     2     8     1          0.0956    0.0  18.0   \n",
       "8  ...  0.7516 -1.6150   2.0     2     6     4          0.1158    0.0   0.0   \n",
       "9  ...  0.7516 -0.4537   2.0     1     8     3          0.1570    0.0   0.0   \n",
       "\n",
       "   age_while_joining  \n",
       "0               39.0  \n",
       "1               27.0  \n",
       "2               23.0  \n",
       "3               31.0  \n",
       "4               32.0  \n",
       "5               24.0  \n",
       "6               24.0  \n",
       "7               35.0  \n",
       "8               19.0  \n",
       "9               19.0  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "features = ['Gender','Age', 'Education_Level', 'Relationship_Status', 'Hometown', 'Unit',\n",
    "            'Decision_skill_possess', 'Time_of_service', 'Time_since_promotion', 'growth_rate',\n",
    "            'Travel_Rate', 'Post_Level', 'Pay_Scale', 'Compensation_and_Benefits', 'Work_Life_balance',\n",
    "            'VAR1', 'VAR2', 'VAR3', 'VAR4', 'VAR5', 'VAR6', 'VAR7','time','age_while_joining']\n",
    "target = 'Attrition_rate'\n",
    "print(len(features))\n",
    "\n",
    "unused_feat=['Employee_ID','kfold']\n",
    "target = ['Attrition_rate']\n",
    "\n",
    "feature = [ col for col in df.columns if col not in unused_feat+[target]] \n",
    "cat_idxs = [ i for i, f in enumerate(feature) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(feature) if f in categorical_columns]\n",
    "cat_emb_dim = [5, 4, 3, 6, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 5, 6, 13]\n",
      "[2, 2, 5, 12, 4, 5]\n",
      "[5, 4, 3, 6, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(cat_idxs)\n",
    "print(cat_dims)\n",
    "print(cat_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n"
     ]
    }
   ],
   "source": [
    "model = TabNetRegressor(n_d=16,\n",
    "                       n_a=16,\n",
    "                       n_steps=4,\n",
    "                       gamma=1.3,\n",
    "                       n_independent=2,\n",
    "                       n_shared=2,\n",
    "                       seed=seed,\n",
    "                       optimizer_fn = torch.optim.Adam,\n",
    "                        cat_dims=cat_dims,cat_emb_dim=2,cat_idxs=cat_idxs,\n",
    "                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Miniconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(subsample= 0.7,n_estimators=500,\n",
    "                                 min_child_weight= 2,max_depth= 5,\n",
    "                                 learning_rate= 0.2,booster='gblinear',\n",
    "                                 base_score= 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros((test.shape[0],1, NUM_FOLDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 1, 4), (3000, 26))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape , test.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    df_train = df[df.kfold != fold]\n",
    "    df_valid = df[df.kfold == fold]\n",
    "    \n",
    "    X_train = df_train[features].values\n",
    "    Y_train = df_train[target].values.reshape(-1, 1)\n",
    "    \n",
    "    X_valid = df_valid[features].values\n",
    "    Y_valid = df_valid[target].values.reshape(-1, 1)\n",
    "    \n",
    "    y_oof = np.zeros((df_valid.shape[0],1))   # Out of folds validation\n",
    "    \n",
    "    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n",
    "     \n",
    "    #model.fit(X_train = X_train,y_train = Y_train,X_valid = X_valid,\n",
    "     #        y_valid = Y_valid,max_epochs = 1000,patience =50)\n",
    "    \n",
    "    xgb_model.fit(X_train, Y_train,eval_set=(X_valid,Y_valid),\n",
    "                  early_stopping_rounds=50)\n",
    "    print(\"--------Validating For fold {}------------\".format(fold+1))\n",
    "    \n",
    "    y_oof = model.predict(X_valid)\n",
    "    y_test[:,:,fold] = model.predict(test[features].values)\n",
    "    \n",
    "    \n",
    "    \n",
    "    val_score = metric(Y_valid,y_oof)\n",
    "    \n",
    "    print(\"Validation score: {:<8.5f}\".format(val_score))\n",
    "    \n",
    "    # VISUALIZTION\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(model.history['train']['loss'])\n",
    "    plt.plot(model.history['valid']['loss'])\n",
    "    \n",
    "    plt.plot(model.history['train']['metric'])\n",
    "    plt.plot(model.history['valid']['metric'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 1-------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected input type for `eval_set`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-d64cf0f07b3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-119-4430adc5524b>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(fold)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     xgb_model.fit(X_train, Y_train,eval_set=( X_valid,Y_valid),\n\u001b[1;32m---> 19\u001b[1;33m                   early_stopping_rounds=50)\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------Validating For fold {}------------\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0meval_set\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unexpected input type for `eval_set`'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msample_weight_eval_set\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m                 \u001b[0msample_weight_eval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected input type for `eval_set`"
     ]
    }
   ],
   "source": [
    "run(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 2-------------\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -1.27955 |  -320.96552 |   2.5       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.28885 |  -90.72405 |   4.9       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.14439 |  -6.84033 |   7.2       \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.09085 |  -1.73557 |   9.7       \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.06913 |  -1.01610 |   12.0      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.05549 |  -0.52360 |   14.6      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04768 |  -0.27363 |   17.0      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.04540 |  -0.19661 |   19.4      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.04288 |  -0.13361 |   21.8      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.04108 |  -0.09159 |   24.4      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.04008 |  -0.06877 |   26.8      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03903 |  -0.05691 |   29.4      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03879 |  -0.05293 |   32.1      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03902 |  -0.04771 |   34.7      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03795 |  -0.04588 |   37.7      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03731 |  -0.05880 |   40.2      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03733 |  -0.05909 |   42.9      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03703 |  -0.04640 |   45.9      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03706 |  -0.04647 |   48.6      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03724 |  -0.04013 |   51.2      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03701 |  -0.03920 |   53.6      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03669 |  -0.03896 |   56.1      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03642 |  -0.03782 |   58.5      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03618 |  -0.03662 |   60.8      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03596 |  -0.03620 |   63.2      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03551 |  -0.03719 |   65.6      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03555 |  -0.03526 |   68.0      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03553 |  -0.03489 |   70.5      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03576 |  -0.03448 |   72.9      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03526 |  -0.03406 |   75.3      \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03494 |  -0.03389 |   77.8      \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03519 |  -0.03414 |   80.4      \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03497 |  -0.03445 |   82.9      \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03532 |  -0.03380 |   85.5      \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03498 |  -0.03413 |   88.1      \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03481 |  -0.03413 |   90.6      \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03481 |  -0.03409 |   93.0      \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03464 |  -0.03460 |   95.4      \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03484 |  -0.03466 |   98.5      \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03501 |  -0.03486 |   101.3     \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03560 |  -0.03361 |   104.4     \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03489 |  -0.03352 |   107.1     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03495 |  -0.03450 |   109.5     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03504 |  -0.03450 |   112.3     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03492 |  -0.03481 |   114.9     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03530 |  -0.03434 |   117.7     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03488 |  -0.03407 |   120.3     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03459 |  -0.03424 |   123.0     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03475 |  -0.03481 |   125.7     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03487 |  -0.03408 |   128.4     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03457 |  -0.03443 |   131.2     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03488 |  -0.03414 |   133.8     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03464 |  -0.03435 |   136.2     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03491 |  -0.03423 |   138.8     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03495 |  -0.03547 |   141.3     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03616 |  -0.03523 |   143.9     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03543 |  -0.03400 |   146.4     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03474 |  -0.03475 |   149.1     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03523 |  -0.03389 |   151.5     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03497 |  -0.03404 |   154.1     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03441 |  -0.03470 |   156.4     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03472 |  -0.03404 |   159.0     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03434 |  -0.03411 |   161.5     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03449 |  -0.03374 |   164.0     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03483 |  -0.03482 |   166.5     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03463 |  -0.03421 |   169.0     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03417 |  -0.03398 |   171.4     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03401 |  -0.03375 |   173.9     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03439 |  -0.03412 |   176.2     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03510 |  -0.03416 |   178.9     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03465 |  -0.03384 |   181.3     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03453 |  -0.03398 |   183.8     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03433 |  -0.03392 |   186.3     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03488 |  -0.03471 |   188.9     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03452 |  -0.03419 |   191.4     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03469 |  -0.03448 |   194.0     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03518 |  -0.03448 |   196.5     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03480 |  -0.03399 |   199.1     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03439 |  -0.03399 |   201.6     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03444 |  -0.03488 |   204.1     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03452 |  -0.03429 |   206.5     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03449 |  -0.03399 |   209.0     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03481 |  -0.03461 |   211.8     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03433 |  -0.03387 |   214.4     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03444 |  -0.03404 |   217.1     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03450 |  -0.03410 |   219.6     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03434 |  -0.03412 |   222.2     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03434 |  -0.03535 |   225.1     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03528 |  -0.03475 |   227.7     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03485 |  -0.03419 |   230.3     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03448 |  -0.03495 |   232.9     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03475 |  -0.03425 |   235.4     \n",
      "Early stopping occured at epoch 92\n",
      "Training done in 235.412 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 2------------\n",
      "Validation score: 81.69047\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 3-------------\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -1.59115 |  -57.64458 |   2.5       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.33833 |  -9.64072 |   5.1       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.14040 |  -2.47373 |   7.5       \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.09257 |  -3.06410 |   10.0      \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.07412 |  -0.43414 |   12.4      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.06184 |  -0.53068 |   15.2      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04937 |  -0.27702 |   18.1      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.04770 |  -0.19388 |   20.8      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.04539 |  -0.24706 |   23.3      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.04215 |  -0.08523 |   25.9      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.04141 |  -0.08591 |   28.9      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03958 |  -0.08408 |   31.6      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03976 |  -0.06431 |   34.3      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03906 |  -0.05767 |   37.5      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03809 |  -0.05592 |   40.5      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03745 |  -0.04154 |   43.3      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03756 |  -0.04099 |   45.8      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03761 |  -0.03880 |   48.2      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03762 |  -0.03951 |   50.5      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03714 |  -0.03633 |   52.9      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03675 |  -0.03705 |   55.6      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03622 |  -0.03701 |   58.1      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03621 |  -0.03512 |   60.9      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03680 |  -0.03732 |   63.3      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03721 |  -0.03874 |   65.8      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03728 |  -0.03751 |   68.4      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03725 |  -0.03524 |   70.8      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03643 |  -0.03452 |   73.5      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03651 |  -0.03464 |   75.9      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03560 |  -0.03494 |   78.4      \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03596 |  -0.03449 |   80.8      \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03617 |  -0.03451 |   83.4      \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03575 |  -0.03430 |   85.9      \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03572 |  -0.03388 |   88.5      \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03568 |  -0.03348 |   91.1      \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03549 |  -0.03390 |   93.7      \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03509 |  -0.03454 |   96.2      \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03562 |  -0.03433 |   98.6      \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03524 |  -0.03383 |   101.2     \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03542 |  -0.03359 |   103.6     \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03503 |  -0.03458 |   106.1     \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03590 |  -0.03449 |   108.5     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03516 |  -0.03423 |   111.2     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03562 |  -0.03361 |   113.6     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03514 |  -0.03402 |   116.8     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03539 |  -0.03369 |   119.3     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03553 |  -0.03363 |   121.6     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03499 |  -0.03390 |   124.2     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03479 |  -0.03363 |   126.6     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03518 |  -0.03453 |   129.1     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03619 |  -0.03343 |   131.5     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03549 |  -0.03477 |   134.0     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03615 |  -0.03410 |   136.6     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03598 |  -0.03436 |   139.2     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03554 |  -0.03408 |   142.2     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03560 |  -0.03362 |   145.3     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03528 |  -0.03404 |   147.8     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03500 |  -0.03376 |   150.7     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03505 |  -0.03380 |   154.1     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03487 |  -0.03373 |   157.0     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03496 |  -0.03347 |   159.8     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03517 |  -0.03357 |   162.4     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03472 |  -0.03379 |   164.8     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03488 |  -0.03441 |   167.3     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03518 |  -0.03362 |   169.7     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03486 |  -0.03360 |   172.3     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03461 |  -0.03380 |   174.9     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03484 |  -0.03493 |   177.4     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03499 |  -0.03422 |   180.7     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03477 |  -0.03428 |   183.3     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03457 |  -0.03398 |   186.1     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03428 |  -0.03419 |   188.6     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03438 |  -0.03399 |   191.0     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03443 |  -0.03399 |   193.6     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03433 |  -0.03400 |   196.1     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03446 |  -0.03403 |   198.7     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03474 |  -0.03372 |   201.3     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03406 |  -0.03455 |   203.8     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03438 |  -0.03427 |   206.4     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03419 |  -0.03418 |   209.0     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03503 |  -0.03382 |   211.6     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03461 |  -0.03368 |   214.2     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03476 |  -0.03376 |   217.0     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03464 |  -0.03538 |   219.9     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03500 |  -0.03361 |   222.6     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03421 |  -0.03360 |   225.1     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03421 |  -0.03363 |   227.9     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03434 |  -0.03375 |   230.5     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03417 |  -0.03410 |   233.0     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03435 |  -0.03441 |   235.6     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03482 |  -0.03385 |   238.3     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03434 |  -0.03491 |   240.9     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03451 |  -0.03378 |   243.5     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03429 |  -0.03392 |   246.1     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03457 |  -0.03435 |   248.6     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03412 |  -0.03416 |   251.2     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03404 |  -0.03414 |   254.0     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03437 |  -0.03468 |   256.6     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03402 |  -0.03389 |   259.3     \n",
      "Current learning rate:  0.02\n",
      "| 100   | -0.03395 |  -0.03439 |   261.9     \n",
      "Current learning rate:  0.02\n",
      "| 101   | -0.03463 |  -0.03595 |   264.5     \n",
      "Early stopping occured at epoch 101\n",
      "Training done in 264.531 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 3------------\n",
      "Validation score: 81.71579\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 4-------------\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -1.25571 |  -21.22816 |   2.6       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.30853 |  -11.82255 |   5.2       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.13888 |  -2.54096 |   8.7       \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.08934 |  -2.47569 |   11.8      \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.06970 |  -0.38567 |   15.0      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.05541 |  -0.28321 |   18.2      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04824 |  -0.26287 |   20.9      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.04378 |  -0.15826 |   23.7      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.04175 |  -0.08091 |   26.4      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.03915 |  -0.06206 |   28.8      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.03930 |  -0.05507 |   31.4      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03840 |  -0.04847 |   34.1      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03712 |  -0.04667 |   36.7      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03709 |  -0.04474 |   39.1      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03713 |  -0.04422 |   41.6      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03602 |  -0.04237 |   44.1      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03592 |  -0.04102 |   46.6      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03551 |  -0.03979 |   49.0      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03546 |  -0.04002 |   51.4      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03597 |  -0.04000 |   53.9      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03628 |  -0.04074 |   56.3      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03562 |  -0.03866 |   58.8      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03537 |  -0.03829 |   61.7      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03464 |  -0.03840 |   64.3      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03475 |  -0.03837 |   66.8      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03517 |  -0.03785 |   69.2      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03501 |  -0.03730 |   71.9      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03460 |  -0.03803 |   74.5      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03466 |  -0.03810 |   77.0      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03480 |  -0.03850 |   79.5      \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03502 |  -0.03726 |   82.0      \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03463 |  -0.03806 |   84.5      \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03491 |  -0.03696 |   87.1      \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03422 |  -0.03700 |   89.7      \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03419 |  -0.03690 |   92.8      \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03447 |  -0.03676 |   95.4      \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03406 |  -0.03687 |   97.9      \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03418 |  -0.03717 |   100.4     \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03456 |  -0.03695 |   102.8     \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03454 |  -0.03656 |   105.3     \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03431 |  -0.03677 |   108.2     \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03432 |  -0.03657 |   110.7     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03405 |  -0.03648 |   113.2     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03413 |  -0.03683 |   115.6     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03414 |  -0.03687 |   118.3     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03378 |  -0.03684 |   121.3     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03420 |  -0.03697 |   123.9     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03437 |  -0.03704 |   126.5     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03427 |  -0.03709 |   129.2     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03416 |  -0.03672 |   131.7     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03402 |  -0.03735 |   134.3     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03437 |  -0.03707 |   136.9     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03408 |  -0.03669 |   139.5     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03401 |  -0.03662 |   142.1     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03421 |  -0.03641 |   144.6     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03397 |  -0.03647 |   147.1     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03380 |  -0.03669 |   149.6     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03359 |  -0.03645 |   152.1     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03380 |  -0.03650 |   154.5     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03403 |  -0.03629 |   157.0     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03397 |  -0.03702 |   159.4     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03430 |  -0.03658 |   161.9     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03407 |  -0.03678 |   164.5     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03383 |  -0.03639 |   166.9     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03394 |  -0.03646 |   169.4     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03377 |  -0.03616 |   171.9     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03384 |  -0.03620 |   174.7     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03378 |  -0.03613 |   178.0     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03361 |  -0.03627 |   181.7     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03371 |  -0.03641 |   184.8     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03366 |  -0.03641 |   187.4     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03378 |  -0.03723 |   190.0     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03399 |  -0.03718 |   192.6     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03397 |  -0.03686 |   195.2     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03385 |  -0.03733 |   198.0     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03400 |  -0.03654 |   201.6     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03369 |  -0.03655 |   204.2     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03374 |  -0.03682 |   206.7     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03369 |  -0.03654 |   209.3     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03385 |  -0.03651 |   212.5     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03353 |  -0.03651 |   215.2     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03359 |  -0.03653 |   217.9     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03366 |  -0.03632 |   220.7     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03364 |  -0.03653 |   224.3     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03404 |  -0.03776 |   227.6     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03409 |  -0.03632 |   230.4     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03364 |  -0.03613 |   233.0     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03361 |  -0.03646 |   235.7     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03360 |  -0.03653 |   238.5     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03360 |  -0.03625 |   241.9     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03355 |  -0.03646 |   244.9     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03388 |  -0.03653 |   247.6     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03355 |  -0.03656 |   251.0     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03353 |  -0.03679 |   254.2     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03355 |  -0.03655 |   257.3     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03341 |  -0.03664 |   260.4     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03338 |  -0.03669 |   263.3     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03346 |  -0.03654 |   266.2     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03315 |  -0.03667 |   269.2     \n",
      "Current learning rate:  0.02\n",
      "| 100   | -0.03321 |  -0.03665 |   272.2     \n",
      "Current learning rate:  0.02\n",
      "| 101   | -0.03335 |  -0.03655 |   275.6     \n",
      "Current learning rate:  0.02\n",
      "| 102   | -0.03301 |  -0.03702 |   279.0     \n",
      "Current learning rate:  0.02\n",
      "| 103   | -0.03315 |  -0.03700 |   282.7     \n",
      "Current learning rate:  0.02\n",
      "| 104   | -0.03296 |  -0.03667 |   285.6     \n",
      "Current learning rate:  0.02\n",
      "| 105   | -0.03313 |  -0.03689 |   288.6     \n",
      "Current learning rate:  0.02\n",
      "| 106   | -0.03333 |  -0.03712 |   291.1     \n",
      "Current learning rate:  0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 107   | -0.03341 |  -0.03682 |   293.6     \n",
      "Current learning rate:  0.02\n",
      "| 108   | -0.03322 |  -0.03680 |   296.0     \n",
      "Current learning rate:  0.02\n",
      "| 109   | -0.03311 |  -0.03719 |   298.9     \n",
      "Current learning rate:  0.02\n",
      "| 110   | -0.03331 |  -0.03699 |   301.3     \n",
      "Current learning rate:  0.02\n",
      "| 111   | -0.03334 |  -0.03754 |   304.0     \n",
      "Current learning rate:  0.02\n",
      "| 112   | -0.03381 |  -0.03729 |   306.6     \n",
      "Current learning rate:  0.02\n",
      "| 113   | -0.03365 |  -0.03685 |   309.3     \n",
      "Current learning rate:  0.02\n",
      "| 114   | -0.03284 |  -0.03684 |   312.0     \n",
      "Current learning rate:  0.02\n",
      "| 115   | -0.03288 |  -0.03719 |   315.1     \n",
      "Current learning rate:  0.02\n",
      "| 116   | -0.03295 |  -0.03704 |   317.8     \n",
      "Current learning rate:  0.02\n",
      "| 117   | -0.03309 |  -0.03668 |   320.6     \n",
      "Current learning rate:  0.02\n",
      "| 118   | -0.03313 |  -0.03682 |   323.3     \n",
      "Early stopping occured at epoch 118\n",
      "Training done in 323.349 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 4------------\n",
      "Validation score: 80.99230\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tes = y_test.mean(axis=-1) # Taking mean of all the fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17794737],\n",
       "       [0.18304728],\n",
       "       [0.17835943],\n",
       "       ...,\n",
       "       [0.15356284],\n",
       "       [0.1750649 ],\n",
       "       [0.21984392]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Employee_ID'] = EMP_ID\n",
    "sub['Attrition_rate'] = y_tes\n",
    "sub.to_csv('tabnet_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"tabnet_submission.csv\")\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 24)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(test[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Employee_ID'] = test['Employee_ID']\n",
    "sub['Attrition_rate'] = y_test\n",
    "sub.to_csv('tabnet_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = model.predict(test[features].values)\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

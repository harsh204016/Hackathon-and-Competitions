{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import random\n",
    "\n",
    "#Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#Torch and Tabnet\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Sklearn only for splitting\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5  # you can specify your folds here\n",
    "seed = 2020   # seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_val, y_pr):\n",
    "    overall_score=100*max(0,1-np.sqrt(mean_squared_error(y_val,y_pr)))\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "EMP_ID = test['Employee_ID']\n",
    "\n",
    "df[\"kfold\"] = -1\n",
    "\n",
    "df = df.sample(frac=1,random_state=2020).reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=NUM_FOLDS)\n",
    "\n",
    "for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df)):\n",
    "    df.loc[val_, 'kfold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Relationship_Status</th>\n",
       "      <th>Hometown</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Decision_skill_possess</th>\n",
       "      <th>Time_of_service</th>\n",
       "      <th>Time_since_promotion</th>\n",
       "      <th>...</th>\n",
       "      <th>Work_Life_balance</th>\n",
       "      <th>VAR1</th>\n",
       "      <th>VAR2</th>\n",
       "      <th>VAR3</th>\n",
       "      <th>VAR4</th>\n",
       "      <th>VAR5</th>\n",
       "      <th>VAR6</th>\n",
       "      <th>VAR7</th>\n",
       "      <th>Attrition_rate</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EID_1087</td>\n",
       "      <td>F</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Married</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Conceptual</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1048</td>\n",
       "      <td>-1.6150</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EID_4428</td>\n",
       "      <td>F</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Single</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>Operarions</td>\n",
       "      <td>Directive</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EID_15654</td>\n",
       "      <td>F</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Married</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EID_23536</td>\n",
       "      <td>M</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Married</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Accounting and Finance</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>-0.4537</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EID_4125</td>\n",
       "      <td>F</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Single</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.8176</td>\n",
       "      <td>0.7075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Employee_ID Gender   Age  Education_Level Relationship_Status     Hometown  \\\n",
       "0    EID_1087      F  61.0                1             Married   Washington   \n",
       "1    EID_4428      F  64.0                1              Single     Franklin   \n",
       "2   EID_15654      F  26.0                3             Married     Franklin   \n",
       "3   EID_23536      M  47.0                1             Married  Springfield   \n",
       "4    EID_4125      F  58.0                3              Single  Springfield   \n",
       "\n",
       "                     Unit Decision_skill_possess  Time_of_service  \\\n",
       "0               Logistics             Conceptual             22.0   \n",
       "1              Operarions              Directive             37.0   \n",
       "2               Logistics             Behavioral              3.0   \n",
       "3  Accounting and Finance             Behavioral             16.0   \n",
       "4                   Sales             Behavioral             26.0   \n",
       "\n",
       "   Time_since_promotion  ...  Work_Life_balance  VAR1    VAR2    VAR3 VAR4  \\\n",
       "0                     3  ...                3.0     3 -0.1048 -1.6150  2.0   \n",
       "1                     1  ...                4.0     4     NaN -0.4537  2.0   \n",
       "2                     2  ...                2.0     3  0.7516 -0.4537  NaN   \n",
       "3                     4  ...                1.0     3  0.7516 -0.4537  2.0   \n",
       "4                     4  ...                3.0     3 -1.8176  0.7075  NaN   \n",
       "\n",
       "   VAR5  VAR6  VAR7  Attrition_rate  kfold  \n",
       "0     4     7     5          0.0590      0  \n",
       "1     2     7     4          0.0313      0  \n",
       "2     3     8     3          0.1479      0  \n",
       "3     1     6     3          0.0290      0  \n",
       "4     4     8     4          0.0767      0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 25)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintest = df.append(test)\n",
    "traintest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anynull_value 2\n"
     ]
    }
   ],
   "source": [
    "traintest.drop(['Employee_ID'],axis=1,inplace=True)\n",
    "traintest['VAR4'].fillna(4.0,inplace=True)\n",
    "traintest['Age'].fillna(traintest.groupby(['Gender', 'Time_since_promotion'])['Age'].transform('median'),inplace=True)\n",
    "traintest['VAR2'].fillna(0.0,inplace=True)\n",
    "traintest['Pay_Scale'].fillna(traintest.Pay_Scale.median(),inplace=True)\n",
    "traintest['Time_of_service'].fillna(traintest.Time_of_service.mean(),inplace=True)\n",
    "traintest['Work_Life_balance'].fillna(traintest.Work_Life_balance.mean(),inplace=True)\n",
    "cat_features =  list(traintest.select_dtypes(include=object).columns)\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "\n",
    "for i in cat_features:\n",
    "    l = LabelEncoder()\n",
    "    traintest[i] = l.fit_transform(traintest[i].values)\n",
    "    categorical_columns.append(i)\n",
    "    categorical_dims[i]=len(l.classes_)\n",
    "print('anynull_value',traintest.isnull().any().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape (7000, 24)\n",
      "test data shape (3000, 24)\n"
     ]
    }
   ],
   "source": [
    "df = traintest[:7000]\n",
    "test = traintest[7000:]\n",
    "print('training data shape',df.shape)\n",
    "print('test data shape',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns[1:-2])\n",
    "target = 'Attrition_rate'\n",
    "\n",
    "unused_feat=['Employee_ID']\n",
    "target = ['Attrition_rate']\n",
    "\n",
    "feature = [ col for col in df.columns if col not in unused_feat+[target]] \n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(feature) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(feature) if f in categorical_columns]\n",
    "\n",
    "cat_emb_dim = [5, 4, 3, 6, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n"
     ]
    }
   ],
   "source": [
    "model = TabNetRegressor(n_d=16,\n",
    "                       n_a=16,\n",
    "                       n_steps=4,\n",
    "                       gamma=1.3,\n",
    "                       n_independent=2,\n",
    "                       n_shared=2,\n",
    "                       seed=seed,\n",
    "                       optimizer_fn = torch.optim.Adam,\n",
    "                        cat_idxs=cat_idxs,cat_dims=cat_dims,cat_emb_dim=2,\n",
    "                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabNetRegressor(cat_dims=[2, 2, 5, 12, 4, 5], cat_emb_dim=2,\n",
       "                cat_idxs=[0, 3, 4, 5, 6, 13], clip_value=1, device_name='auto',\n",
       "                epsilon=1e-15, gamma=1.3, lambda_sparse=0.001, lr=0.02,\n",
       "                momentum=0.02, n_a=16, n_d=16, n_independent=2, n_shared=2,\n",
       "                n_steps=4, optimizer_fn=<class 'torch.optim.adam.Adam'>,\n",
       "                scheduler_fn=<class 'torch.optim.lr_scheduler.MultiStepLR'>,\n",
       "                scheduler_params={'gamma': 0.2,\n",
       "                                  'milestones': [150, 250, 300, 350, 400, 450]},\n",
       "                seed=2020, verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros((test.shape[0],1, NUM_FOLDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    df_train = df[df.kfold != fold]\n",
    "    df_valid = df[df.kfold == fold]\n",
    "    \n",
    "    X_train = df_train[features].values\n",
    "    Y_train = df_train[target].values.reshape(-1, 1)\n",
    "    \n",
    "    X_valid = df_valid[features].values\n",
    "    Y_valid = df_valid[target].values.reshape(-1, 1)\n",
    "    \n",
    "    y_oof = np.zeros((df_valid.shape[0],1))   # Out of folds validation\n",
    "    \n",
    "    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n",
    "     \n",
    "    model.fit(X_train = X_train,y_train = Y_train,X_valid = X_valid,\n",
    "             y_valid = Y_valid,max_epochs = 1000,patience =70)\n",
    "    print(\"--------Validating For fold {}------------\".format(fold+1))\n",
    "    \n",
    "    y_oof = model.predict(X_valid)\n",
    "    y_test[:,:,fold] = model.predict(test[features].values)\n",
    "    \n",
    "    val_score = metric(Y_valid,y_oof)\n",
    "    \n",
    "    print(\"Validation score: {:<8.5f}\".format(val_score))\n",
    "    \n",
    "    # VISUALIZTION\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(model.history['train']['loss'])\n",
    "    plt.plot(model.history['valid']['loss'])\n",
    "    \n",
    "    plt.plot(model.history['train']['metric'])\n",
    "    plt.plot(model.history['valid']['metric'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 1-------------\n",
      "Will train until validation stopping metric hasn't improved in 70 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index out of range: Tried to access index 34 out of table with 1 rows. at C:\\w\\1\\s\\tmp_conda_3.6_014803\\conda\\conda-bld\\pytorch_1565315401686\\work\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:237",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-d64cf0f07b3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-4f08305aa433>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(fold)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     model.fit(X_train = X_train,y_train = Y_train,X_valid = X_valid,\n\u001b[1;32m---> 16\u001b[1;33m              y_valid = Y_valid,max_epochs = 1000,patience =70)\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------Validating For fold {}------------\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pytorch_tabnet\\tab_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, X_valid, y_valid, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last)\u001b[0m\n\u001b[0;32m    165\u001b[0m                self.patience_counter < self.patience):\n\u001b[0;32m    166\u001b[0m             \u001b[0mstarting_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[0mfit_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# leaving it here, may be used for callbacks later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pytorch_tabnet\\tab_model.py\u001b[0m in \u001b[0;36mfit_epoch\u001b[1;34m(self, train_dataloader, valid_dataloader)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mDataLoader\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mtrain_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[0mvalid_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pytorch_tabnet\\tab_model.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m             \u001b[0my_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y_preds\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m             \u001b[0mys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pytorch_tabnet\\tab_model.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, data, targets)\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    483\u001b[0m                 \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_init_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m                 \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcat_feat_counter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_init_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m                 \u001b[0mcat_feat_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1465\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index out of range: Tried to access index 34 out of table with 1 rows. at C:\\w\\1\\s\\tmp_conda_3.6_014803\\conda\\conda-bld\\pytorch_1565315401686\\work\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:237"
     ]
    }
   ],
   "source": [
    "run(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 2-------------\n",
      "Will train until validation stopping metric hasn't improved in 70 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -0.84041 |  -7.98243 |   3.3       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.16441 |  -11.00894 |   6.6       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.09067 |  -2.53502 |   9.9       \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.06402 |  -1.21058 |   13.1      \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.05442 |  -0.38452 |   16.5      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.04727 |  -0.21744 |   20.0      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04513 |  -0.06270 |   23.4      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.04198 |  -0.05965 |   26.8      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.03953 |  -0.05408 |   30.2      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.03869 |  -0.05081 |   33.5      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.03784 |  -0.04691 |   36.8      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03767 |  -0.04400 |   40.2      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03721 |  -0.04124 |   43.6      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03716 |  -0.04111 |   46.9      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03682 |  -0.03921 |   50.3      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03653 |  -0.03862 |   53.6      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03654 |  -0.03680 |   57.0      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03625 |  -0.03720 |   60.3      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03601 |  -0.03506 |   63.6      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03612 |  -0.03508 |   66.9      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03545 |  -0.03649 |   70.2      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03594 |  -0.03729 |   73.5      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03551 |  -0.03646 |   76.8      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03513 |  -0.03588 |   80.2      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03554 |  -0.03541 |   83.4      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03576 |  -0.03563 |   86.7      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03591 |  -0.03546 |   90.1      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03532 |  -0.03464 |   93.5      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03510 |  -0.03410 |   96.8      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03496 |  -0.03406 |   100.2     \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03503 |  -0.03479 |   103.5     \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03476 |  -0.03457 |   106.9     \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03536 |  -0.03474 |   110.2     \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03525 |  -0.03445 |   113.9     \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03523 |  -0.03395 |   117.3     \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03501 |  -0.03484 |   120.6     \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03532 |  -0.03446 |   123.9     \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03490 |  -0.03435 |   127.2     \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03519 |  -0.03443 |   130.6     \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03457 |  -0.03433 |   134.0     \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03494 |  -0.03491 |   137.4     \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03525 |  -0.03453 |   140.7     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03547 |  -0.03379 |   144.0     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03499 |  -0.03534 |   147.3     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03504 |  -0.03418 |   150.7     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03479 |  -0.03395 |   154.1     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03513 |  -0.03397 |   157.4     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03492 |  -0.03485 |   160.9     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03487 |  -0.03373 |   164.2     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03465 |  -0.03364 |   167.6     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03477 |  -0.03338 |   171.1     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03434 |  -0.03375 |   174.4     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03426 |  -0.03393 |   177.8     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03385 |  -0.03410 |   181.0     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03399 |  -0.03439 |   184.5     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03384 |  -0.03440 |   188.0     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03403 |  -0.03451 |   191.5     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03407 |  -0.03413 |   194.8     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03407 |  -0.03432 |   198.1     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03364 |  -0.03428 |   201.4     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03369 |  -0.03389 |   204.8     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03382 |  -0.03466 |   208.1     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03378 |  -0.03401 |   211.4     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03390 |  -0.03404 |   214.7     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03384 |  -0.03458 |   218.0     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03355 |  -0.03431 |   221.3     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03364 |  -0.03423 |   224.7     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03355 |  -0.03428 |   228.0     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03367 |  -0.03431 |   231.4     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03390 |  -0.03448 |   234.6     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03347 |  -0.03465 |   237.9     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03317 |  -0.03445 |   241.3     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03363 |  -0.03412 |   244.7     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03353 |  -0.03438 |   248.0     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03334 |  -0.03417 |   251.3     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03357 |  -0.03436 |   254.7     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03347 |  -0.03410 |   258.0     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03377 |  -0.03532 |   261.4     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03375 |  -0.03441 |   264.8     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03384 |  -0.03406 |   268.1     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03380 |  -0.03346 |   271.5     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03343 |  -0.03319 |   274.9     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03355 |  -0.03321 |   278.1     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03369 |  -0.03376 |   281.4     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03377 |  -0.03374 |   284.7     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03391 |  -0.03369 |   288.1     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03376 |  -0.03396 |   291.3     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03380 |  -0.03474 |   294.6     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03372 |  -0.03413 |   298.0     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03374 |  -0.03501 |   301.3     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03370 |  -0.03509 |   304.7     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03376 |  -0.03516 |   307.9     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03350 |  -0.03489 |   311.3     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03331 |  -0.03460 |   314.7     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03317 |  -0.03440 |   318.1     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03284 |  -0.03479 |   321.4     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03307 |  -0.03470 |   324.8     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03310 |  -0.03465 |   328.1     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03330 |  -0.03447 |   331.4     \n",
      "Current learning rate:  0.02\n",
      "| 100   | -0.03294 |  -0.03477 |   334.8     \n",
      "Current learning rate:  0.02\n",
      "| 101   | -0.03303 |  -0.03454 |   338.1     \n",
      "Current learning rate:  0.02\n",
      "| 102   | -0.03303 |  -0.03416 |   341.4     \n",
      "Current learning rate:  0.02\n",
      "| 103   | -0.03313 |  -0.03441 |   344.7     \n",
      "Current learning rate:  0.02\n",
      "| 104   | -0.03290 |  -0.03427 |   348.0     \n",
      "Current learning rate:  0.02\n",
      "| 105   | -0.03297 |  -0.03439 |   351.4     \n",
      "Current learning rate:  0.02\n",
      "| 106   | -0.03277 |  -0.03461 |   354.8     \n",
      "Current learning rate:  0.02\n",
      "| 107   | -0.03300 |  -0.03502 |   358.1     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate:  0.02\n",
      "| 108   | -0.03260 |  -0.03437 |   361.5     \n",
      "Current learning rate:  0.02\n",
      "| 109   | -0.03291 |  -0.03454 |   364.8     \n",
      "Current learning rate:  0.02\n",
      "| 110   | -0.03287 |  -0.03472 |   368.1     \n",
      "Current learning rate:  0.02\n",
      "| 111   | -0.03263 |  -0.03506 |   371.5     \n",
      "Current learning rate:  0.02\n",
      "| 112   | -0.03289 |  -0.03481 |   374.9     \n",
      "Current learning rate:  0.02\n",
      "| 113   | -0.03247 |  -0.03501 |   378.3     \n",
      "Current learning rate:  0.02\n",
      "| 114   | -0.03234 |  -0.03529 |   381.6     \n",
      "Current learning rate:  0.02\n",
      "| 115   | -0.03288 |  -0.03575 |   384.9     \n",
      "Current learning rate:  0.02\n",
      "| 116   | -0.03274 |  -0.03538 |   388.3     \n",
      "Current learning rate:  0.02\n",
      "| 117   | -0.03246 |  -0.03525 |   391.7     \n",
      "Current learning rate:  0.02\n",
      "| 118   | -0.03243 |  -0.03493 |   395.0     \n",
      "Current learning rate:  0.02\n",
      "| 119   | -0.03212 |  -0.03563 |   398.4     \n",
      "Current learning rate:  0.02\n",
      "| 120   | -0.03270 |  -0.03568 |   401.6     \n",
      "Current learning rate:  0.02\n",
      "| 121   | -0.03232 |  -0.03508 |   404.9     \n",
      "Current learning rate:  0.02\n",
      "| 122   | -0.03260 |  -0.03564 |   408.2     \n",
      "Current learning rate:  0.02\n",
      "| 123   | -0.03226 |  -0.03634 |   411.5     \n",
      "Current learning rate:  0.02\n",
      "| 124   | -0.03237 |  -0.03594 |   414.9     \n",
      "Current learning rate:  0.02\n",
      "| 125   | -0.03256 |  -0.03553 |   418.1     \n",
      "Current learning rate:  0.02\n",
      "| 126   | -0.03283 |  -0.03553 |   421.8     \n",
      "Current learning rate:  0.02\n",
      "| 127   | -0.03281 |  -0.03536 |   425.1     \n",
      "Current learning rate:  0.02\n",
      "| 128   | -0.03242 |  -0.03567 |   428.4     \n",
      "Current learning rate:  0.02\n",
      "| 129   | -0.03211 |  -0.03560 |   431.7     \n",
      "Current learning rate:  0.02\n",
      "| 130   | -0.03249 |  -0.03610 |   435.1     \n",
      "Current learning rate:  0.02\n",
      "| 131   | -0.03271 |  -0.03509 |   438.5     \n",
      "Current learning rate:  0.02\n",
      "| 132   | -0.03349 |  -0.03441 |   441.9     \n",
      "Current learning rate:  0.02\n",
      "| 133   | -0.03391 |  -0.03451 |   445.2     \n",
      "Current learning rate:  0.02\n",
      "| 134   | -0.03390 |  -0.03469 |   448.5     \n",
      "Current learning rate:  0.02\n",
      "| 135   | -0.03369 |  -0.03552 |   451.9     \n",
      "Current learning rate:  0.02\n",
      "| 136   | -0.03387 |  -0.03402 |   455.3     \n",
      "Current learning rate:  0.02\n",
      "| 137   | -0.03374 |  -0.03484 |   458.6     \n",
      "Current learning rate:  0.02\n",
      "| 138   | -0.03374 |  -0.03455 |   461.9     \n",
      "Current learning rate:  0.02\n",
      "| 139   | -0.03337 |  -0.03483 |   465.3     \n",
      "Current learning rate:  0.02\n",
      "| 140   | -0.03313 |  -0.03451 |   468.5     \n",
      "Current learning rate:  0.02\n",
      "| 141   | -0.03310 |  -0.03459 |   471.9     \n",
      "Current learning rate:  0.02\n",
      "| 142   | -0.03290 |  -0.03469 |   475.3     \n",
      "Current learning rate:  0.02\n",
      "| 143   | -0.03295 |  -0.03460 |   478.6     \n",
      "Current learning rate:  0.02\n",
      "| 144   | -0.03271 |  -0.03475 |   482.0     \n",
      "Current learning rate:  0.02\n",
      "| 145   | -0.03300 |  -0.03466 |   485.3     \n",
      "Current learning rate:  0.02\n",
      "| 146   | -0.03269 |  -0.03489 |   488.6     \n",
      "Current learning rate:  0.02\n",
      "| 147   | -0.03259 |  -0.03493 |   492.0     \n",
      "Current learning rate:  0.02\n",
      "| 148   | -0.03253 |  -0.03491 |   495.3     \n",
      "Current learning rate:  0.02\n",
      "| 149   | -0.03252 |  -0.03565 |   498.7     \n",
      "Current learning rate:  0.004\n",
      "| 150   | -0.03262 |  -0.03520 |   502.0     \n",
      "Current learning rate:  0.004\n",
      "| 151   | -0.03257 |  -0.03518 |   505.2     \n",
      "Current learning rate:  0.004\n",
      "| 152   | -0.03238 |  -0.03508 |   508.7     \n",
      "Early stopping occured at epoch 152\n",
      "Training done in 508.666 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 2------------\n",
      "Validation score: 81.78322\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 3-------------\n",
      "Will train until validation stopping metric hasn't improved in 70 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -1.10019 |  -29.93924 |   3.4       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.17531 |  -2.37999 |   6.8       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.09277 |  -0.64758 |   10.2      \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.06289 |  -0.43943 |   13.6      \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.04994 |  -0.61079 |   16.9      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.04482 |  -0.58944 |   20.2      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04166 |  -0.42754 |   23.6      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.04022 |  -0.21011 |   27.0      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.03941 |  -0.09931 |   30.2      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.03780 |  -0.07610 |   33.5      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.03715 |  -0.06666 |   36.8      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03670 |  -0.05583 |   40.1      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03651 |  -0.04450 |   43.5      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03619 |  -0.04223 |   46.9      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03604 |  -0.04118 |   50.2      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03541 |  -0.04049 |   53.7      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03509 |  -0.03968 |   57.1      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03528 |  -0.03860 |   60.5      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03525 |  -0.03723 |   63.8      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03474 |  -0.03730 |   67.2      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03499 |  -0.03695 |   70.6      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03518 |  -0.03620 |   73.9      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03501 |  -0.03585 |   77.3      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03475 |  -0.03612 |   80.7      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03488 |  -0.03597 |   84.0      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03469 |  -0.03553 |   87.3      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03458 |  -0.03518 |   90.8      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03443 |  -0.03568 |   94.2      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03433 |  -0.03509 |   97.5      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03439 |  -0.03531 |   100.7     \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03438 |  -0.03531 |   104.1     \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03494 |  -0.03555 |   107.4     \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03453 |  -0.03543 |   110.7     \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03440 |  -0.03548 |   114.1     \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03412 |  -0.03576 |   117.5     \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03407 |  -0.03617 |   120.9     \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03420 |  -0.03608 |   124.2     \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03438 |  -0.03584 |   127.5     \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03441 |  -0.03576 |   130.8     \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03410 |  -0.03556 |   134.1     \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03405 |  -0.03588 |   137.5     \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03433 |  -0.03652 |   140.9     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03434 |  -0.03561 |   144.3     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03398 |  -0.03542 |   147.6     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03369 |  -0.03566 |   151.0     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03389 |  -0.03533 |   154.3     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03406 |  -0.03532 |   157.7     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03377 |  -0.03569 |   161.0     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03395 |  -0.03589 |   164.3     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03365 |  -0.03536 |   167.7     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03365 |  -0.03587 |   171.4     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03350 |  -0.03551 |   174.7     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03387 |  -0.03550 |   178.2     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03375 |  -0.03560 |   181.5     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03350 |  -0.03530 |   184.9     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03372 |  -0.03607 |   188.3     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03366 |  -0.03612 |   191.6     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03347 |  -0.03589 |   195.0     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03351 |  -0.03577 |   198.3     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03358 |  -0.03579 |   201.7     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03394 |  -0.03638 |   205.1     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03366 |  -0.03635 |   208.4     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03352 |  -0.03562 |   211.8     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03339 |  -0.03601 |   215.1     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03304 |  -0.03537 |   218.5     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03329 |  -0.03576 |   221.8     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03379 |  -0.03618 |   225.1     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03332 |  -0.03626 |   228.4     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03320 |  -0.03645 |   231.8     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03316 |  -0.03614 |   235.1     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03329 |  -0.03584 |   238.4     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03308 |  -0.03605 |   241.7     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03294 |  -0.03604 |   245.0     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03318 |  -0.03646 |   248.4     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03308 |  -0.03599 |   251.8     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03329 |  -0.03563 |   255.0     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03321 |  -0.03637 |   258.3     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03298 |  -0.03675 |   261.6     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03295 |  -0.03681 |   265.0     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03314 |  -0.03584 |   268.4     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03263 |  -0.03568 |   271.7     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03271 |  -0.03586 |   275.0     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03254 |  -0.03626 |   278.3     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03255 |  -0.03592 |   281.7     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03282 |  -0.03601 |   285.0     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03275 |  -0.03613 |   288.4     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03282 |  -0.03672 |   291.7     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03279 |  -0.03660 |   295.0     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03278 |  -0.03640 |   298.2     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03264 |  -0.03658 |   301.9     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03238 |  -0.03616 |   305.2     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03237 |  -0.03629 |   308.6     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03239 |  -0.03646 |   311.9     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03218 |  -0.03606 |   315.2     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03241 |  -0.03611 |   318.6     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03260 |  -0.03628 |   321.9     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03224 |  -0.03670 |   325.1     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03236 |  -0.03657 |   328.4     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03315 |  -0.03651 |   331.8     \n",
      "Early stopping occured at epoch 99\n",
      "Training done in 331.811 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 3------------\n",
      "Validation score: 81.26735\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 4-------------\n",
      "Will train until validation stopping metric hasn't improved in 70 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -1.61117 |  -27.51681 |   3.4       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.24628 |  -14.99469 |   6.8       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.11892 |  -8.10575 |   10.1      \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.08663 |  -3.26861 |   13.4      \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.06627 |  -0.36399 |   16.8      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.05479 |  -0.31093 |   20.1      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04814 |  -0.26099 |   23.7      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.04451 |  -0.34716 |   27.1      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.04162 |  -0.19048 |   30.4      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.04010 |  -0.12609 |   33.7      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.03959 |  -0.09529 |   37.2      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03891 |  -0.07550 |   40.5      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03863 |  -0.06609 |   43.8      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03847 |  -0.04459 |   47.2      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03811 |  -0.04212 |   50.5      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03780 |  -0.04001 |   53.8      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03727 |  -0.03691 |   57.2      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03657 |  -0.03569 |   60.6      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03662 |  -0.03499 |   64.0      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03676 |  -0.03504 |   67.2      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03646 |  -0.03621 |   70.6      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03635 |  -0.03908 |   73.9      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03584 |  -0.03796 |   77.2      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03613 |  -0.03748 |   80.5      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03594 |  -0.03724 |   83.8      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03599 |  -0.03539 |   87.1      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03560 |  -0.03358 |   90.5      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03575 |  -0.03277 |   93.9      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03553 |  -0.03300 |   97.3      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03536 |  -0.03370 |   100.6     \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03554 |  -0.03225 |   104.4     \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03524 |  -0.03178 |   107.8     \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03569 |  -0.03177 |   111.1     \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03509 |  -0.03236 |   114.4     \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03568 |  -0.03202 |   117.7     \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03518 |  -0.03201 |   121.1     \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03472 |  -0.03181 |   124.5     \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03479 |  -0.03109 |   127.8     \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03485 |  -0.03083 |   131.1     \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03480 |  -0.03101 |   134.5     \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03488 |  -0.03103 |   137.9     \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03510 |  -0.03130 |   141.2     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03544 |  -0.03046 |   144.5     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03527 |  -0.03031 |   147.8     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03521 |  -0.03067 |   151.2     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03497 |  -0.03072 |   154.5     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03511 |  -0.03067 |   157.8     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03483 |  -0.03076 |   161.1     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03474 |  -0.03069 |   164.4     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03456 |  -0.03048 |   167.7     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03458 |  -0.03077 |   171.1     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03481 |  -0.03079 |   174.4     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03473 |  -0.03090 |   177.7     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03480 |  -0.03061 |   181.0     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03442 |  -0.03074 |   184.4     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03475 |  -0.03095 |   187.7     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03457 |  -0.03094 |   191.1     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03468 |  -0.03086 |   194.4     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03453 |  -0.03074 |   197.9     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03440 |  -0.03087 |   201.2     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03429 |  -0.03084 |   204.5     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03429 |  -0.03076 |   207.8     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03439 |  -0.03082 |   211.2     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03436 |  -0.03073 |   214.5     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03414 |  -0.03083 |   217.8     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03404 |  -0.03078 |   221.1     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03391 |  -0.03085 |   224.4     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03414 |  -0.03162 |   227.7     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03474 |  -0.03072 |   231.0     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03425 |  -0.03126 |   234.3     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03426 |  -0.03118 |   237.7     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03445 |  -0.03109 |   241.0     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03409 |  -0.03083 |   244.3     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03419 |  -0.03080 |   247.6     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03433 |  -0.03128 |   250.9     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03460 |  -0.03177 |   254.3     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03469 |  -0.03131 |   257.6     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03436 |  -0.03144 |   260.9     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03435 |  -0.03100 |   264.2     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03453 |  -0.03131 |   267.7     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03447 |  -0.03078 |   271.1     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03448 |  -0.03056 |   274.4     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03421 |  -0.03098 |   277.6     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03440 |  -0.03109 |   280.9     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03436 |  -0.03068 |   284.2     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03435 |  -0.03076 |   287.5     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03443 |  -0.03082 |   290.8     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03430 |  -0.03094 |   294.2     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03427 |  -0.03067 |   297.5     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03416 |  -0.03063 |   300.8     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03384 |  -0.03127 |   304.2     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03432 |  -0.03102 |   307.5     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03408 |  -0.03094 |   310.7     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03385 |  -0.03088 |   314.0     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03386 |  -0.03131 |   317.4     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03386 |  -0.03109 |   320.8     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03377 |  -0.03109 |   324.1     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03387 |  -0.03112 |   327.4     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03377 |  -0.03135 |   330.7     \n",
      "Current learning rate:  0.02\n",
      "| 100   | -0.03389 |  -0.03093 |   334.0     \n",
      "Current learning rate:  0.02\n",
      "| 101   | -0.03413 |  -0.03170 |   337.4     \n",
      "Current learning rate:  0.02\n",
      "| 102   | -0.03379 |  -0.03173 |   340.7     \n",
      "Current learning rate:  0.02\n",
      "| 103   | -0.03408 |  -0.03162 |   344.0     \n",
      "Current learning rate:  0.02\n",
      "| 104   | -0.03407 |  -0.03145 |   347.3     \n",
      "Current learning rate:  0.02\n",
      "| 105   | -0.03384 |  -0.03149 |   350.7     \n",
      "Current learning rate:  0.02\n",
      "| 106   | -0.03383 |  -0.03175 |   354.1     \n",
      "Current learning rate:  0.02\n",
      "| 107   | -0.03368 |  -0.03222 |   357.4     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate:  0.02\n",
      "| 108   | -0.03366 |  -0.03241 |   360.6     \n",
      "Current learning rate:  0.02\n",
      "| 109   | -0.03387 |  -0.03253 |   364.0     \n",
      "Current learning rate:  0.02\n",
      "| 110   | -0.03392 |  -0.03234 |   367.3     \n",
      "Current learning rate:  0.02\n",
      "| 111   | -0.03367 |  -0.03217 |   370.7     \n",
      "Current learning rate:  0.02\n",
      "| 112   | -0.03362 |  -0.03241 |   374.1     \n",
      "Current learning rate:  0.02\n",
      "| 113   | -0.03375 |  -0.03258 |   377.4     \n",
      "Current learning rate:  0.02\n",
      "| 114   | -0.03370 |  -0.03220 |   380.7     \n",
      "Early stopping occured at epoch 114\n",
      "Training done in 380.723 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 4------------\n",
      "Validation score: 82.58921\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAFlCAYAAADiVIA6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc+ElEQVR4nO3db4xl91kf8O8z/+yZ9djetXfjXTuJQ+omNqg4YZUGUqGUlBJoREA0aiKBopbKvAA1qajahL6AvqiaSvwpL1okQwKRCkEof5oUAiRyUQMSBNYQEgfnX5NA1l5713HsXdtr7+7Mry/unc14PeOdnbkzd+b+Ph/p6tx77r3nPLNndua7zz7nnmqtBQAAejA17gIAAGCnCL8AAHRD+AUAoBvCLwAA3RB+AQDohvALAEA3ZnZyZzfeeGO79dZbd3KXAAB06N57732ktXbw0vU7Gn5vvfXWHDt2bCd3CQBAh6rqb9dab+wBAIBuCL8AAHRD+AUAoBvCLwAA3RB+AQDohvALAEA3hF8AALoh/AIA0A3hFwCAbgi/AAB0Q/gFAKAbkx9+Tz+YfOFjyfmz464EAIAxm/zw++X/m/zWm5IzJ8ZdCQAAYzb54Xd2frDU+QUA6N7kh9+5fYOl8AsA0L3JD78rnd9zT463DgAAxq6f8KvzCwDQvQ7C78Jgef6p8dYBAMDYdRB+dX4BABjoIPyunPCm8wsA0LsOwu9K51f4BQDoXUfh19gDAEDvJj/8Tk0n01fp/AIA0EH4TQbdX51fAIDu9RF+5/bp/AIA0En4nZ1Pzgm/AAC96yf8GnsAAOheJ+F3wdgDAAC9hF+dXwAAugm/TngDAKCb8Dsv/AIA0FP4NfYAANC7TsKvE94AAOgl/M4t6PwCANBJ+J1dSJbOJUsXxl0JAABj1En4nR8sjT4AAHTtsuG3ql5YVX9UVfdX1Wer6m3D9T9XVQ9U1aeGtx/Y/nI36WL4NfoAANCzmQ285kKSn26t/WVVLSa5t6o+Pnzul1prP7995Y3I7MJgqfMLANC1y4bf1tqJJCeG989U1f1Jbt7uwkbqYvjV+QUA6NkVzfxW1a1JXpHkk8NVP1VVn66q91TV/hHXNjo6vwAA5ArCb1Vdk+QDSd7eWjud5FeSvDTJnRl0hn9hnffdVVXHqurYqVOnRlDyJjjhDQCAbDD8VtVsBsH3N1trH0yS1trDrbWl1tpykl9N8qq13ttau7u1drS1dvTgwYOjqvvKGHsAACAb+7SHSvLuJPe31n5x1frDq172w0nuG315I6LzCwBANvZpD69J8mNJPlNVnxqu+5kkb6mqO5O0JF9N8hPbUuEozOn8AgCwsU97+JMktcZTHx19OdtkZezh3JPjrQMAgLHq7ApvOr8AAD3rI/zOCL8AAPQSfqdnkuk5J7wBAHSuj/CbDOZ+hV8AgK4JvwAAdKOj8Dtv5hcAoHMdhd8F4RcAoHP9hN85Yw8AAL3rJ/zOzifnhF8AgJ51FH6NPQAA9K6j8Dtv7AEAoHOdhV+dXwCAnnUUfvfp/AIAdK6j8GvsAQCgdx2F34Vk6VyydGHclQAAMCYdhd/5wfKCuV8AgF71F36d9AYA0K1+wu/cvsHy3JPjrQMAgLHpJ/zq/AIAdK+j8LswWAq/AADd6ij8rnR+fdwZAECvOgq/Or8AAL3rMPw64Q0AoFcdhV8nvAEA9K6j8LvS+TXzCwDQq47Cr84vAEDvOgq/TngDAOhdP+F3eiaZnnOFNwCAjvUTfpPB6IPOLwBAtzoLvwtOeAMA6Fhn4VfnFwCgZ52F333CLwBAxzoLv/Ou8AYA0LEOw6/OLwBArzoLv054AwDoWWfhV+cXAKBnfYXfuYXknM4vAECv+gq/xh4AALrWWfg19gAA0LPOwu9CsvRMsrw07koAABiDzsLv/GCp+wsA0KXOwu/CYGnuFwCgS8IvAADd6Cz8GnsAAOhZZ+FX5xcAoGeXDb9V9cKq+qOqur+qPltVbxuuP1BVH6+qLw6X+7e/3C3S+QUA6NpGOr8Xkvx0a+32JK9O8pNVdUeSdyS5p7V2W5J7ho93t7l9g6WrvAEAdOmy4be1dqK19pfD+2eS3J/k5iRvTPLe4cvem+SHtqvIkbnY+RV+AQB6dEUzv1V1a5JXJPlkkhe01k4kg4Cc5NCoixs5Yw8AAF3bcPitqmuSfCDJ21trp6/gfXdV1bGqOnbq1KnN1Dg6TngDAOjahsJvVc1mEHx/s7X2weHqh6vq8PD5w0lOrvXe1trdrbWjrbWjBw8eHEXNmyf8AgB0bSOf9lBJ3p3k/tbaL6566iNJ3jq8/9YkHx59eSNm7AEAoGszG3jNa5L8WJLPVNWnhut+Jsm7kvxOVf14kr9L8qbtKXGEpmeTqVmdXwCATl02/LbW/iRJrfP060Zbzg6YXdD5BQDoVF9XeEsGow86vwAAXeov/M4tuMgFAECn+gu/xh4AALrVYfg19gAA0KtOw6/OLwBAjzoMvws6vwAAnRJ+AQDoRqfh19gDAECPOgy/TngDAOhVp+FX5xcAoEcdht+F5MLTyfLyuCsBAGCH9Rd+5xYGS6MPAADd6S/8zq6EX6MPAAC96TD8zg+WOr8AAN3pOPzq/AIA9KbD8LtvsDz/5HjrAABgx3UYfnV+AQB61WH4dcIbAECvOgy/TngDAOhVx+FX5xcAoDf9hd+54Qlv55zwBgDQm/7Cr84vAEC3Ogy/TngDAOhVf+F3ejaZmnHCGwBAh/oLv8mg+6vzCwDQnY7DrxPeAAB602n4ndf5BQDoUKfh19gDAECPOg2/8054AwDoUL/h95zwCwDQmz7D79w+nV8AgA71GX6d8AYA0KVOw68T3gAAetRp+HXCGwBAjzoNvwvCLwBAh/oNvxeeTpaXx10JAAA7qNPwOz9YXjD3CwDQk07D78Jg6aQ3AICudBp+h51fc78AAF3pM/zODTu/rvIGANCVPsPvxbEH4RcAoCedht+VsQczvwAAPek0/DrhDQCgR52G35XO75PjrQMAgB3VafjV+QUA6NFlw29VvaeqTlbVfavW/VxVPVBVnxrefmB7yxwxJ7wBAHRpI53f30jy+jXW/1Jr7c7h7aOjLWubOeENAKBLlw2/rbVPJHl0B2rZOTq/AABd2srM709V1aeHYxH713tRVd1VVceq6tipU6e2sLsRmp5NpmaSc054AwDoyWbD768keWmSO5OcSPIL672wtXZ3a+1oa+3owYMHN7m7EatKrr4+OfvYuCsBAGAHbSr8ttYebq0ttdaWk/xqkleNtqwdsHAgOTtZ0xwAADy/TYXfqjq86uEPJ7lvvdfuWvP7k7PfGHcVAADsoJnLvaCq3pfktUlurKrjSX42yWur6s4kLclXk/zENta4PeYPJI8fH3cVAADsoMuG39baW9ZY/e5tqGVnze9PHvrMuKsAAGAH9XmFt8TMLwBAh/oNv/PXDz7n9/zT464EAIAd0nH4PTBYOukNAKAb/YbfhZXwa/QBAKAX/Ybf+eFF6XR+AQC60XH4HXZ+n9L5BQDoRcfhV+cXAKA3/YZfM78AAN3pN/zOLiTTczq/AAAd6Tf8Vg3mfs38AgB0o9/wmwyv8qbzCwDQi77D7/x+4RcAoCPCr7EHAIBuCL86vwAA3eg7/C4cGHzUWWvjrgQAgB3Qd/id358snUvOPzXuSgAA2AGdh1+XOAYA6Enf4ffiVd7M/QIA9KDv8Du/f7B0iWMAgC50Hn51fgEAetJ5+B12fs38AgB0QfhNjD0AAHSi7/A7e3Uyu5CcfWzclQAAsAP6Dr/JYO7X2AMAQBeEX5c4BgDohvC7sN/MLwBAJ4Tf+QM6vwAAnRB+5/eb+QUA6ITwuzDs/LY27koAANhmwu/8/qQtJc+cHnclAABsM+HXJY4BALoh/LrEMQBAN4TfhZXOr/ALADDphN+LYw8ucQwAMOmEX2MPAADdEH5Xwq8T3gAAJp7wOz2TXHWtmV8AgA4Iv8mg+6vzCwAw8YTfxCWOAQA6Ifwm37zEMQAAE034TYZjDzq/AACTTvhNBp/1q/MLADDxhN9kOPbwWLK8NO5KAADYRsJvMvys35Y8/fi4KwEAYBsJv8mqSxwbfQAAmGSXDb9V9Z6qOllV961ad6CqPl5VXxwu929vmdvMJY4BALqwkc7vbyR5/SXr3pHkntbabUnuGT7euxZ0fgEAenDZ8Nta+0SSS1uib0zy3uH99yb5oRHXtbNWOr8+7gwAYKJtdub3Ba21E0kyXB5a74VVdVdVHauqY6dOndrk7rbZxfCr8wsAMMm2/YS31trdrbWjrbWjBw8e3O7dbc7V1ycpM78AABNus+H34ao6nCTD5cnRlTQGU1PJ/PU6vwAAE26z4fcjSd46vP/WJB8eTTljNH/AzC8AwITbyEedvS/JnyZ5WVUdr6ofT/KuJN9bVV9M8r3Dx3vb/H6dXwCACTdzuRe01t6yzlOvG3Et47VwIHlib09vAADw/FzhbcX8fmMPAAATTvhdMX8gOfvYuKsAAGAbCb8r5vcnz5xOls6PuxIAALaJ8Lvi4iWOdX8BACaV8LvCJY4BACae8LvCJY4BACae8LtiZezBJY4BACaW8LtC5xcAYOIJvyvmV0540/kFAJhUwu+KqxaTqRljDwAAE0z4XVE1vMqbsQcAgEkl/K42f8DYAwDABBN+V9P5BQCYaMLvagsHkqeEXwCASSX8rja/39gDAMAEE35X23cweeJk0tq4KwEAYBsIv6tdeyRZPp889fVxVwIAwDYQfldbvGmwPHNivHUAALAthN/VFo8MlqeFXwCASST8rqbzCwAw0YTf1S6G34fGWwcAANtC+F1tenbwiQ9nHhx3JQAAbAPh91KLN+n8AgBMKOH3UotHktM6vwAAk0j4vZTOLwDAxBJ+L3XtkeTJU8nS+XFXAgDAiAm/l1q8KUlLnnh43JUAADBiwu+lVi50YfQBAGDiCL+XWvmsXye9AQBMHOH3UouHB0udXwCAiSP8XmrhhmRq1oUuAAAmkPB7qakpH3cGADChhN+1LB428wsAMIGE37Xo/AIATCThdy3XHhF+AQAmkPC7lsWbkmceT849Oe5KAAAYIeF3LT7uDABgIgm/a1kJv056AwCYKMLvWnR+AQAmkvC7lmtXwq/OLwDAJBF+13LVYjJ3jc4vAMCEEX7Xs3g4OXNi3FUAADBCwu96Fm9KTgu/AACTRPhdj84vAMDEEX7Xc+3hwcxva+OuBACAEZnZypur6qtJziRZSnKhtXZ0FEXtCouHk6VnkrPfSBYOjLsaAABGYEvhd+gft9YeGcF2dpfVF7oQfgEAJoKxh/W40AUAwMTZavhtST5WVfdW1V1rvaCq7qqqY1V17NSpU1vc3Q66eKELJ70BAEyKrYbf17TWXpnk+5P8ZFV996UvaK3d3Vo72lo7evDgwS3ubgdd84LBUvgFAJgYWwq/rbUHh8uTST6U5FWjKGpXmLkqWbhB+AUAmCCbDr9Vta+qFlfuJ/mnSe4bVWG7wuIRF7oAAJggW/m0hxck+VBVrWznt1prfzCSqnaLxZt0fgEAJsimw29r7ctJvn2Etew+1x5OHvr0uKsAAGBEfNTZ81k8nDxxMlm6MO5KAAAYAeH3+SweTtKSJ0+OuxIAAEZA+H0+F6/yZu4XAGASCL/PZ/GmwdJJbwAAE0H4fT7XHhkshV8AgIkg/D6fhRuTqRnhFwBgQgi/z2dqKrnmpuTMQ+OuBACAERB+L2fxpuT0g+OuAgCAERB+L+fawzq/AAATQvi9nMXDyRmdXwCASSD8Xs7iTcnTjyfnnhp3JQAAbJHwezmLPu4MAGBSCL+X40IXAAATQ/i9nP0vHiwf/cp46wAAYMuE38u5/tZkZj45ef+4KwEAYIuE38uZmkoOvTw5+TfjrgQAgC0Sfjfi0B06vwAAE0D43YhDtydPPJQ89ei4KwEAYAuE3404dPtgqfsLALCnCb8bceiOwdLcLwDAnib8bsTi4eTq63R+AQD2OOF3I6qc9AYAMAGE3406dPtg7KG1cVcCAMAmCb8bdeiO5OnHkjMPjbsSAAA2SfjdqIuf+OCkNwCAvUr43aiDwi8AwF4n/G7UvhuSa17gpDcAgD1M+L0SKye9AQCwJwm/V+LQHcnJzyXLy+OuBACATRB+r8Sh25MLZ5PHvjruSgAA2ISJD7/3PfB43vnBT+f80gi6tRcvc2zuFwBgL5r48Hvi8afzvj//Wj7xhVNb39jBlw2W5n4BAPakiQ+/r33Zwdywby7vv/f41jd21WJy/Yt0fgEA9qiJD7+z01N545035577T+axp85tfYOH7hB+AQD2qIkPv0nyI99xc84tLed///WDW9/YoduTR76QXBhBkAYAYEd1EX6/9ch1eflNi6MZfTh0R7J8IXn0/219WwAA7Kguwm+S/PPvuCV/ffzxfOnkma1t6JDLHAMA7FXdhN833nlzpqcq77/3ga1t6Ibbkpo29wsAsAd1E34PLl6V1/79g/nQXx3P0nLb/IZmr05ueKnwCwCwB3UTfpPB6MPDp5/Jn3zpka1t6NDtxh4AAPagrsLv99x+KNfNz+YDWz3x7dAdyaNfSc49NZrCAADYEV2F36tmpvOD334kf/jZh3L66fOb39Ch25O05JHPj6w2AAC2X1fhNxmMPjxzYTm/9+kTm9/IoTsGS3O/AAB7Snfh9x/ccl3+3qFrtjb6sP8lyfRVyRc/lhw/ljz+QLJ0YePvby05fSI58/DmawAA4IrNjLuAnVZV+ZFX3pL/+gefy1ceeTIvuXHflW9keia5+TuSz35ocEuSmkqueUGyeDhZvGl4OzxYt+9gcvqBQaf45P2Dk+WefmzwviOvTG5/Q/LyNyQHXza6LxQAgOeo1jb/sV9V9fokv5xkOsmvtdbe9XyvP3r0aDt27Nim9zcqDz3+dL7rXffk2vnZvPolN+Q7X3pDXv0tN+S2Q9dkaqo2tpHzZ5NHvpicfjA58+Cgk7ty/8zDyRMPJU99/dnvueq6wbzwodsHoxPnziSf+73kgXsHz99wW/Ky1ycHX55c/+Jk/4uTa29OpqZH+wcAADDhqure1trR56zfbPitqukkX0jyvUmOJ/mLJG9pra37GWC7JfwmyR9/8VQ+/KkH82df/nqOf+NskuTAvrl8283X5fr52Vw3vF07P5Nrr57NwlUzmZ+dHtzmpjI/O5OrZqcyOzWV2ZnK7PTU8FaZqsrMVGV6+VzqyZPJE6cGneBrjyS1Rrg+/WDy+Y8m9/9u8tU/Hlw+ecXUTHLdLcm+Q8nV1z37dtViMrcvmZ1PZhcGy5mrk+m5ZHo2mZoddKmnZgbrZxeSuYVkdt9gPQDAhNqO8PudSX6utfZ9w8fvTJLW2n9Z7z27Kfyu9rVHn8qfffnr+dMvfz1fOvlEHj97Po+fPZ/TZ89nK9fDSJKpSqanBoF4cBuMXlQlU6uXGayfyYUcziO5OQ/n5nYqR/JwjrSTuT6nc017KtfkyVzTnsxinsxMljZd1zOZzbnMZTlTGXyJleVUWiqpSkvSVh4nWR6Oh6+8vmVq+Pzgvbn4+jwr4K+8fy0r705bvZUM9zq4Ta2q5LnbXL02ac/6h8Xa+13Z0mAbK3V/8/GlX/flvo5n/wlcuq/V+6x1XpVc+i22wf97WPf9G9/IN4/b2tt99vpae0+X9axjtMGvbrP7unQr7C6rj+t6x3itv3+j+X549j42sv0r2e9Gv7ev1Cj/zmxXjRsxyvqu/Pvh0u2O7vtpfN+bG3n3aL7Sjf3mWr+aJ+YO5ZX/4fdHUMmVWy/8bqX9d3OSr616fDzJP1xjx3cluStJXvSiF21hd9vnhQcW8sIDC3nT0Rc+a31rLU88cyGnn76Qs+cu5Oy55Zw9vzS4nbuQZy4s5/xSy/ml5VxYWs654f2l5Zbl5ZYLyy1Lyy1LrWW5tbSWLC+3LLekZfCalkH2W24r91taO5LWkm+k5dGWfGatolvL9PK5zC0/nbn2dGaXn8nc8tOZbWcz3S5kqi1lul0Y3M9SZldeu3x21Xuevhgy09pz7j8rErbli1FpKstJkmrLWfkLcDFGtrV/uVXaOj/Yhu+8GFxr8GdQK+F6eBs+X+2S/a2xr6zzD7rVEf05v4Rbe9bzG69/9fp61vqV9w22v2ZJWe/H/VZ+CWzkh903/+Gx8e2uV9da/5mxVjV1Bf/Qbrn0HzPDlWvsq1pb+7XPU8saW9lwbRuzTrFjNtpf01v3rO+nGhzL1f90ziWPxuE531truJLv7crGv5b1Q8dzn7ncNtf7cx2djf/MWKuC52sgbHxvz7WZr3Tjf5Yrrx/Vn+d6e95o1F17Cxv4Fl7f8/36ep4Nr/4dsnT1gS0UsD22En439L3aWrs7yd3JoPO7hf3tuKrK4tWzWbx6dtylAAAwAlv5qLPjSVa3Sm9J8uDWygEAgO2zlfD7F0luq6qXVNVckjcn+choygIAgNHb9NhDa+1CVf1Ukj/M4KPO3tNa++zIKgMAgBHb0uddtdY+muSjI6oFAAC2VXeXNwYAoF/CLwAA3RB+AQDohvALAEA3hF8AALoh/AIA0A3hFwCAbgi/AAB0Q/gFAKAb1VrbuZ1VnUrytzu2w2+6MckjY9gvm+N47S2O197ieO0djtXe4njtPi9urR28dOWOht9xqapjrbWj466DjXG89hbHa29xvPYOx2pvcbz2DmMPAAB0Q/gFAKAbvYTfu8ddAFfE8dpbHK+9xfHaOxyrvcXx2iO6mPkFAICkn84vAABMfvitqtdX1eer6ktV9Y5x18OzVdULq+qPqur+qvpsVb1tuP5AVX28qr44XO4fd60MVNV0Vf1VVf3u8LFjtUtV1fVV9f6q+tzw79h3Ol67V1X92+HPwfuq6n1VdbXjtXtU1Xuq6mRV3bdq3brHp6reOcwen6+q7xtP1axlosNvVU0n+e9Jvj/JHUneUlV3jLcqLnEhyU+31m5P8uokPzk8Ru9Ick9r7bYk9wwfszu8Lcn9qx47VrvXLyf5g9bay5N8ewbHzfHaharq5iT/JsnR1tq3JZlO8uY4XrvJbyR5/SXr1jw+w99jb07yrcP3/I9hJmEXmOjwm+RVSb7UWvtya+1ckt9O8sYx18QqrbUTrbW/HN4/k8Ev55szOE7vHb7svUl+aDwVslpV3ZLknyX5tVWrHatdqKquTfLdSd6dJK21c621x+J47WYzSearaibJQpIH43jtGq21TyR59JLV6x2fNyb57dbaM621ryT5UgaZhF1g0sPvzUm+turx8eE6dqGqujXJK5J8MskLWmsnkkFATnJofJWxyn9L8u+TLK9a51jtTt+S5FSSXx+OqfxaVe2L47UrtdYeSPLzSf4uyYkkj7fWPhbHa7db7/jIH7vYpIffWmOdj7fYharqmiQfSPL21trpcdfDc1XVG5KcbK3dO+5a2JCZJK9M8iuttVckeTL+y3zXGs6KvjHJS5IcSbKvqn50vFWxBfLHLjbp4fd4kheuenxLBv+NxC5SVbMZBN/fbK19cLj64ao6PHz+cJKT46qPi16T5Aer6qsZjBB9T1X9zzhWu9XxJMdba58cPn5/BmHY8dqd/kmSr7TWTrXWzif5YJLviuO12613fOSPXWzSw+9fJLmtql5SVXMZDJ9/ZMw1sUpVVQYzife31n5x1VMfSfLW4f23JvnwTtfGs7XW3tlau6W1dmsGf5f+T2vtR+NY7UqttYeSfK2qXjZc9bokfxPHa7f6uySvrqqF4c/F12VwDoTjtbutd3w+kuTNVXVVVb0kyW1J/nwM9bGGib/IRVX9QAZzitNJ3tNa+89jLolVquofJfnjJJ/JN+dIfyaDud/fSfKiDH4pvKm1dumJBoxJVb02yb9rrb2hqm6IY7UrVdWdGZycOJfky0n+ZQZND8drF6qq/5TkX2TwKTh/leRfJ7kmjteuUFXvS/LaJDcmeTjJzyb5X1nn+FTVf0zyrzI4nm9vrf3+GMpmDRMffgEAYMWkjz0AAMBFwi8AAN0QfgEA6IbwCwBAN4RfAAC6IfwCANAN4RcAgG4IvwAAdOP/Axxo1ydlDbC2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 5-------------\n",
      "Will train until validation stopping metric hasn't improved in 70 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -1.35234 |  -25.76419 |   2.7       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.15596 |  -5.87070 |   5.4       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.07371 |  -1.15915 |   9.9       \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.05622 |  -0.69486 |   13.1      \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.04625 |  -0.43828 |   16.9      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.04239 |  -0.29464 |   20.6      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04163 |  -0.27035 |   23.6      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.03872 |  -0.21341 |   26.1      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.03846 |  -0.17299 |   28.6      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.03654 |  -0.12210 |   31.0      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.03688 |  -0.08953 |   33.5      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03576 |  -0.07450 |   36.1      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03594 |  -0.06348 |   38.6      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03586 |  -0.05529 |   41.0      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03561 |  -0.06158 |   43.4      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03550 |  -0.05096 |   46.0      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03543 |  -0.04345 |   48.5      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03544 |  -0.04253 |   50.9      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03527 |  -0.04019 |   53.4      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03502 |  -0.04000 |   55.9      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03520 |  -0.04015 |   58.3      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03526 |  -0.03992 |   61.3      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03480 |  -0.03865 |   63.8      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03492 |  -0.03900 |   66.2      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03490 |  -0.03985 |   68.7      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03506 |  -0.04006 |   71.1      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03493 |  -0.03898 |   73.5      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03495 |  -0.03755 |   76.0      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03500 |  -0.03769 |   78.5      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03447 |  -0.03750 |   80.9      \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03410 |  -0.03765 |   83.4      \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03439 |  -0.03688 |   86.0      \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03419 |  -0.03647 |   88.7      \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03372 |  -0.03661 |   91.2      \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03445 |  -0.03696 |   93.7      \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03434 |  -0.03688 |   96.5      \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03405 |  -0.03668 |   100.1     \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03394 |  -0.03657 |   102.6     \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03393 |  -0.03645 |   105.1     \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03380 |  -0.03605 |   107.6     \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03398 |  -0.03633 |   110.0     \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03383 |  -0.03600 |   112.6     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03394 |  -0.03566 |   115.2     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03394 |  -0.03579 |   117.7     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03414 |  -0.03530 |   120.2     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03399 |  -0.03550 |   122.9     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03407 |  -0.03555 |   126.0     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03435 |  -0.03570 |   128.6     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03396 |  -0.03532 |   131.0     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03403 |  -0.03573 |   133.7     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03409 |  -0.03543 |   136.4     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03382 |  -0.03616 |   138.8     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03379 |  -0.03548 |   141.3     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03376 |  -0.03522 |   143.9     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03380 |  -0.03516 |   146.4     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03370 |  -0.03509 |   149.0     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03364 |  -0.03535 |   151.6     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03372 |  -0.03572 |   154.4     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03348 |  -0.03564 |   157.1     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03361 |  -0.03590 |   159.6     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03340 |  -0.03584 |   162.1     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03351 |  -0.03563 |   164.8     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03340 |  -0.03574 |   167.4     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03322 |  -0.03566 |   169.9     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03338 |  -0.03564 |   172.6     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03328 |  -0.03574 |   175.1     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03327 |  -0.03581 |   177.5     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03337 |  -0.03596 |   180.2     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03320 |  -0.03601 |   182.9     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03317 |  -0.03692 |   185.5     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03367 |  -0.03613 |   188.0     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03321 |  -0.03670 |   190.4     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03337 |  -0.03615 |   192.8     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03320 |  -0.03646 |   195.2     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03312 |  -0.03647 |   197.6     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03305 |  -0.03600 |   200.0     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03324 |  -0.03601 |   202.4     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03311 |  -0.03653 |   204.8     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03307 |  -0.03625 |   207.3     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03277 |  -0.03634 |   209.8     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03272 |  -0.03674 |   212.2     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03244 |  -0.03635 |   214.8     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03272 |  -0.03649 |   217.6     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03258 |  -0.03632 |   220.5     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03274 |  -0.03626 |   223.2     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03299 |  -0.03655 |   225.8     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03262 |  -0.03669 |   228.4     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03296 |  -0.03643 |   230.8     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03245 |  -0.03729 |   233.5     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03266 |  -0.03671 |   236.0     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03286 |  -0.03604 |   238.4     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03249 |  -0.03572 |   240.9     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03269 |  -0.03575 |   243.2     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03263 |  -0.03637 |   245.6     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03255 |  -0.03624 |   248.1     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03291 |  -0.03637 |   250.5     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03331 |  -0.03599 |   253.1     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03255 |  -0.03578 |   255.5     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03248 |  -0.03615 |   257.9     \n",
      "Current learning rate:  0.02\n",
      "| 100   | -0.03284 |  -0.03560 |   260.3     \n",
      "Current learning rate:  0.02\n",
      "| 101   | -0.03286 |  -0.03512 |   262.9     \n",
      "Current learning rate:  0.02\n",
      "| 102   | -0.03263 |  -0.03565 |   265.3     \n",
      "Current learning rate:  0.02\n",
      "| 103   | -0.03251 |  -0.03557 |   267.8     \n",
      "Current learning rate:  0.02\n",
      "| 104   | -0.03266 |  -0.03534 |   270.1     \n",
      "Current learning rate:  0.02\n",
      "| 105   | -0.03245 |  -0.03585 |   272.6     \n",
      "Current learning rate:  0.02\n",
      "| 106   | -0.03271 |  -0.03585 |   275.0     \n",
      "Current learning rate:  0.02\n",
      "| 107   | -0.03258 |  -0.03616 |   277.4     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate:  0.02\n",
      "| 108   | -0.03265 |  -0.03607 |   279.9     \n",
      "Current learning rate:  0.02\n",
      "| 109   | -0.03283 |  -0.03578 |   282.4     \n",
      "Current learning rate:  0.02\n",
      "| 110   | -0.03322 |  -0.03595 |   284.7     \n",
      "Current learning rate:  0.02\n",
      "| 111   | -0.03322 |  -0.03629 |   287.2     \n",
      "Current learning rate:  0.02\n",
      "| 112   | -0.03322 |  -0.03678 |   289.7     \n",
      "Current learning rate:  0.02\n",
      "| 113   | -0.03329 |  -0.03595 |   292.1     \n",
      "Current learning rate:  0.02\n",
      "| 114   | -0.03304 |  -0.03527 |   294.6     \n",
      "Current learning rate:  0.02\n",
      "| 115   | -0.03290 |  -0.03530 |   297.3     \n",
      "Current learning rate:  0.02\n",
      "| 116   | -0.03310 |  -0.03505 |   299.8     \n",
      "Current learning rate:  0.02\n",
      "| 117   | -0.03308 |  -0.03540 |   302.3     \n",
      "Current learning rate:  0.02\n",
      "| 118   | -0.03309 |  -0.03484 |   304.8     \n",
      "Current learning rate:  0.02\n",
      "| 119   | -0.03286 |  -0.03488 |   307.1     \n",
      "Current learning rate:  0.02\n",
      "| 120   | -0.03302 |  -0.03463 |   309.6     \n",
      "Current learning rate:  0.02\n",
      "| 121   | -0.03287 |  -0.03457 |   312.1     \n",
      "Current learning rate:  0.02\n",
      "| 122   | -0.03279 |  -0.03487 |   314.9     \n",
      "Current learning rate:  0.02\n",
      "| 123   | -0.03287 |  -0.03463 |   317.3     \n",
      "Current learning rate:  0.02\n",
      "| 124   | -0.03277 |  -0.03531 |   319.7     \n",
      "Current learning rate:  0.02\n",
      "| 125   | -0.03272 |  -0.03483 |   322.4     \n",
      "Current learning rate:  0.02\n",
      "| 126   | -0.03245 |  -0.03496 |   325.0     \n",
      "Current learning rate:  0.02\n",
      "| 127   | -0.03231 |  -0.03536 |   327.6     \n",
      "Current learning rate:  0.02\n",
      "| 128   | -0.03240 |  -0.03562 |   330.1     \n",
      "Current learning rate:  0.02\n",
      "| 129   | -0.03244 |  -0.03582 |   332.5     \n",
      "Current learning rate:  0.02\n",
      "| 130   | -0.03262 |  -0.03588 |   334.9     \n",
      "Current learning rate:  0.02\n",
      "| 131   | -0.03249 |  -0.03579 |   337.4     \n",
      "Current learning rate:  0.02\n",
      "| 132   | -0.03254 |  -0.03582 |   339.8     \n",
      "Current learning rate:  0.02\n",
      "| 133   | -0.03235 |  -0.03608 |   342.2     \n",
      "Current learning rate:  0.02\n",
      "| 134   | -0.03244 |  -0.03654 |   344.6     \n",
      "Current learning rate:  0.02\n",
      "| 135   | -0.03247 |  -0.03589 |   347.0     \n",
      "Current learning rate:  0.02\n",
      "| 136   | -0.03286 |  -0.03578 |   349.4     \n",
      "Current learning rate:  0.02\n",
      "| 137   | -0.03266 |  -0.03581 |   351.8     \n",
      "Current learning rate:  0.02\n",
      "| 138   | -0.03270 |  -0.03648 |   354.3     \n",
      "Current learning rate:  0.02\n",
      "| 139   | -0.03249 |  -0.03624 |   356.7     \n",
      "Current learning rate:  0.02\n",
      "| 140   | -0.03250 |  -0.03584 |   359.1     \n",
      "Current learning rate:  0.02\n",
      "| 141   | -0.03261 |  -0.03593 |   361.5     \n",
      "Current learning rate:  0.02\n",
      "| 142   | -0.03243 |  -0.03565 |   363.9     \n",
      "Current learning rate:  0.02\n",
      "| 143   | -0.03270 |  -0.03474 |   366.4     \n",
      "Current learning rate:  0.02\n",
      "| 144   | -0.03236 |  -0.03511 |   368.8     \n",
      "Current learning rate:  0.02\n",
      "| 145   | -0.03249 |  -0.03495 |   371.4     \n",
      "Current learning rate:  0.02\n",
      "| 146   | -0.03239 |  -0.03601 |   373.8     \n",
      "Current learning rate:  0.02\n",
      "| 147   | -0.03241 |  -0.03607 |   376.3     \n",
      "Current learning rate:  0.02\n",
      "| 148   | -0.03226 |  -0.03582 |   378.8     \n",
      "Current learning rate:  0.02\n",
      "| 149   | -0.03232 |  -0.03551 |   381.2     \n",
      "Current learning rate:  0.004\n",
      "| 150   | -0.03231 |  -0.03630 |   383.6     \n",
      "Current learning rate:  0.004\n",
      "| 151   | -0.03210 |  -0.03654 |   386.1     \n",
      "Current learning rate:  0.004\n",
      "| 152   | -0.03221 |  -0.03652 |   388.5     \n",
      "Current learning rate:  0.004\n",
      "| 153   | -0.03200 |  -0.03666 |   390.9     \n",
      "Current learning rate:  0.004\n",
      "| 154   | -0.03202 |  -0.03657 |   393.4     \n",
      "Current learning rate:  0.004\n",
      "| 155   | -0.03205 |  -0.03651 |   395.8     \n",
      "Current learning rate:  0.004\n",
      "| 156   | -0.03181 |  -0.03645 |   398.3     \n",
      "Current learning rate:  0.004\n",
      "| 157   | -0.03186 |  -0.03661 |   400.7     \n",
      "Current learning rate:  0.004\n",
      "| 158   | -0.03181 |  -0.03656 |   403.2     \n",
      "Current learning rate:  0.004\n",
      "| 159   | -0.03189 |  -0.03665 |   405.7     \n",
      "Current learning rate:  0.004\n",
      "| 160   | -0.03190 |  -0.03674 |   408.4     \n",
      "Current learning rate:  0.004\n",
      "| 161   | -0.03190 |  -0.03676 |   411.0     \n",
      "Current learning rate:  0.004\n",
      "| 162   | -0.03187 |  -0.03670 |   413.7     \n",
      "Current learning rate:  0.004\n",
      "| 163   | -0.03176 |  -0.03675 |   416.2     \n",
      "Current learning rate:  0.004\n",
      "| 164   | -0.03198 |  -0.03678 |   418.7     \n",
      "Current learning rate:  0.004\n",
      "| 165   | -0.03171 |  -0.03655 |   421.1     \n",
      "Current learning rate:  0.004\n",
      "| 166   | -0.03184 |  -0.03691 |   423.5     \n",
      "Current learning rate:  0.004\n",
      "| 167   | -0.03193 |  -0.03661 |   426.0     \n",
      "Current learning rate:  0.004\n",
      "| 168   | -0.03163 |  -0.03647 |   428.4     \n",
      "Current learning rate:  0.004\n",
      "| 169   | -0.03123 |  -0.03672 |   430.9     \n",
      "Current learning rate:  0.004\n",
      "| 170   | -0.03151 |  -0.03667 |   433.3     \n",
      "Current learning rate:  0.004\n",
      "| 171   | -0.03154 |  -0.03674 |   435.7     \n",
      "Current learning rate:  0.004\n",
      "| 172   | -0.03151 |  -0.03669 |   438.2     \n",
      "Current learning rate:  0.004\n",
      "| 173   | -0.03127 |  -0.03675 |   440.7     \n",
      "Current learning rate:  0.004\n",
      "| 174   | -0.03136 |  -0.03674 |   443.2     \n",
      "Current learning rate:  0.004\n",
      "| 175   | -0.03113 |  -0.03682 |   445.6     \n",
      "Current learning rate:  0.004\n",
      "| 176   | -0.03106 |  -0.03691 |   448.1     \n",
      "Current learning rate:  0.004\n",
      "| 177   | -0.03151 |  -0.03689 |   450.5     \n",
      "Current learning rate:  0.004\n",
      "| 178   | -0.03135 |  -0.03682 |   452.9     \n",
      "Current learning rate:  0.004\n",
      "| 179   | -0.03110 |  -0.03696 |   455.3     \n",
      "Current learning rate:  0.004\n",
      "| 180   | -0.03111 |  -0.03701 |   457.8     \n",
      "Current learning rate:  0.004\n",
      "| 181   | -0.03106 |  -0.03699 |   460.2     \n",
      "Current learning rate:  0.004\n",
      "| 182   | -0.03105 |  -0.03685 |   462.7     \n",
      "Current learning rate:  0.004\n",
      "| 183   | -0.03122 |  -0.03669 |   465.1     \n",
      "Current learning rate:  0.004\n",
      "| 184   | -0.03081 |  -0.03676 |   467.6     \n",
      "Current learning rate:  0.004\n",
      "| 185   | -0.03105 |  -0.03688 |   470.0     \n",
      "Current learning rate:  0.004\n",
      "| 186   | -0.03119 |  -0.03686 |   472.6     \n",
      "Current learning rate:  0.004\n",
      "| 187   | -0.03110 |  -0.03685 |   475.0     \n",
      "Current learning rate:  0.004\n",
      "| 188   | -0.03090 |  -0.03697 |   477.5     \n",
      "Current learning rate:  0.004\n",
      "| 189   | -0.03117 |  -0.03707 |   479.9     \n",
      "Current learning rate:  0.004\n",
      "| 190   | -0.03079 |  -0.03732 |   482.3     \n",
      "Current learning rate:  0.004\n",
      "| 191   | -0.03068 |  -0.03737 |   484.8     \n",
      "Early stopping occured at epoch 191\n",
      "Training done in 484.751 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 5------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-a2b73a6d923d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-ac8e2f235701>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(fold)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0my_oof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mval_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_oof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pytorch_tabnet\\tab_model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# array of string classes and object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp_str_obj_array_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "run(fold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 6-------------\n",
      "Will train until validation stopping metric hasn't improved in 70 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -0.97630 |  -130.66858 |   2.4       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.15341 |  -11.95034 |   4.9       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.08091 |  -0.86420 |   7.3       \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.05809 |  -1.05292 |   9.7       \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.04717 |  -0.38167 |   12.2      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.04390 |  -0.09270 |   14.6      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.04052 |  -0.08952 |   17.1      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.03873 |  -0.07250 |   19.6      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.03777 |  -0.05747 |   22.0      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.03725 |  -0.05303 |   24.5      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.03759 |  -0.04670 |   26.9      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03694 |  -0.04180 |   29.3      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03618 |  -0.04006 |   31.8      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03645 |  -0.03869 |   34.2      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03542 |  -0.03845 |   36.6      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03556 |  -0.03893 |   39.0      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03512 |  -0.03822 |   41.4      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03486 |  -0.03787 |   43.8      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03469 |  -0.03838 |   46.3      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03464 |  -0.03909 |   48.7      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03476 |  -0.03697 |   51.1      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03468 |  -0.03717 |   53.5      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03448 |  -0.03755 |   56.0      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03465 |  -0.03746 |   58.4      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03415 |  -0.03757 |   60.8      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03414 |  -0.03759 |   63.3      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03469 |  -0.03784 |   65.7      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03465 |  -0.03760 |   68.1      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03480 |  -0.03691 |   70.5      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03467 |  -0.03717 |   73.0      \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03477 |  -0.03665 |   75.4      \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03463 |  -0.03683 |   77.8      \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03432 |  -0.03659 |   80.2      \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03433 |  -0.03679 |   82.6      \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03432 |  -0.03677 |   85.0      \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03393 |  -0.03629 |   87.4      \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03411 |  -0.03658 |   89.8      \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03402 |  -0.03671 |   92.2      \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03385 |  -0.03624 |   94.7      \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03387 |  -0.03624 |   97.2      \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03416 |  -0.03661 |   99.6      \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03407 |  -0.03743 |   102.0     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03450 |  -0.03825 |   104.4     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03535 |  -0.03715 |   106.8     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03515 |  -0.03656 |   109.2     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03431 |  -0.03789 |   111.6     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03382 |  -0.03658 |   114.0     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03360 |  -0.03662 |   116.5     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03412 |  -0.03657 |   119.0     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03426 |  -0.03670 |   121.7     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03400 |  -0.03711 |   124.1     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03402 |  -0.03680 |   126.5     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03375 |  -0.03674 |   128.9     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03341 |  -0.03721 |   131.4     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03372 |  -0.03729 |   133.8     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03380 |  -0.03666 |   136.2     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03344 |  -0.03717 |   138.5     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03351 |  -0.03728 |   141.0     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03357 |  -0.03781 |   143.5     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03374 |  -0.03718 |   146.1     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03346 |  -0.03728 |   148.6     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03307 |  -0.03738 |   151.1     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03300 |  -0.03791 |   153.6     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03307 |  -0.03716 |   156.0     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03332 |  -0.03709 |   158.5     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03319 |  -0.03698 |   160.9     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03349 |  -0.03730 |   163.4     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03339 |  -0.03710 |   165.8     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03314 |  -0.03713 |   168.2     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03318 |  -0.03707 |   170.7     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03318 |  -0.03703 |   173.1     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03325 |  -0.03697 |   175.6     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03327 |  -0.03727 |   178.0     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03303 |  -0.03698 |   180.4     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03323 |  -0.03726 |   182.8     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03322 |  -0.03700 |   185.2     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03336 |  -0.03720 |   187.7     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03283 |  -0.03706 |   190.1     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03292 |  -0.03747 |   192.5     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03306 |  -0.03697 |   194.9     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03265 |  -0.03687 |   197.3     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03285 |  -0.03693 |   199.9     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03293 |  -0.03707 |   202.4     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03316 |  -0.03726 |   204.9     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03322 |  -0.03726 |   207.3     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03320 |  -0.03723 |   209.8     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03293 |  -0.03772 |   212.2     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03312 |  -0.03763 |   214.6     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03293 |  -0.03719 |   217.1     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03284 |  -0.03724 |   219.5     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03304 |  -0.03768 |   221.9     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03272 |  -0.03737 |   224.4     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03271 |  -0.03744 |   226.8     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03274 |  -0.03746 |   229.3     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03269 |  -0.03794 |   231.7     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03297 |  -0.03701 |   234.1     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03303 |  -0.03741 |   236.5     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03269 |  -0.03759 |   239.0     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03274 |  -0.03765 |   241.3     \n",
      "Current learning rate:  0.02\n",
      "| 100   | -0.03270 |  -0.03833 |   243.7     \n",
      "Current learning rate:  0.02\n",
      "| 101   | -0.03250 |  -0.03785 |   246.1     \n",
      "Current learning rate:  0.02\n",
      "| 102   | -0.03278 |  -0.03771 |   248.6     \n",
      "Current learning rate:  0.02\n",
      "| 103   | -0.03247 |  -0.03773 |   251.3     \n",
      "Current learning rate:  0.02\n",
      "| 104   | -0.03252 |  -0.03790 |   254.0     \n",
      "Current learning rate:  0.02\n",
      "| 105   | -0.03215 |  -0.03758 |   256.6     \n",
      "Current learning rate:  0.02\n",
      "| 106   | -0.03285 |  -0.03776 |   259.3     \n",
      "Current learning rate:  0.02\n",
      "| 107   | -0.03288 |  -0.03761 |   261.7     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate:  0.02\n",
      "| 108   | -0.03293 |  -0.03777 |   264.2     \n",
      "Current learning rate:  0.02\n",
      "| 109   | -0.03304 |  -0.03756 |   266.6     \n",
      "Current learning rate:  0.02\n",
      "| 110   | -0.03274 |  -0.03800 |   269.0     \n",
      "Early stopping occured at epoch 110\n",
      "Training done in 269.003 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 6------------\n",
      "Validation score: 80.96424\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training Begining for fold 7-------------\n",
      "Will train until validation stopping metric hasn't improved in 70 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "Current learning rate:  0.02\n",
      "| 1     | -0.57585 |  -13.03573 |   2.4       \n",
      "Current learning rate:  0.02\n",
      "| 2     | -0.12376 |  -8.42655 |   4.8       \n",
      "Current learning rate:  0.02\n",
      "| 3     | -0.07789 |  -2.31332 |   7.3       \n",
      "Current learning rate:  0.02\n",
      "| 4     | -0.05598 |  -0.66771 |   9.7       \n",
      "Current learning rate:  0.02\n",
      "| 5     | -0.04810 |  -0.50590 |   12.1      \n",
      "Current learning rate:  0.02\n",
      "| 6     | -0.04193 |  -0.20648 |   14.5      \n",
      "Current learning rate:  0.02\n",
      "| 7     | -0.03984 |  -0.14656 |   16.9      \n",
      "Current learning rate:  0.02\n",
      "| 8     | -0.03887 |  -0.11180 |   19.3      \n",
      "Current learning rate:  0.02\n",
      "| 9     | -0.03756 |  -0.08997 |   21.8      \n",
      "Current learning rate:  0.02\n",
      "| 10    | -0.03716 |  -0.06562 |   24.2      \n",
      "Current learning rate:  0.02\n",
      "| 11    | -0.03680 |  -0.05682 |   26.7      \n",
      "Current learning rate:  0.02\n",
      "| 12    | -0.03615 |  -0.05127 |   29.0      \n",
      "Current learning rate:  0.02\n",
      "| 13    | -0.03576 |  -0.04720 |   31.5      \n",
      "Current learning rate:  0.02\n",
      "| 14    | -0.03522 |  -0.05299 |   33.9      \n",
      "Current learning rate:  0.02\n",
      "| 15    | -0.03517 |  -0.04801 |   36.3      \n",
      "Current learning rate:  0.02\n",
      "| 16    | -0.03495 |  -0.04678 |   38.8      \n",
      "Current learning rate:  0.02\n",
      "| 17    | -0.03551 |  -0.04491 |   41.2      \n",
      "Current learning rate:  0.02\n",
      "| 18    | -0.03529 |  -0.04208 |   43.7      \n",
      "Current learning rate:  0.02\n",
      "| 19    | -0.03470 |  -0.04401 |   46.1      \n",
      "Current learning rate:  0.02\n",
      "| 20    | -0.03490 |  -0.04102 |   48.5      \n",
      "Current learning rate:  0.02\n",
      "| 21    | -0.03461 |  -0.04107 |   50.9      \n",
      "Current learning rate:  0.02\n",
      "| 22    | -0.03454 |  -0.04018 |   53.3      \n",
      "Current learning rate:  0.02\n",
      "| 23    | -0.03448 |  -0.04015 |   55.7      \n",
      "Current learning rate:  0.02\n",
      "| 24    | -0.03431 |  -0.04045 |   58.2      \n",
      "Current learning rate:  0.02\n",
      "| 25    | -0.03443 |  -0.04035 |   60.6      \n",
      "Current learning rate:  0.02\n",
      "| 26    | -0.03423 |  -0.03916 |   63.1      \n",
      "Current learning rate:  0.02\n",
      "| 27    | -0.03397 |  -0.03934 |   65.5      \n",
      "Current learning rate:  0.02\n",
      "| 28    | -0.03412 |  -0.03872 |   67.9      \n",
      "Current learning rate:  0.02\n",
      "| 29    | -0.03406 |  -0.03852 |   70.3      \n",
      "Current learning rate:  0.02\n",
      "| 30    | -0.03392 |  -0.03808 |   72.8      \n",
      "Current learning rate:  0.02\n",
      "| 31    | -0.03415 |  -0.03766 |   75.2      \n",
      "Current learning rate:  0.02\n",
      "| 32    | -0.03405 |  -0.03701 |   77.6      \n",
      "Current learning rate:  0.02\n",
      "| 33    | -0.03396 |  -0.03715 |   80.1      \n",
      "Current learning rate:  0.02\n",
      "| 34    | -0.03400 |  -0.03705 |   82.5      \n",
      "Current learning rate:  0.02\n",
      "| 35    | -0.03356 |  -0.03697 |   85.0      \n",
      "Current learning rate:  0.02\n",
      "| 36    | -0.03401 |  -0.03701 |   87.4      \n",
      "Current learning rate:  0.02\n",
      "| 37    | -0.03378 |  -0.03785 |   89.8      \n",
      "Current learning rate:  0.02\n",
      "| 38    | -0.03404 |  -0.03868 |   92.2      \n",
      "Current learning rate:  0.02\n",
      "| 39    | -0.03421 |  -0.03815 |   94.6      \n",
      "Current learning rate:  0.02\n",
      "| 40    | -0.03372 |  -0.03842 |   97.0      \n",
      "Current learning rate:  0.02\n",
      "| 41    | -0.03410 |  -0.03859 |   99.4      \n",
      "Current learning rate:  0.02\n",
      "| 42    | -0.03372 |  -0.03778 |   101.8     \n",
      "Current learning rate:  0.02\n",
      "| 43    | -0.03358 |  -0.03708 |   104.3     \n",
      "Current learning rate:  0.02\n",
      "| 44    | -0.03367 |  -0.03708 |   106.7     \n",
      "Current learning rate:  0.02\n",
      "| 45    | -0.03363 |  -0.03709 |   109.1     \n",
      "Current learning rate:  0.02\n",
      "| 46    | -0.03343 |  -0.03713 |   111.5     \n",
      "Current learning rate:  0.02\n",
      "| 47    | -0.03338 |  -0.03723 |   113.9     \n",
      "Current learning rate:  0.02\n",
      "| 48    | -0.03324 |  -0.03710 |   116.3     \n",
      "Current learning rate:  0.02\n",
      "| 49    | -0.03339 |  -0.03661 |   118.7     \n",
      "Current learning rate:  0.02\n",
      "| 50    | -0.03333 |  -0.03613 |   121.1     \n",
      "Current learning rate:  0.02\n",
      "| 51    | -0.03342 |  -0.03712 |   123.8     \n",
      "Current learning rate:  0.02\n",
      "| 52    | -0.03365 |  -0.03660 |   126.3     \n",
      "Current learning rate:  0.02\n",
      "| 53    | -0.03351 |  -0.03660 |   128.8     \n",
      "Current learning rate:  0.02\n",
      "| 54    | -0.03332 |  -0.03688 |   131.2     \n",
      "Current learning rate:  0.02\n",
      "| 55    | -0.03325 |  -0.03711 |   133.6     \n",
      "Current learning rate:  0.02\n",
      "| 56    | -0.03329 |  -0.03689 |   136.1     \n",
      "Current learning rate:  0.02\n",
      "| 57    | -0.03323 |  -0.03675 |   138.4     \n",
      "Current learning rate:  0.02\n",
      "| 58    | -0.03325 |  -0.03708 |   140.8     \n",
      "Current learning rate:  0.02\n",
      "| 59    | -0.03336 |  -0.03785 |   143.3     \n",
      "Current learning rate:  0.02\n",
      "| 60    | -0.03416 |  -0.03632 |   145.7     \n",
      "Current learning rate:  0.02\n",
      "| 61    | -0.03399 |  -0.03695 |   148.2     \n",
      "Current learning rate:  0.02\n",
      "| 62    | -0.03371 |  -0.03682 |   150.6     \n",
      "Current learning rate:  0.02\n",
      "| 63    | -0.03363 |  -0.03689 |   153.0     \n",
      "Current learning rate:  0.02\n",
      "| 64    | -0.03350 |  -0.03735 |   155.4     \n",
      "Current learning rate:  0.02\n",
      "| 65    | -0.03345 |  -0.03614 |   157.8     \n",
      "Current learning rate:  0.02\n",
      "| 66    | -0.03326 |  -0.03600 |   160.3     \n",
      "Current learning rate:  0.02\n",
      "| 67    | -0.03327 |  -0.03611 |   162.7     \n",
      "Current learning rate:  0.02\n",
      "| 68    | -0.03304 |  -0.03656 |   165.0     \n",
      "Current learning rate:  0.02\n",
      "| 69    | -0.03327 |  -0.03702 |   167.4     \n",
      "Current learning rate:  0.02\n",
      "| 70    | -0.03314 |  -0.03646 |   169.8     \n",
      "Current learning rate:  0.02\n",
      "| 71    | -0.03290 |  -0.03643 |   172.2     \n",
      "Current learning rate:  0.02\n",
      "| 72    | -0.03302 |  -0.03668 |   174.6     \n",
      "Current learning rate:  0.02\n",
      "| 73    | -0.03312 |  -0.03647 |   177.0     \n",
      "Current learning rate:  0.02\n",
      "| 74    | -0.03307 |  -0.03662 |   179.4     \n",
      "Current learning rate:  0.02\n",
      "| 75    | -0.03346 |  -0.03657 |   181.9     \n",
      "Current learning rate:  0.02\n",
      "| 76    | -0.03321 |  -0.03701 |   184.3     \n",
      "Current learning rate:  0.02\n",
      "| 77    | -0.03322 |  -0.03661 |   186.7     \n",
      "Current learning rate:  0.02\n",
      "| 78    | -0.03318 |  -0.03698 |   189.1     \n",
      "Current learning rate:  0.02\n",
      "| 79    | -0.03309 |  -0.03729 |   191.5     \n",
      "Current learning rate:  0.02\n",
      "| 80    | -0.03306 |  -0.03725 |   193.9     \n",
      "Current learning rate:  0.02\n",
      "| 81    | -0.03286 |  -0.03706 |   196.3     \n",
      "Current learning rate:  0.02\n",
      "| 82    | -0.03306 |  -0.03707 |   198.7     \n",
      "Current learning rate:  0.02\n",
      "| 83    | -0.03303 |  -0.03772 |   201.1     \n",
      "Current learning rate:  0.02\n",
      "| 84    | -0.03285 |  -0.03720 |   203.5     \n",
      "Current learning rate:  0.02\n",
      "| 85    | -0.03282 |  -0.03727 |   206.0     \n",
      "Current learning rate:  0.02\n",
      "| 86    | -0.03304 |  -0.03769 |   208.4     \n",
      "Current learning rate:  0.02\n",
      "| 87    | -0.03301 |  -0.03726 |   210.7     \n",
      "Current learning rate:  0.02\n",
      "| 88    | -0.03282 |  -0.03733 |   213.1     \n",
      "Current learning rate:  0.02\n",
      "| 89    | -0.03284 |  -0.03736 |   215.5     \n",
      "Current learning rate:  0.02\n",
      "| 90    | -0.03302 |  -0.03715 |   218.0     \n",
      "Current learning rate:  0.02\n",
      "| 91    | -0.03277 |  -0.03723 |   220.4     \n",
      "Current learning rate:  0.02\n",
      "| 92    | -0.03256 |  -0.03774 |   222.8     \n",
      "Current learning rate:  0.02\n",
      "| 93    | -0.03292 |  -0.03755 |   225.2     \n",
      "Current learning rate:  0.02\n",
      "| 94    | -0.03275 |  -0.03733 |   227.6     \n",
      "Current learning rate:  0.02\n",
      "| 95    | -0.03295 |  -0.03709 |   230.0     \n",
      "Current learning rate:  0.02\n",
      "| 96    | -0.03323 |  -0.03728 |   232.4     \n",
      "Current learning rate:  0.02\n",
      "| 97    | -0.03311 |  -0.03722 |   234.8     \n",
      "Current learning rate:  0.02\n",
      "| 98    | -0.03330 |  -0.03746 |   237.2     \n",
      "Current learning rate:  0.02\n",
      "| 99    | -0.03327 |  -0.03764 |   239.6     \n",
      "Current learning rate:  0.02\n",
      "| 100   | -0.03336 |  -0.03740 |   242.0     \n",
      "Current learning rate:  0.02\n",
      "| 101   | -0.03315 |  -0.03787 |   244.4     \n",
      "Current learning rate:  0.02\n",
      "| 102   | -0.03362 |  -0.03715 |   246.9     \n",
      "Current learning rate:  0.02\n",
      "| 103   | -0.03328 |  -0.03802 |   249.3     \n",
      "Current learning rate:  0.02\n",
      "| 104   | -0.03335 |  -0.03729 |   251.7     \n",
      "Current learning rate:  0.02\n",
      "| 105   | -0.03335 |  -0.03733 |   254.1     \n",
      "Current learning rate:  0.02\n",
      "| 106   | -0.03331 |  -0.03725 |   256.5     \n",
      "Current learning rate:  0.02\n",
      "| 107   | -0.03332 |  -0.03699 |   258.9     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate:  0.02\n",
      "| 108   | -0.03328 |  -0.03682 |   261.4     \n",
      "Current learning rate:  0.02\n",
      "| 109   | -0.03347 |  -0.03699 |   263.7     \n",
      "Current learning rate:  0.02\n",
      "| 110   | -0.03336 |  -0.03754 |   266.1     \n",
      "Current learning rate:  0.02\n",
      "| 111   | -0.03341 |  -0.03742 |   268.6     \n",
      "Current learning rate:  0.02\n",
      "| 112   | -0.03363 |  -0.03698 |   271.0     \n",
      "Current learning rate:  0.02\n",
      "| 113   | -0.03329 |  -0.03783 |   273.4     \n",
      "Current learning rate:  0.02\n",
      "| 114   | -0.03353 |  -0.03702 |   275.8     \n",
      "Current learning rate:  0.02\n",
      "| 115   | -0.03314 |  -0.03696 |   278.2     \n",
      "Current learning rate:  0.02\n",
      "| 116   | -0.03366 |  -0.03704 |   280.6     \n",
      "Current learning rate:  0.02\n",
      "| 117   | -0.03341 |  -0.03749 |   283.0     \n",
      "Current learning rate:  0.02\n",
      "| 118   | -0.03308 |  -0.03699 |   285.4     \n",
      "Current learning rate:  0.02\n",
      "| 119   | -0.03283 |  -0.03718 |   287.9     \n",
      "Current learning rate:  0.02\n",
      "| 120   | -0.03291 |  -0.03717 |   290.3     \n",
      "Current learning rate:  0.02\n",
      "| 121   | -0.03276 |  -0.03747 |   292.7     \n",
      "Current learning rate:  0.02\n",
      "| 122   | -0.03281 |  -0.03704 |   295.1     \n",
      "Current learning rate:  0.02\n",
      "| 123   | -0.03266 |  -0.03704 |   297.6     \n",
      "Current learning rate:  0.02\n",
      "| 124   | -0.03298 |  -0.03690 |   300.0     \n",
      "Current learning rate:  0.02\n",
      "| 125   | -0.03261 |  -0.03711 |   302.4     \n",
      "Current learning rate:  0.02\n",
      "| 126   | -0.03269 |  -0.03772 |   304.8     \n",
      "Current learning rate:  0.02\n",
      "| 127   | -0.03281 |  -0.03793 |   307.2     \n",
      "Current learning rate:  0.02\n",
      "| 128   | -0.03261 |  -0.03752 |   309.6     \n",
      "Current learning rate:  0.02\n",
      "| 129   | -0.03258 |  -0.03796 |   312.0     \n",
      "Current learning rate:  0.02\n",
      "| 130   | -0.03260 |  -0.03804 |   314.4     \n",
      "Current learning rate:  0.02\n",
      "| 131   | -0.03209 |  -0.03784 |   316.8     \n",
      "Current learning rate:  0.02\n",
      "| 132   | -0.03262 |  -0.03828 |   319.3     \n",
      "Current learning rate:  0.02\n",
      "| 133   | -0.03250 |  -0.03846 |   321.7     \n",
      "Current learning rate:  0.02\n",
      "| 134   | -0.03258 |  -0.03833 |   324.1     \n",
      "Current learning rate:  0.02\n",
      "| 135   | -0.03239 |  -0.03836 |   326.5     \n",
      "Current learning rate:  0.02\n",
      "| 136   | -0.03253 |  -0.03849 |   329.2     \n",
      "Early stopping occured at epoch 136\n",
      "Training done in 329.168 seconds.\n",
      "---------------------------------------\n",
      "--------Validating For fold 7------------\n",
      "Validation score: 81.02720\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(fold=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.mean(axis=-1) # Taking mean of all the fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Employee_ID'] = test['Employee_ID']\n",
    "sub['Attrition_rate'] = y_test\n",
    "sub.to_csv('tabnet_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"tabnet_submission.csv\")\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 24)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(test[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Employee_ID'] = test['Employee_ID']\n",
    "sub['Attrition_rate'] = y_test\n",
    "sub.to_csv('tabnet_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

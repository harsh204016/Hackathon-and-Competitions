{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 198166 samples, validate on 10430 samples\n",
      "Epoch 1/100\n",
      "198166/198166 [==============================] - 194s 977us/step - loss: 6.0958 - acc: 0.0319 - val_loss: 5.1682 - val_acc: 0.0479\n",
      "Epoch 2/100\n",
      "198166/198166 [==============================] - 186s 941us/step - loss: 4.9814 - acc: 0.0566 - val_loss: 4.7912 - val_acc: 0.0623\n",
      "Epoch 3/100\n",
      "198166/198166 [==============================] - 187s 942us/step - loss: 4.6468 - acc: 0.0746 - val_loss: 4.6126 - val_acc: 0.0759\n",
      "Epoch 4/100\n",
      "198166/198166 [==============================] - 186s 941us/step - loss: 4.4272 - acc: 0.0882 - val_loss: 4.5130 - val_acc: 0.0771\n",
      "Epoch 5/100\n",
      "198166/198166 [==============================] - 187s 942us/step - loss: 4.2542 - acc: 0.1024 - val_loss: 4.4446 - val_acc: 0.0847\n",
      "Epoch 6/100\n",
      "198166/198166 [==============================] - 187s 942us/step - loss: 4.1173 - acc: 0.1112 - val_loss: 4.3987 - val_acc: 0.0862\n",
      "Epoch 7/100\n",
      "198166/198166 [==============================] - 186s 941us/step - loss: 4.0072 - acc: 0.1194 - val_loss: 4.3722 - val_acc: 0.0901\n",
      "Epoch 8/100\n",
      "198166/198166 [==============================] - 186s 940us/step - loss: 3.9101 - acc: 0.1281 - val_loss: 4.3642 - val_acc: 0.0862\n",
      "Epoch 9/100\n",
      "198166/198166 [==============================] - 187s 941us/step - loss: 3.8283 - acc: 0.1358 - val_loss: 4.3490 - val_acc: 0.0832\n",
      "Epoch 10/100\n",
      "198166/198166 [==============================] - 186s 941us/step - loss: 3.7591 - acc: 0.1415 - val_loss: 4.3501 - val_acc: 0.0854\n",
      "Epoch 11/100\n",
      "198166/198166 [==============================] - 187s 945us/step - loss: 3.7010 - acc: 0.1463 - val_loss: 4.3519 - val_acc: 0.0827\n",
      "Epoch 12/100\n",
      "198166/198166 [==============================] - 187s 944us/step - loss: 3.6480 - acc: 0.1500 - val_loss: 4.3536 - val_acc: 0.0795\n",
      "Epoch 13/100\n",
      "198166/198166 [==============================] - 187s 946us/step - loss: 3.5997 - acc: 0.1550 - val_loss: 4.3699 - val_acc: 0.0823\n",
      "Epoch 14/100\n",
      "198166/198166 [==============================] - 186s 940us/step - loss: 3.5591 - acc: 0.1568 - val_loss: 4.3701 - val_acc: 0.0769\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,Input,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    \n",
    "    # Create labels\n",
    "    label = train[train.challenge_sequence > 10][['user_id','challenge']]\n",
    "    label.rename(columns={'challenge':'label'},inplace=True)\n",
    "    \n",
    "    # Treat the sequence of challenges as text\n",
    "    df = train[train.challenge_sequence <= 10].groupby('user_id').challenge.aggregate(lambda x: ' '.join(x)).reset_index()\n",
    "    \n",
    "    # Merge Labels\n",
    "    df = df.merge(label)\n",
    "    \n",
    "    # Validation split for early stopping\n",
    "    df_train, df_validation = train_test_split(df.sample(frac=1,random_state=123), test_size=0.05, random_state=123)\n",
    "    \n",
    "    # Load all the challenges\n",
    "    challenges = pd.read_csv('challenge_data.csv')\n",
    "    \n",
    "    # Encode challenges\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(challenges['challenge_ID'])\n",
    "    df_train['brand_id_encoded'] = encoder.transform(df_train.label)\n",
    "    df_validation['brand_id_encoded'] = encoder.transform(df_validation.label)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(df_train['challenge'])\n",
    "    \n",
    "    # Constants\n",
    "    NB_WORDS = len(tokenizer.word_index)+1\n",
    "    MAX_SEQUENCE_LENGTH = 10\n",
    "    N_CATEGORIES = challenges.shape[0]\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences_train = tokenizer.texts_to_sequences(df_train['challenge'])\n",
    "    sequences_validation = tokenizer.texts_to_sequences(df_validation['challenge'])\n",
    "    \n",
    "    # Pad sequences\n",
    "    x_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    x_validation = pad_sequences(sequences_validation, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    # Set Labels\n",
    "    y_train = df_train['brand_id_encoded'].values\n",
    "    y_validation= df_validation['brand_id_encoded'].values\n",
    "    \n",
    "    # NN architecture\n",
    "    def get_model(path='',lr=0.001):\n",
    "        adam = Adam(lr=lr)\n",
    "        inp = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "        x = Embedding(NB_WORDS,256)(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Bidirectional(LSTM(128, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        x = Dense(N_CATEGORIES, activation=\"softmax\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        if path != '':\n",
    "            model.load_weights(path)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = get_model()\n",
    "    \n",
    "    # Model callbacks\n",
    "    path = 'best_model_weights'\n",
    "    es_callback = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    mc_callback = ModelCheckpoint('{}.hdf5'.format(path), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "    callbacks = [es_callback,mc_callback]\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=1024,\n",
    "              validation_data=(x_validation, y_validation),\n",
    "              callbacks = callbacks\n",
    "             )\n",
    "    \n",
    "    # Load best weights\n",
    "    model = get_model('{}.hdf5'.format(path))\n",
    "    \n",
    "    # Test preprocessing\n",
    "    def padding(text):\n",
    "        return pad_sequences(tokenizer.texts_to_sequences(text), maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    test_text = test[test.challenge_sequence <= 10].groupby('user_id').challenge.aggregate(lambda x: ' '.join(x)).reset_index()\n",
    "    x_test = padding(test_text.challenge)\n",
    "    \n",
    "    # Get top 3 predictions for each user\n",
    "    pred = model.predict(x_test,batch_size=2048)\n",
    "    pred = pred.argsort(axis=1)[:,-3:][:,::-1]\n",
    "    \n",
    "    # Write Predictions\n",
    "    df_list = []\n",
    "    for i in range(3):\n",
    "        test_11 = test_text[['user_id']]\n",
    "        test_11['user_sequence'] = test_11.user_id.astype(str) + '_'+str(i+11)\n",
    "        test_11['challenge'] = encoder.inverse_transform(pred[:,i])\n",
    "        df_list.append(test_11[['user_sequence','challenge']])\n",
    "    pd.concat(df_list).to_csv('bes_submission.csv',index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
